// internal/pkg/server/consumer.go
package server

import (
	"context"
	"encoding/json"
	"fmt"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/maxiaolu1981/cretem/nexuscore/component-base/validation"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/code"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/metrics"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/options"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/usercache"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/log"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/storage"
	v1 "github.com/maxiaolu1981/cretem/nexuscore/api/apiserver/v1"
	"github.com/maxiaolu1981/cretem/nexuscore/errors"
	"github.com/segmentio/kafka-go"
	"gorm.io/gorm"
)

type UserConsumer struct {
	reader     *kafka.Reader
	db         *gorm.DB
	redis      *storage.RedisCluster
	producer   *UserProducer
	topic      string
	groupID    string
	instanceID int // æ–°å¢ï¼šå®ä¾‹ID
	opts       *options.KafkaOptions
	// ç§»é™¤æœ¬åœ°ä¿æŠ¤çŠ¶æ€ï¼Œå…¨éƒ¨èµ°rediså…¨å±€key
	// ä¸»æ§é€‰ä¸¾ç›¸å…³
	isMaster bool
}

func NewUserConsumer(opts *options.KafkaOptions, topic, groupID string, db *gorm.DB, redis *storage.RedisCluster) *UserConsumer {
	consumer := &UserConsumer{
		reader: kafka.NewReader(kafka.ReaderConfig{
			Brokers: opts.Brokers,
			Topic:   topic,
			GroupID: groupID,

			// ä¼˜åŒ–é…ç½®
			MinBytes: 1 * 1024 * 1024, // é™ä½æœ€å°å­—èŠ‚æ•°ï¼Œç«‹å³æ¶ˆè´¹
			//MinBytes:      10e3,
			MaxBytes:      10e6,                   // 10MB
			MaxWait:       time.Millisecond * 100, // å¢åŠ åˆ°100ms
			QueueCapacity: 100,                    // é™ä½é˜Ÿåˆ—å®¹é‡ï¼Œé¿å…æ¶ˆæ¯å †ç§¯åœ¨å†…å­˜

			CommitInterval: 0,
			StartOffset:    kafka.FirstOffset,

			// æ·»åŠ é‡è¯•é…ç½®
			MaxAttempts:    opts.MaxRetries,
			ReadBackoffMin: time.Millisecond * 100,
			ReadBackoffMax: time.Millisecond * 1000,
		}),
		db:      db,
		redis:   redis,
		topic:   topic,
		groupID: groupID,
		opts:    opts,
		// æ–°å¢ï¼šå®ä¾‹IDèµ‹å€¼
		instanceID: parseInstanceID(opts.InstanceID),
	}
	//go consumer.startLagMonitor(context.Background())
	return consumer

}

// parseInstanceID æ”¯æŒ string->int è½¬æ¢ï¼Œè‹¥å¤±è´¥åˆ™ç”¨ hash å…œåº•
func parseInstanceID(idStr string) int {
	if idStr == "" {
		return 0
	}
	if n, err := strconv.Atoi(idStr); err == nil {
		return n
	}
	// fallback: hash string
	sum := 0
	for _, c := range idStr {
		sum += int(c)
	}
	return sum & 0x7FFFFFFF // ä¿è¯æ­£æ•°
}

// æ¶ˆè´¹
func (c *UserConsumer) StartConsuming(ctx context.Context, workerCount int, ready *sync.WaitGroup) {
	log.Infof("[Consumer] StartConsuming: topic=%s, groupID=%s, workerCount=%d", c.topic, c.groupID, workerCount)
	// job ç”¨äºåœ¨ fetcher ä¸ worker ä¹‹é—´ä¼ é€’æ¶ˆæ¯ï¼Œå¹¶æºå¸¦ä¸€ä¸ª done é€šé“ç”¨äºè¿”å›å¤„ç†ç»“æœ
	type job struct {
		msg      kafka.Message
		done     chan error
		workerID int
	}

	jobs := make(chan *job, 2048) // æå‡é€šé“å®¹é‡ï¼Œæ”¯æŒé«˜å¹¶å‘
	readyOnce := sync.Once{}
	signalReady := func() {
		if ready != nil {
			readyOnce.Do(func() {
				ready.Done()
			})
		}
	}
	defer signalReady()

	// å¯åŠ¨ worker æ± ï¼Œåªè´Ÿè´£å¤„ç†ä¸šåŠ¡ï¼Œä¸ç›´æ¥è°ƒç”¨ FetchMessage/CommitMessages
	var workerWg sync.WaitGroup
	// workeræ•°é‡ä¸åˆ†åŒºæ•°åŠ¨æ€åŒ¹é…ï¼Œä¿è¯æ¯ä¸ªåˆ†åŒºæœ‰ç‹¬ç«‹worker
	partitionCount := c.opts.DesiredPartitions
	actualWorkerCount := workerCount
	if partitionCount > 0 && workerCount < partitionCount {
		actualWorkerCount = partitionCount
	}
	for i := 0; i < actualWorkerCount; i++ {
		workerWg.Add(1)
		go func(workerID int) {
			defer workerWg.Done()
			for j := range jobs {
				// è®°å½•å¼€å§‹æ—¶é—´
				operation := c.getOperationFromHeaders(j.msg.Headers)
				messageKey := string(j.msg.Key)
				processStart := time.Now()

				// å¤„ç†æ¶ˆæ¯ï¼ˆå¸¦é‡è¯•çš„ä¸šåŠ¡å¤„ç†ï¼‰
				err := c.processMessageWithRetry(ctx, j.msg, 3)

				// åœ¨æœ¬åœ°è®°å½•æŒ‡æ ‡ï¼ˆworker è´Ÿè´£è®°å½•å¤„ç†è€—æ—¶/æˆåŠŸ/å¤±è´¥ï¼‰
				c.recordConsumerMetrics(operation, messageKey, processStart, err, j.workerID)

				// å°†å¤„ç†ç»“æœè¿”å›ç»™ fetcherï¼Œç”± fetcher è´Ÿè´£æäº¤åç§»
				j.done <- err
			}
		}(i)
	}

	// fetcher: è´Ÿè´£ä» Kafka æ‹‰å–æ¶ˆæ¯ï¼Œå¹¶åœ¨ worker å¤„ç†å®Œæˆåæäº¤åç§»é‡
	fetchLoopDone := make(chan struct{})
	go func() {
		defer close(fetchLoopDone)
		nextWorker := 0
		// æ‰¹é‡èšåˆæ”¯æŒ: é’ˆå¯¹ create/update/deleteï¼Œä½¿ç”¨ä¸€ä¸ªæ‰¹é‡ç¼“å­˜ç”± worker æ‰§è¡Œæ‰¹é‡DBå†™
		type batchItem struct {
			op  string
			msg kafka.Message
		}

		// å•ç‹¬çš„æ‰¹å¤„ç†é˜Ÿåˆ— (ç”¨äºæ‰¹é‡DBå†™) â€” ç”±ä¸€ä¸ªè½»é‡ goroutine ç®¡ç†å®šæ—¶åˆ·æ–°
		batchCh := make(chan batchItem, 4096) // æ‰¹é‡é€šé“å®¹é‡æå‡

		// æ‰¹é‡ç¼“å†²ä¸æäº¤ goroutine
		go func() {
			// æ‰¹é‡å†™å…¥è¶…æ—¶å‚æ•°æå‡ï¼Œé»˜è®¤50ms-200ms
			batchTimeout := c.opts.BatchTimeout
			if batchTimeout < 50*time.Millisecond {
				batchTimeout = 50 * time.Millisecond
			} else if batchTimeout > 200*time.Millisecond {
				batchTimeout = 200 * time.Millisecond
			}
			ticker := time.NewTicker(batchTimeout)
			defer ticker.Stop()
			var createBatch []kafka.Message
			var deleteBatch []kafka.Message
			var updateBatch []kafka.Message
			// æ‰¹é‡å†™å…¥æœ€å¤§æ¡æ•°æå‡ï¼Œé»˜è®¤100-500
			maxBatchSize := c.opts.MaxDBBatchSize
			if maxBatchSize < 100 {
				maxBatchSize = 100
			} else if maxBatchSize > 500 {
				maxBatchSize = 500
			}
			flush := func() {
				if len(createBatch) > 0 {
					c.batchCreateToDB(ctx, createBatch)
					createBatch = createBatch[:0]
				}
				if len(deleteBatch) > 0 {
					c.batchDeleteFromDB(ctx, deleteBatch)
					deleteBatch = deleteBatch[:0]
				}
				if len(updateBatch) > 0 {
					c.batchUpdateToDB(ctx, updateBatch)
					updateBatch = updateBatch[:0]
				}
			}
			for {
				select {
				case bi, ok := <-batchCh:
					if !ok {
						flush()
						return
					}
					switch bi.op {
					case OperationCreate:
						createBatch = append(createBatch, bi.msg)
						if len(createBatch) >= maxBatchSize {
							c.batchCreateToDB(ctx, createBatch)
							createBatch = createBatch[:0]
						}
					case OperationDelete:
						deleteBatch = append(deleteBatch, bi.msg)
						if len(deleteBatch) >= maxBatchSize {
							c.batchDeleteFromDB(ctx, deleteBatch)
							deleteBatch = deleteBatch[:0]
						}
					case OperationUpdate:
						updateBatch = append(updateBatch, bi.msg)
						if len(updateBatch) >= maxBatchSize {
							c.batchUpdateToDB(ctx, updateBatch)
							updateBatch = updateBatch[:0]
						}
					default:
						// ignore others for batching
					}
				case <-ticker.C:
					flush()
				case <-ctx.Done():
					flush()
					return
				}
			}
		}()

		// ====== æ¶ˆè´¹é€Ÿç‡ç»Ÿè®¡ç›¸å…³å˜é‡ ======
		var consumeCount int64 = 0

		for {
			select {
			case <-ctx.Done():
				return
			default:
			}

			// ====== å®Œå…¨æ— é™é€Ÿï¼Œæ— æ—¥å¿— ======
			stats := c.reader.Stats()
			lag := stats.Lag
			if lag == 0 {
				select {
				case <-time.After(100 * time.Millisecond):
				case <-ctx.Done():
					return
				}
			}
			// ====== ç»“æŸ ======

			// FetchMessage å¸¦é‡è¯•ï¼šä¸ä¹‹å‰é€»è¾‘ä¿æŒä¸€è‡´
			var msg kafka.Message
			var fetchErr error
			for retry := 0; retry < c.opts.MaxRetries; retry++ {
				msg, fetchErr = c.reader.FetchMessage(ctx)
				if fetchErr == nil {
					break
				}
				if errors.Is(fetchErr, context.Canceled) || errors.Is(fetchErr, context.DeadlineExceeded) {
					log.Debugf("Fetcher: ä¸Šä¸‹æ–‡å·²å–æ¶ˆï¼Œåœæ­¢è·å–æ¶ˆæ¯")
					return
				}
				log.Warnf("Fetcher: è·å–æ¶ˆæ¯å¤±è´¥ (é‡è¯• %d/%d): %v", retry+1, c.opts.MaxRetries, fetchErr)
				backoff := time.Second * time.Duration(1<<uint(retry))
				select {
				case <-time.After(backoff):
				case <-ctx.Done():
					log.Debugf("Fetcher: é‡è¯•æœŸé—´ä¸Šä¸‹æ–‡å–æ¶ˆ")
					return
				}
			}
			if fetchErr != nil {
				log.Errorf("Fetcher: è·å–æ¶ˆæ¯æœ€ç»ˆå¤±è´¥: %v", fetchErr)
				// åœ¨ fetch å¤±è´¥æ—¶çŸ­æš‚åœé¡¿ï¼Œé¿å…ç´§å¾ªç¯
				select {
				case <-time.After(500 * time.Millisecond):
				case <-ctx.Done():
					return
				}
				continue
			}

			// dispatch to worker
			j := &job{msg: msg, done: make(chan error, 1), workerID: nextWorker}
			// ...existing code... // ç§»é™¤é˜Ÿåˆ—é•¿åº¦ç›‘æ§ï¼Œç¡®ä¿æ— é™æµ
			select {
			case jobs <- j:
				// dispatched
			case <-ctx.Done():
				return
			case <-time.After(10 * time.Millisecond):
				// å¦‚æœ worker é˜Ÿåˆ—é˜»å¡ï¼Œå°è¯•æŠŠæ¶ˆæ¯æ”¾åˆ°æ‰¹é‡é€šé“ä»¥è§¦å‘æ‰¹é‡å†™ï¼ˆé™ä½å»¶è¿Ÿï¼‰
				op := c.getOperationFromHeaders(msg.Headers)
				if op == OperationCreate || op == OperationDelete || op == OperationUpdate {
					select {
					case batchCh <- batchItem{op: op, msg: msg}:
					default:
						// å¦‚æœæ‰¹é‡é€šé“ä¹Ÿæ»¡äº†ï¼Œåˆ™ç»§ç»­ç­‰å¾…æ­£å¸¸ dispatch
						select {
						case jobs <- j:
						case <-ctx.Done():
							return
						}
					}
					continue
				}
			}

			// round-robin
			nextWorker = (nextWorker + 1) % workerCount

			// ç­‰å¾… worker å®Œæˆå¤„ç†
			procErr := <-j.done
			if procErr != nil {
				log.Warnf("Fetcher: message processing failed (worker=%d): %v", j.workerID, procErr)
				// å¤„ç†å¤±è´¥ï¼Œä¸æäº¤åç§»é‡ï¼ˆä¸ä¹‹å‰è¡Œä¸ºä¸€è‡´ï¼‰ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªæ¶ˆæ¯
				continue
			}

			// å¤„ç†æˆåŠŸåæäº¤åç§»
			if err := c.commitWithRetry(ctx, msg, j.workerID); err != nil {
				log.Errorf("Fetcher: æäº¤åç§»å¤±è´¥: %v", err)
				// æäº¤å¤±è´¥åˆ™ä¸é˜»å¡ fetcherï¼Œç»§ç»­ä¸‹ä¸€æ¡ï¼ˆcommitWithRetry å†…éƒ¨å·²åšé‡è¯•ï¼‰
			}

			// æ¶ˆè´¹é€Ÿç‡ç»Ÿè®¡ï¼šæ¯å¤„ç†ä¸€æ¡æ¶ˆæ¯è®¡æ•°+1
			consumeCount++
		}
	}()

	// åœ¨ worker ä¸ fetcher å¯åŠ¨åæ ‡è®°å°±ç»ª
	signalReady()

	// ç­‰å¾… fetcher ç»“æŸï¼ˆé€šå¸¸ç”± ctx å–æ¶ˆè§¦å‘ï¼‰ï¼Œç„¶åå…³é—­ jobs å¹¶ç­‰å¾… workers é€€å‡º
	<-fetchLoopDone
	close(jobs)
	workerWg.Wait()
}

// æ¶ˆæ¯è°ƒåº¦ - å·²å¼ƒç”¨
// StartConsuming å·²ç»é‡‡ç”¨å• fetcher + worker æ± çš„æ¨¡å¼æ›¿ä»£äº†æ—§çš„å¹¶å‘ Fetch/Commit å®ç°ã€‚
// ä¿ç•™è¯¥å‡½æ•°ç­¾åä»¥é¿å…æ½œåœ¨å¤–éƒ¨å¼•ç”¨ç¼–è¯‘é”™è¯¯ï¼Œä½†å®ç°ä¸ºç©ºã€‚

// å¤„ç†æ¶ˆæ¯
// ...old worker and processSingleMessage removed. Use StartConsuming with the new fetcher+worker flow.

// commitWithRetry å°è¯•æäº¤æ¶ˆæ¯åç§»ï¼Œé‡åˆ°ä¸´æ—¶é”™è¯¯ä¼šé‡è¯•
func (c *UserConsumer) commitWithRetry(ctx context.Context, msg kafka.Message, workerID int) error {
	maxAttempts := 3
	var lastErr error
	for i := 0; i < maxAttempts; i++ {
		if err := c.reader.CommitMessages(ctx, msg); err != nil {
			lastErr = err
			metrics.ConsumerProcessingErrors.WithLabelValues(c.topic, c.groupID, "commit", "commit_error").Inc()
			// record commit failure metric (partition as string)
			metrics.ConsumerCommitFailures.WithLabelValues(c.topic, c.groupID, fmt.Sprintf("%d", msg.Partition)).Inc()
			log.Warnf("Worker %d: æäº¤åç§»é‡å¤±è´¥ (å°è¯• %d/%d): topic=%s partition=%d offset=%d err=%v",
				workerID, i+1, maxAttempts, msg.Topic, msg.Partition, msg.Offset, err)
			// æŒ‡æ•°é€€é¿
			wait := time.Duration(100*(1<<uint(i))) * time.Millisecond
			select {
			case <-time.After(wait):
				continue
			case <-ctx.Done():
				return ctx.Err()
			}
		} else {
			// record commit success metric
			metrics.ConsumerCommitSuccess.WithLabelValues(c.topic, c.groupID, fmt.Sprintf("%d", msg.Partition)).Inc()
			//	log.Debugf("Worker %d: åç§»é‡æäº¤æˆåŠŸ: topic=%s partition=%d offset=%d", workerID, msg.Topic, msg.Partition, msg.Offset)
			return nil
		}
	}
	log.Errorf("Worker %d: æäº¤åç§»é‡æœ€ç»ˆå¤±è´¥: %v", workerID, lastErr)
	return lastErr
}

// ä¸šåŠ¡å¤„ç†
func (c *UserConsumer) processMessage(ctx context.Context, msg kafka.Message) error {
	operation := c.getOperationFromHeaders(msg.Headers)

	switch operation {
	case OperationCreate:
		return c.processCreateOperation(ctx, msg)
	case OperationUpdate:
		return c.processUpdateOperation(ctx, msg)
	case OperationDelete:
		return c.processDeleteOperation(ctx, msg)
	default:
		log.Errorf("æœªçŸ¥æ“ä½œç±»å‹: %s", operation)
		if c.producer != nil {
			return c.producer.SendToDeadLetterTopic(ctx, msg, "UNKNOWN_OPERATION: "+operation)
		}
		return fmt.Errorf("æœªçŸ¥æ“ä½œç±»å‹: %s", operation)
	}
}

func (c *UserConsumer) processCreateOperation(ctx context.Context, msg kafka.Message) error {
	// å…¼å®¹ä¸¤ç§ç»“æ„ï¼š1. æ‰å¹³ v1.User 2. å¸¦ metadata çš„åµŒå¥—ç»“æ„
	var user v1.User
	if err := json.Unmarshal(msg.Value, &user); err == nil {
		if err := validation.ValidateUserFields(user.Name, user.Nickname, user.Password, user.Email, user.Phone); err != nil {
			// å­—æ®µæ ¡éªŒå¤±è´¥ï¼Œç›´æ¥å†™å…¥æ­»ä¿¡åŒº
			return c.sendToDeadLetter(ctx, msg, err.Error())
		}
		log.Debugf("å¼€å§‹å»ºç«‹ç”¨æˆ·: username=%s", user.Name)
		if err := c.createUserInDB(ctx, &user); err != nil {
			// æ•°æ®åº“å†™å…¥å¤±è´¥ï¼Œç›´æ¥å†™å…¥æ­»ä¿¡åŒº
			return c.sendToDeadLetter(ctx, msg, "CREATE_DB_ERROR: "+err.Error())
		}
		if err := c.setUserCache(ctx, &user, nil); err != nil {
			log.Warnf("ç”¨æˆ·åˆ›å»ºæˆåŠŸä½†ç¼“å­˜è®¾ç½®å¤±è´¥: username=%s, error=%v", user.Name, err)
		} else {
			log.Debugf("ç”¨æˆ·%sç¼“å­˜æˆåŠŸ", user.Name)
		}
		log.Debugf("ç”¨æˆ·åˆ›å»ºæˆåŠŸ: username=%s", user.Name)
		return nil
	}

	return nil
}

// åˆ é™¤
func (c *UserConsumer) processDeleteOperation(ctx context.Context, msg kafka.Message) error {

	var deleteRequest struct {
		Username  string `json:"username"`
		DeletedAt string `json:"deleted_at"`
	}

	if err := json.Unmarshal(msg.Value, &deleteRequest); err != nil {
		return c.sendToDeadLetter(ctx, msg, "UNMARSHAL_ERROR: "+err.Error())
	}

	log.Debugf("å¼€å§‹åˆ é™¤ç”¨æˆ·: username=%s", deleteRequest.Username)

	var (
		userID           uint64
		existingSnapshot *v1.User
	)
	if deleteRequest.Username != "" {
		var existing v1.User
		if err := c.db.WithContext(ctx).
			Where("name = ?", deleteRequest.Username).
			First(&existing).Error; err != nil {
			if err != gorm.ErrRecordNotFound {
				return c.sendToRetry(ctx, msg, "æŸ¥è¯¢ç”¨æˆ·å¤±è´¥: "+err.Error())
			}
		} else {
			userID = existing.ID
			existingCopy := existing
			existingSnapshot = &existingCopy
		}
	}

	if err := c.deleteUserFromDB(ctx, deleteRequest.Username); err != nil {
		return c.sendToRetry(ctx, msg, "åˆ é™¤ç”¨æˆ·å¤±è´¥: "+err.Error())
	}

	c.purgeUserState(ctx, deleteRequest.Username, userID, existingSnapshot)
	log.Debugf("ç”¨æˆ·åˆ é™¤æˆåŠŸ: username=%s", deleteRequest.Username)

	return nil
}

func (c *UserConsumer) processUpdateOperation(ctx context.Context, msg kafka.Message) error {
	var user v1.User
	if err := json.Unmarshal(msg.Value, &user); err != nil {
		return c.sendToDeadLetter(ctx, msg, "UNMARSHAL_ERROR: "+err.Error())
	}
	if err := validation.ValidateUserFields(user.Name, user.Nickname, user.Password, user.Email, user.Phone); err != nil {
		return c.sendToDeadLetter(ctx, msg, err.Error())
	}

	log.Debugf("å¤„ç†ç”¨æˆ·æ›´æ–°: username=%s", user.Name)

	var existingSnapshot *v1.User
	var existing v1.User
	if err := c.db.WithContext(ctx).
		Where("name = ?", user.Name).
		First(&existing).Error; err != nil {
		if err == gorm.ErrRecordNotFound {
			return c.sendToDeadLetter(ctx, msg, "UPDATE_TARGET_NOT_FOUND: "+user.Name)
		}
		return c.sendToRetry(ctx, msg, "æŸ¥è¯¢ç”¨æˆ·å¤±è´¥: "+err.Error())
	}
	existingCopy := existing
	existingSnapshot = &existingCopy

	if err := c.updateUserInDB(ctx, &user); err != nil {
		return c.sendToRetry(ctx, msg, "æ›´æ–°ç”¨æˆ·å¤±è´¥: "+err.Error())
	}

	if err := c.setUserCache(ctx, &user, existingSnapshot); err != nil {
		log.Warnf("ç”¨æˆ·æ›´æ–°æˆåŠŸä½†ç¼“å­˜åˆ·æ–°å¤±è´¥: username=%s err=%v", user.Name, err)
	}

	log.Debugf("ç”¨æˆ·æ›´æ–°æˆåŠŸ: username=%s", user.Name)
	return nil
}

func (c *UserConsumer) createUserInDB(ctx context.Context, user *v1.User) error {

	now := time.Now()
	user.CreatedAt = now
	user.UpdatedAt = now

	//	log.Infof("[å•æ¡æ’å…¥] å°è¯•æ’å…¥ç”¨æˆ·: %s", user.Name)
	// æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ c.dbï¼Œåœ¨é›†ç¾¤æ¨¡å¼ä¸‹è¿™æ˜¯ä¸»åº“è¿æ¥
	// åœ¨å•æœºæ¨¡å¼ä¸‹è¿™æ˜¯å”¯ä¸€æ•°æ®åº“è¿æ¥
	if err := c.db.WithContext(ctx).Create(user).Error; err != nil {
		//	metrics.DatabaseQueryErrors.WithLabelValues("create", "users", getErrorType(err)).Inc()
		return fmt.Errorf("æ•°æ®åˆ›å»ºå¤±è´¥: %v", err)
	}
	//	log.Infof("[å•æ¡æ’å…¥] æˆåŠŸ: %s", user.Name)
	return nil
}

func (c *UserConsumer) deleteUserFromDB(ctx context.Context, username string) error {
	result := c.db.WithContext(ctx).
		Where("name = ? ", username).
		Delete(&v1.User{})
	if result.Error != nil {
		return result.Error
	}
	// å…³é”®ï¼šæ£€æŸ¥å®é™…å½±å“è¡Œæ•°
	if result.RowsAffected == 0 {
		return errors.WithCode(code.ErrUserNotFound, "ç”¨æˆ·æ²¡æœ‰å‘ç°")
	}
	return nil
}

func (c *UserConsumer) updateUserInDB(ctx context.Context, user *v1.User) error {
	user.UpdatedAt = time.Now()

	if err := c.db.WithContext(ctx).Model(&v1.User{}).
		Where("name = ?", user.Name).
		Updates(map[string]interface{}{
			"email":     user.Email,
			"password":  user.Password,
			"status":    user.Status,
			"updatedAt": user.UpdatedAt,
		}).Error; err != nil {
		return fmt.Errorf("æ•°æ®åº“æ›´æ–°å¤±è´¥: %v", err)
	}
	return nil
}

// è¾…åŠ©å‡½æ•°
// processMessageWithRetry å¸¦é‡è¯•çš„æ¶ˆæ¯å¤„ç†
func (c *UserConsumer) processMessageWithRetry(ctx context.Context, msg kafka.Message, maxRetries int) error {
	log.Debugf("[Consumer] processMessageWithRetry: key=%s, maxRetries=%d", string(msg.Key), maxRetries)
	var lastErr error

	for attempt := 1; attempt <= maxRetries; attempt++ {
		//log.Debugf("å¼€å§‹ç¬¬%dæ¬¡å¤„ç†æ¶ˆæ¯", attempt)
		err := c.processMessage(ctx, msg)
		if err == nil {
			//	log.Debugf("ç¬¬%dæ¬¡å¤„ç†æˆåŠŸ", attempt)
			return nil // å¤„ç†æˆåŠŸ,è·³å‡ºå¾ªç¯
		}

		lastErr = err

		// æ£€æŸ¥é”™è¯¯ç±»å‹
		if !shouldRetry(err) {
			log.Warn("è¿›å…¥ä¸å¯é‡è¯•å¤„ç†æµç¨‹...")
			return nil //è®¤ä¸ºå¤„ç†å®Œæˆ
		}

		// å¯é‡è¯•é”™è¯¯ï¼šè®°å½•æ—¥å¿—å¹¶ç­‰å¾…é‡è¯•
		log.Warnf("æ¶ˆæ¯å¤„ç†å¤±è´¥ï¼Œå‡†å¤‡é‡è¯• (å°è¯• %d/%d): %v", attempt, maxRetries, err)

		if attempt < maxRetries {
			// æŒ‡æ•°é€€é¿ï¼Œä½†æœ‰ä¸Šé™
			backoff := c.calculateBackoff(attempt)
			log.Debugf("ç­‰å¾… %v åè¿›è¡Œç¬¬%dæ¬¡é‡è¯•", backoff, attempt+1)
			select {
			case <-time.After(backoff):
				// ç»§ç»­é‡è¯•
			case <-ctx.Done():
				return fmt.Errorf("é‡è¯•æœŸé—´ä¸Šä¸‹æ–‡å–æ¶ˆ: %v", ctx.Err())
			}
		}
	}

	// é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œå‘é€åˆ°é‡è¯•ä¸»é¢˜
	log.Errorf("æ¶ˆæ¯å¤„ç†é‡è¯•æ¬¡æ•°ç”¨å°½: %v", lastErr)
	retryErr := c.sendToRetry(ctx, msg, fmt.Sprintf("é‡è¯•æ¬¡æ•°ç”¨å°½: %v", lastErr))
	if retryErr != nil {
		return fmt.Errorf("å‘é€é‡è¯•ä¸»é¢˜å¤±è´¥: %v (åŸé”™è¯¯: %v)", retryErr, lastErr)
	}

	log.Debugf("æ¶ˆæ¯å·²å‘é€åˆ°é‡è¯•ä¸»é¢˜: %s", string(msg.Key))
	return nil // é‡è¯•ä¸»é¢˜å‘é€æˆåŠŸï¼Œè®¤ä¸ºå¤„ç†å®Œæˆ
}

// calculateBackoff è®¡ç®—æŒ‡æ•°é€€é¿å»¶è¿Ÿæ—¶é—´
func (c *UserConsumer) calculateBackoff(attempt int) time.Duration {
	maxBackoff := 30 * time.Second
	minBackoff := 1 * time.Second

	// æŒ‡æ•°é€€é¿å…¬å¼ï¼šbase * 2^(attempt-1)
	backoff := minBackoff * time.Duration(1<<uint(attempt-1))

	// é™åˆ¶æœ€å¤§å»¶è¿Ÿ
	if backoff > maxBackoff {
		return maxBackoff
	}
	return backoff
}

// è®°å½•æ¶ˆè´¹ä¿¡æ¯
func (c *UserConsumer) recordConsumerMetrics(operation, messageKey string, processStart time.Time, processingErr error, workerID int) {
	processingDuration := time.Since(processStart).Seconds()

	// æ·»åŠ è¯¦ç»†çš„å¤„ç†æ—¶é—´æ—¥å¿—
	if processingErr != nil {
		log.Errorf("Worker %d ä¸šåŠ¡å¤„ç†å¤±è´¥: topic=%s, key=%s, operation=%s, å¤„ç†è€—æ—¶=%.3fs, é”™è¯¯=%v",
			workerID, c.topic, messageKey, operation, processingDuration, processingErr)
	} else {
		//	log.Debugf("Worker %d ä¸šåŠ¡å¤„ç†æˆåŠŸ: topic=%s, operation=%s, è€—æ—¶=%.3fs",
		//	workerID, c.topic, operation, processingDuration)
	}

	// è®°å½•æ¶ˆæ¯æ¥æ”¶ï¼ˆæ— è®ºæˆåŠŸå¤±è´¥ï¼‰
	if operation != "" {
		metrics.ConsumerMessagesReceived.WithLabelValues(c.topic, c.groupID, operation).Inc()
	}

	// å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•é”™è¯¯æŒ‡æ ‡
	if processingErr != nil {
		if operation != "" {
			errorType := getErrorType(processingErr)
			metrics.ConsumerProcessingErrors.WithLabelValues(c.topic, c.groupID, operation, errorType).Inc()
			metrics.ConsumerProcessingTime.WithLabelValues(c.topic, c.groupID, operation, "error").Observe(processingDuration)
		}
		return
	}

	// è®°å½•æˆåŠŸå¤„ç†
	if operation != "" {
		metrics.ConsumerMessagesProcessed.WithLabelValues(c.topic, c.groupID, operation).Inc()
		metrics.ConsumerProcessingTime.WithLabelValues(c.topic, c.groupID, operation, "success").Observe(processingDuration)
	}
}

// æ·»åŠ é”™è¯¯ç±»å‹æå–å‡½æ•°
func getErrorType(err error) string {
	if err == nil {
		return "none"
	}
	errStr := err.Error()
	switch {
	case strings.Contains(errStr, "UNMARSHAL_ERROR"):
		return "unmarshal_error"
	case strings.Contains(errStr, "æ•°æ®åº“"):
		return "database_error"
	case strings.Contains(errStr, "ç¼“å­˜"):
		return "cache_error"
	case strings.Contains(errStr, "context deadline exceeded"):
		return "timeout"
	default:
		return "unknown_error"
	}
}

func (c *UserConsumer) getOperationFromHeaders(headers []kafka.Header) string {
	for _, header := range headers {
		if header.Key == HeaderOperation {
			return string(header.Value)
		}
	}
	return OperationCreate
}

func shouldRetry(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()

	// ç¬¬ä¸€å±‚ï¼šæ˜ç¡®ä¸å¯é‡è¯•çš„é”™è¯¯
	if isUnrecoverableError(errStr) {
		return false
	}

	// ç¬¬äºŒå±‚ï¼šæ˜ç¡®å¯é‡è¯•çš„é”™è¯¯
	if isRecoverableError(errStr) {
		return true
	}

	// ç¬¬ä¸‰å±‚ï¼šé»˜è®¤æƒ…å†µ
	return false
}

// isUnrecoverableError åˆ¤æ–­æ˜¯å¦ä¸ºä¸å¯æ¢å¤çš„é”™è¯¯
func isUnrecoverableError(errStr string) bool {
	unrecoverableErrors := []string{
		// æ•°æ®é‡å¤é”™è¯¯
		"Duplicate entry", "1062", "23000", "duplicate key value", "23505",
		"ç”¨æˆ·å·²å­˜åœ¨", "UserAlreadyExist",

		// æ¶ˆæ¯æ ¼å¼é”™è¯¯
		"UNMARSHAL_ERROR", "invalid json", "unknown operation", "poison message",

		// æƒé™å’ŒDEFINERé”™è¯¯
		"definer", "DEFINER", "1449", "permission denied",

		// æ•°æ®ä¸å­˜åœ¨é”™è¯¯ï¼ˆå¹‚ç­‰æ€§ï¼‰
		"does not exist", "not found", "record not found", "ErrRecordNotFound",

		// æ•°æ®åº“çº¦æŸé”™è¯¯
		"constraint", "foreign key", "1451", "1452", "syntax error",

		// å­—æ®µè¶…é•¿é”™è¯¯
		"Data too long for column", "1406",

		// GORM ç›¸å…³ä¸å¯é‡è¯•é”™è¯¯
		"ErrInvalidData", "ErrInvalidTransaction", "ErrNotImplemented", "ErrMissingWhereClause", "ErrPrimaryKeyRequired", "ErrModelValueRequired", "ErrUnsupportedRelation", "ErrRegistered", "ErrInvalidField", "ErrEmptySlice", "ErrDryRunModeUnsupported",

		// ä¸šåŠ¡é€»è¾‘é”™è¯¯
		"invalid format", "validation failed",
	}

	for _, unrecoverableErr := range unrecoverableErrors {
		if strings.Contains(errStr, unrecoverableErr) {
			return true
		}
	}
	return false
}

// isRecoverableError åˆ¤æ–­æ˜¯å¦ä¸ºå¯æ¢å¤çš„é”™è¯¯
func isRecoverableError(errStr string) bool {
	recoverableErrors := []string{
		// è¶…æ—¶å’Œç½‘ç»œé”™è¯¯
		"timeout", "deadline exceeded", "connection refused", "network error",
		"connection reset", "broken pipe", "no route to host",

		// æ•°æ®åº“ä¸´æ—¶é”™è¯¯
		"database is closed", "deadlock", "1213", "40001",
		"temporary", "busy", "lock", "try again",

		// èµ„æºæš‚æ—¶ä¸å¯ç”¨
		"resource temporarily unavailable", "too many connections",

		// GORM å¯é‡è¯•é”™è¯¯
		"ErrInvalidTransaction", "ErrDryRunModeUnsupported",
	}

	for _, recoverableErr := range recoverableErrors {
		if strings.Contains(errStr, recoverableErr) {
			return true
		}
	}
	return false
}

func (c *UserConsumer) setUserCache(ctx context.Context, user *v1.User, previous *v1.User) error {
	startTime := time.Now()
	var operationErr error
	defer func() {
		metrics.RecordRedisOperation("set", time.Since(startTime).Seconds(), operationErr)
	}()

	cacheKey := usercache.UserKey(user.Name)
	data, err := json.Marshal(user)
	if err != nil {
		operationErr = err
		return err
	}
	operationErr = c.redis.SetKey(ctx, cacheKey, string(data), 24*time.Hour)
	if previous != nil {
		c.evictContactCaches(ctx, previous, user)
	}
	c.writeContactCaches(ctx, user)
	return operationErr
}

func (c *UserConsumer) deleteUserCache(ctx context.Context, username string) error {
	cacheKey := usercache.UserKey(username)
	if cacheKey == "" {
		return nil
	}
	if _, err := c.redis.DeleteKey(ctx, cacheKey); err != nil {
		return err
	}
	log.Debugf("åˆ é™¤ç”¨æˆ·%s cacheKey:%sæˆåŠŸ", username, cacheKey)
	return nil
}

func (c *UserConsumer) purgeUserState(ctx context.Context, username string, userID uint64, snapshot *v1.User) {
	if err := c.deleteUserCache(ctx, username); err != nil {
		log.Errorw("ç¼“å­˜åˆ é™¤å¤±è´¥", "username", username, "error", err)
	} else {
		log.Debugf("ç¼“å­˜åˆ é™¤æˆåŠŸ: username=%s", username)
	}

	if snapshot != nil {
		c.evictContactCaches(ctx, snapshot, nil)
	}

	if userID == 0 {
		return
	}

	if err := cleanupUserSessions(ctx, c.redis, userID); err != nil {
		log.Errorw("åˆ·æ–°ä»¤ç‰Œæ¸…ç†å¤±è´¥", "username", username, "userID", userID, "error", err)
		return
	}
	log.Debugf("åˆ·æ–°ä»¤ç‰Œæ¸…ç†æˆåŠŸ: username=%s userID=%d", username, userID)
}

func (c *UserConsumer) evictContactCaches(ctx context.Context, previous *v1.User, current *v1.User) {
	if previous == nil {
		return
	}
	prevEmail := usercache.NormalizeEmail(previous.Email)
	curEmail := ""
	if current != nil {
		curEmail = usercache.NormalizeEmail(current.Email)
	}
	if prevEmail != "" && prevEmail != curEmail {
		c.removeCacheKey(ctx, usercache.EmailKey(previous.Email))
	}

	prevPhone := usercache.NormalizePhone(previous.Phone)
	curPhone := ""
	if current != nil {
		curPhone = usercache.NormalizePhone(current.Phone)
	}
	if prevPhone != "" && prevPhone != curPhone {
		c.removeCacheKey(ctx, usercache.PhoneKey(previous.Phone))
	}
}

func (c *UserConsumer) writeContactCaches(ctx context.Context, user *v1.User) {
	if user == nil {
		return
	}
	if key := usercache.EmailKey(user.Email); key != "" {
		if err := c.redis.SetKey(ctx, key, user.Name, 24*time.Hour); err != nil {
			log.Warnf("é‚®ç®±ç¼“å­˜å†™å…¥å¤±è´¥: username=%s key=%s err=%v", user.Name, key, err)
		}
	}
	if key := usercache.PhoneKey(user.Phone); key != "" {
		if err := c.redis.SetKey(ctx, key, user.Name, 24*time.Hour); err != nil {
			log.Warnf("æ‰‹æœºå·ç¼“å­˜å†™å…¥å¤±è´¥: username=%s key=%s err=%v", user.Name, key, err)
		}
	}
}

func (c *UserConsumer) removeCacheKey(ctx context.Context, cacheKey string) {
	if cacheKey == "" {
		return
	}
	if _, err := c.redis.DeleteKey(ctx, cacheKey); err != nil {
		log.Warnf("ç¼“å­˜åˆ é™¤å¤±è´¥: key=%s err=%v", cacheKey, err)
	}
}

// å‘é€åˆ°é‡è¯•ä¸»é¢˜
func (c *UserConsumer) sendToRetry(ctx context.Context, msg kafka.Message, errorInfo string) error {

	operation := c.getOperationFromHeaders(msg.Headers)

	errorType := getErrorType(fmt.Errorf("%s", errorInfo))
	// è®°å½•é‡è¯•æŒ‡æ ‡
	metrics.ConsumerRetryMessages.WithLabelValues(c.topic, c.groupID, operation, errorType).Inc()

	log.Debugf("ğŸ”„ å‡†å¤‡å‘é€åˆ°é‡è¯•ä¸»é¢˜: key=%s, error=%s", string(msg.Key), errorInfo)
	log.Debugf("  åŸå§‹æ¶ˆæ¯Headers: %+v", msg.Headers)
	if c.producer == nil {
		return fmt.Errorf("produceræœªåˆå§‹åŒ–")
	}

	// âœ… ç¡®ä¿è¿™é‡Œä¼ é€’åŸå§‹æ¶ˆæ¯çš„Headers
	retryMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Headers: msg.Headers, // ç›´æ¥ä½¿ç”¨åŸå§‹Headers
		Time:    time.Now(),
	}

	retryMsg.Headers = append(retryMsg.Headers, kafka.Header{
		Key:   HeaderRetryError,
		Value: []byte(errorInfo),
	})

	return c.producer.sendToRetryTopic(ctx, retryMsg, errorInfo)
}

func (c *UserConsumer) sendToDeadLetter(ctx context.Context, msg kafka.Message, reason string) error {
	operation := c.getOperationFromHeaders(msg.Headers)
	errorType := getErrorType(fmt.Errorf("%s", reason))
	// è®°å½•æ­»ä¿¡æŒ‡æ ‡
	metrics.ConsumerDeadLetterMessages.WithLabelValues(c.topic, c.groupID, operation, errorType).Inc()
	if c.producer == nil {
		return fmt.Errorf("produceræœªåˆå§‹åŒ–")
	}
	return c.producer.SendToDeadLetterTopic(ctx, msg, reason)
}

// ä¿®æ”¹ startLagMonitor æ–¹æ³•
// func (c *UserConsumer) startLagMonitor(ctx context.Context) {
// 	go func() {
// 		ticker := time.NewTicker(c.opts.LagCheckInterval)
// 		defer ticker.Stop()

// 		for {
// 			select {
// 			case <-ticker.C:
// 				// ç›´æ¥è·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸éœ€è¦æ£€æŸ¥ nil
// 				stats := c.reader.Stats()
// 				metrics.ConsumerLag.WithLabelValues(c.topic, c.groupID).Set(float64(stats.Lag))
// 				// 1. æ¯ä¸ªå®ä¾‹å®šæœŸä¸ŠæŠ¥è‡ªå·±çš„ lag åˆ° Redis
// 				instanceKey := fmt.Sprintf("kafka:lag:%s:%s:%d", c.topic, c.groupID, c.instanceID)
// 				err := c.redis.SetKey(ctx, instanceKey, fmt.Sprintf("%d", stats.Lag), 2*c.opts.LagCheckInterval)
// 				if err != nil {
// 					log.Errorf("[LagMonitor] å†™å…¥Rediså¤±è´¥: key=%s, lag=%d, err=%v", instanceKey, stats.Lag, err)
// 				} else {
// 					//		log.Debugf("[LagMonitor] å†™å…¥Redis: key=%s, lag=%d", instanceKey, stats.Lag)
// 				}

// 				// ä¸»æ§é€‰ä¸¾ï¼šç”¨ Redis åˆ†å¸ƒå¼é”ï¼Œé”å®š 2*LagCheckInterval
// 				masterKey := fmt.Sprintf("kafka:lag:master:%s:%s", c.topic, c.groupID)
// 				lockVal := fmt.Sprintf("%d", c.instanceID)
// 				// å°è¯•æŠ¢å ä¸»æ§
// 				gotLock := false
// 				if !c.isMaster {
// 					success, err := c.redis.SetNX(ctx, masterKey, lockVal, 2*c.opts.LagCheckInterval)
// 					if err == nil && success {
// 						c.isMaster = true
// 						gotLock = true
// 						//				log.Debugf("[LagMonitor] æˆä¸ºä¸»æ§: masterKey=%s, val=%s", masterKey, lockVal)
// 					}
// 				} else {
// 					// æ£€æŸ¥è‡ªå·±æ˜¯å¦è¿˜æ˜¯ä¸»æ§
// 					v, err := c.redis.GetKey(ctx, masterKey)
// 					if err == nil && v == lockVal {
// 						gotLock = true
// 					} else {
// 						c.isMaster = false
// 						//						log.Debugf("[LagMonitor] ä¸»æ§å¤±æ•ˆ: masterKey=%s, val=%s, err=%v", masterKey, v, err)

// 					}
// 				}

// 				// å…¨å±€èšåˆæ‰€æœ‰ç›¸å…³ç»„ lag
// 				if gotLock {
// 					groups := []string{"user-service-prod.create", "user-service-prod.update", "user-service-prod.delete"}
// 					totalLag := int64(0)
// 					for _, group := range groups {
// 						keys := c.redis.GetKeys(ctx, fmt.Sprintf("kafka:lag:%s:%s:*", c.topic, group))
// 						for _, k := range keys {
// 							v, err := c.redis.GetKey(ctx, k)
// 							if err == nil {
// 								var lag int64
// 								fmt.Sscanf(v, "%d", &lag)
// 								totalLag += lag
// 							}
// 						}
// 					}
// 					protectTTL := 2 * c.opts.LagCheckInterval
// 					globalProtectKey := "kafka:lag:protect:ALL"
// 					if totalLag >= c.opts.LagScaleThreshold {
// 						_ = c.redis.SetKey(ctx, globalProtectKey, "1", protectTTL)
// 					} else {
// 						_ = c.redis.SetKey(ctx, globalProtectKey, "0", protectTTL)
// 					}
// 					//	log.Warnf("[å…¨å±€ä¿æŠ¤] totalLag=%d, threshold=%d, master=%v", totalLag, c.opts.LagScaleThreshold, c.instanceID)
// 				}

// 				// 3. æ‰€æœ‰å®ä¾‹æ¶ˆè´¹å‰æ£€æŸ¥ä¿æŠ¤ä¿¡å·
// 				v, err := c.redis.GetKey(ctx, "kafka:lag:protect:ALL")
// 				if err == nil && v == "1" {
// 					metrics.ConsumerLag.WithLabelValues(c.topic, c.groupID).Set(1)
// 					// è¿™é‡Œå¯ç›´æ¥ return æˆ– sleepï¼Œé˜»æ–­æ¶ˆè´¹
// 				} else {
// 					metrics.ConsumerLag.WithLabelValues(c.topic, c.groupID).Set(0)
// 				}
// 			case <-ctx.Done():
// 				return
// 			}
// 		}
// 	}()
// }

// batchCreateToDB ä½¿ç”¨ GORM æ‰¹é‡åˆ›å»ºç”¨æˆ·å®ä½“
func (c *UserConsumer) batchCreateToDB(ctx context.Context, msgs []kafka.Message) {
	log.Debugf("[Consumer] batchCreateToDB: msgs=%d", len(msgs))
	if len(msgs) == 0 {
		return
	}
	start := time.Now()
	metrics.BusinessOperationsTotal.WithLabelValues("consumer", "batch_create", "kafka").Inc()
	metrics.BusinessInProgress.WithLabelValues("consumer", "batch_create").Inc()
	defer metrics.BusinessInProgress.WithLabelValues("consumer", "batch_create").Dec()
	var users []v1.User
	for _, m := range msgs {
		var u v1.User
		if err := json.Unmarshal(m.Value, &u); err != nil {
			log.Errorf("æ‰¹é‡åˆ›å»º: ååºåˆ—åŒ–å¤±è´¥: %v", err)
			if c.producer != nil {
				_ = c.producer.SendToDeadLetterTopic(ctx, m, "BATCH_UNMARSHAL_ERROR: "+err.Error())
			}
			continue
		}
		if err := validation.ValidateUserFields(u.Name, u.Nickname, u.Password, u.Email, u.Phone); err != nil {
			log.Errorf("æ‰¹é‡åˆ›å»º: %v", err)
			if c.producer != nil {
				_ = c.producer.SendToDeadLetterTopic(ctx, m, err.Error())
			}
			continue
		}
		now := time.Now()
		u.CreatedAt = now
		u.UpdatedAt = now
		users = append(users, u)
	}

	usernames := make([]string, 0, len(msgs))
	for _, m := range msgs {
		var u v1.User
		if err := json.Unmarshal(m.Value, &u); err == nil {
			usernames = append(usernames, u.Name)
		}
	}
	log.Infof("[æ‰¹é‡æ’å…¥] å°è¯•æ’å…¥ç”¨æˆ·: %v", usernames)

	if len(users) == 0 {
		return
	}
	var opErr error
	if err := c.db.WithContext(ctx).Create(&users).Error; err != nil {
		opErr = err
		log.Errorf("[æ‰¹é‡æ’å…¥] å¤±è´¥: %v, ç”¨æˆ·: %v", err, usernames)
		metrics.BusinessFailures.WithLabelValues("consumer", "batch_create", getErrorType(err)).Inc()
		for _, m := range msgs {
			if c.producer != nil {
				_ = c.producer.sendToRetryTopic(ctx, m, "BATCH_CREATE_DB_ERROR: "+err.Error())
			}
		}
	} else {
		metrics.BusinessSuccess.WithLabelValues("consumer", "batch_create", "success").Inc()
		log.Infof("[æ‰¹é‡æ’å…¥] æˆåŠŸ: %v", usernames)
		log.Debugf("æ‰¹é‡åˆ›å»ºæˆåŠŸ: %d æ¡è®°å½•", len(users))
		// æ‰¹é‡å†™å…¥ç¼“å­˜
		for i := range users {
			if err := c.setUserCache(ctx, &users[i], nil); err != nil {
				log.Warnf("æ‰¹é‡åˆ›å»ºåç¼“å­˜è®¾ç½®å¤±è´¥: username=%s, error=%v", users[i].Name, err)
			} else {
				log.Debugf("æ‰¹é‡åˆ›å»ºåç¼“å­˜æˆåŠŸ: username=%s", users[i].Name)
			}
		}
	}
	duration := time.Since(start).Seconds()
	metrics.BusinessProcessingTime.WithLabelValues("consumer", "batch_create").Observe(duration)
	metrics.BusinessThroughputStats.WithLabelValues("consumer", "batch_create").Observe(duration)
	if opErr != nil {
		errorRate := 1.0
		metrics.BusinessErrorRate.WithLabelValues("consumer", "batch_create").Set(errorRate)
	} else {
		errorRate := 0.0
		metrics.BusinessErrorRate.WithLabelValues("consumer", "batch_create").Set(errorRate)
	}
}

// batchDeleteFromDB æ‰¹é‡åˆ é™¤ç”¨æˆ·ï¼ˆæŒ‰ usernameï¼‰
func (c *UserConsumer) batchDeleteFromDB(ctx context.Context, msgs []kafka.Message) {
	log.Debugf("[Consumer] batchDeleteFromDB: msgs=%d", len(msgs))
	if len(msgs) == 0 {
		return
	}
	start := time.Now()
	metrics.BusinessOperationsTotal.WithLabelValues("consumer", "batch_delete", "kafka").Inc()
	metrics.BusinessInProgress.WithLabelValues("consumer", "batch_delete").Inc()
	defer metrics.BusinessInProgress.WithLabelValues("consumer", "batch_delete").Dec()
	var usernames []string
	cleanupTargets := make(map[string]uint64)
	snapshots := make(map[string]*v1.User)
	for _, m := range msgs {
		var deleteRequest struct {
			Username  string `json:"username"`
			DeletedAt string `json:"deleted_at"`
		}
		if err := json.Unmarshal(m.Value, &deleteRequest); err != nil {
			log.Errorf("æ‰¹é‡åˆ é™¤: ååºåˆ—åŒ–å¤±è´¥: %v", err)
			if c.producer != nil {
				_ = c.producer.SendToDeadLetterTopic(ctx, m, "BATCH_UNMARSHAL_ERROR: "+err.Error())
			}
			continue
		}
		usernames = append(usernames, deleteRequest.Username)
	}
	if len(usernames) == 0 {
		return
	}
	type userIdentifier struct {
		ID    uint64
		Name  string `gorm:"column:name"`
		Email string `gorm:"column:email"`
		Phone string `gorm:"column:phone"`
	}
	var identifiers []userIdentifier
	if err := c.db.WithContext(ctx).
		Model(&v1.User{}).
		Select("id", "name", "email", "phone").
		Where("name IN ?", usernames).
		Find(&identifiers).Error; err != nil {
		log.Warnf("æ‰¹é‡åˆ é™¤å‰æŸ¥è¯¢ç”¨æˆ·IDå¤±è´¥: %v", err)
	} else {
		for _, item := range identifiers {
			cleanupTargets[item.Name] = item.ID
			snapshots[item.Name] = &v1.User{
				Email: item.Email,
				Phone: item.Phone,
			}
		}
	}
	var opErr error
	if err := c.db.WithContext(ctx).Where("name IN ?", usernames).Delete(&v1.User{}).Error; err != nil {
		opErr = err
		log.Errorf("æ‰¹é‡åˆ é™¤ç”¨æˆ·å¤±è´¥: %v", err)
		metrics.BusinessFailures.WithLabelValues("consumer", "batch_delete", getErrorType(err)).Inc()
		for _, m := range msgs {
			if c.producer != nil {
				_ = c.producer.sendToRetryTopic(ctx, m, "BATCH_DELETE_DB_ERROR: "+err.Error())
			}
		}
	} else {
		metrics.BusinessSuccess.WithLabelValues("consumer", "batch_delete", "success").Inc()
		log.Debugf("æ‰¹é‡åˆ é™¤æˆåŠŸ: %d æ¡è®°å½•", len(usernames))
		// æ‰¹é‡åˆ é™¤ç¼“å­˜
		for _, username := range usernames {
			c.purgeUserState(ctx, username, cleanupTargets[username], snapshots[username])
		}
	}
	duration := time.Since(start).Seconds()
	metrics.BusinessProcessingTime.WithLabelValues("consumer", "batch_delete").Observe(duration)
	metrics.BusinessThroughputStats.WithLabelValues("consumer", "batch_delete").Observe(duration)
	if opErr != nil {
		errorRate := 1.0
		metrics.BusinessErrorRate.WithLabelValues("consumer", "batch_delete").Set(errorRate)
	} else {
		errorRate := 0.0
		metrics.BusinessErrorRate.WithLabelValues("consumer", "batch_delete").Set(errorRate)
	}
}

// å”¯ä¸€æ–°å¢çš„æ–¹æ³•
func (c *UserConsumer) SetInstanceID(id int) {
	c.instanceID = id
}

func (c *UserConsumer) SetProducer(producer *UserProducer) {
	c.producer = producer
}

func (c *UserConsumer) Close() error {
	if c.reader != nil {
		return c.reader.Close()
	}
	return nil
}

// batchUpdateToDB æ‰¹é‡æ›´æ–°ç”¨æˆ·ï¼ˆæŒ‰ usernameï¼‰
func (c *UserConsumer) batchUpdateToDB(ctx context.Context, msgs []kafka.Message) {
	log.Debugf("[Consumer] batchUpdateToDB: msgs=%d", len(msgs))
	if len(msgs) == 0 {
		return
	}
	start := time.Now()
	metrics.BusinessOperationsTotal.WithLabelValues("consumer", "batch_update", "kafka").Inc()
	metrics.BusinessInProgress.WithLabelValues("consumer", "batch_update").Inc()
	defer metrics.BusinessInProgress.WithLabelValues("consumer", "batch_update").Dec()
	var opErr error
	var updatedCount int
	for _, m := range msgs {
		var u v1.User
		if err := json.Unmarshal(m.Value, &u); err != nil {
			log.Errorf("æ‰¹é‡æ›´æ–°: ååºåˆ—åŒ–å¤±è´¥: %v", err)
			if c.producer != nil {
				_ = c.producer.SendToDeadLetterTopic(ctx, m, "BATCH_UNMARSHAL_ERROR: "+err.Error())
			}
			continue
		}
		if err := validation.ValidateUserFields(u.Name, u.Nickname, u.Password, u.Email, u.Phone); err != nil {
			log.Errorf("æ‰¹é‡æ›´æ–°: %v", err)
			if c.producer != nil {
				_ = c.producer.SendToDeadLetterTopic(ctx, m, err.Error())
			}
			continue
		}
		u.UpdatedAt = time.Now()
		var existing v1.User
		if err := c.db.WithContext(ctx).
			Where("name = ?", u.Name).
			First(&existing).Error; err != nil {
			if err == gorm.ErrRecordNotFound {
				log.Warnf("æ‰¹é‡æ›´æ–°ç›®æ ‡ä¸å­˜åœ¨: %s", u.Name)
				if c.producer != nil {
					_ = c.producer.SendToDeadLetterTopic(ctx, m, "BATCH_UPDATE_TARGET_NOT_FOUND: "+u.Name)
				}
				continue
			}
			opErr = err
			log.Errorf("æ‰¹é‡æ›´æ–°å‰æŸ¥è¯¢å¤±è´¥: %v, ç”¨æˆ·: %s", err, u.Name)
			metrics.BusinessFailures.WithLabelValues("consumer", "batch_update", getErrorType(err)).Inc()
			if c.producer != nil {
				_ = c.producer.sendToRetryTopic(ctx, m, "BATCH_UPDATE_QUERY_ERROR: "+err.Error())
			}
			continue
		}
		existingCopy := existing
		if err := c.db.WithContext(ctx).Model(&v1.User{}).
			Where("name = ?", u.Name).
			Updates(map[string]interface{}{
				"email":     u.Email,
				"password":  u.Password,
				"status":    u.Status,
				"updatedAt": u.UpdatedAt,
			}).Error; err != nil {
			opErr = err
			log.Errorf("æ‰¹é‡æ›´æ–°å¤±è´¥: %v, ç”¨æˆ·: %s", err, u.Name)
			metrics.BusinessFailures.WithLabelValues("consumer", "batch_update", getErrorType(err)).Inc()
			if c.producer != nil {
				_ = c.producer.sendToRetryTopic(ctx, m, "BATCH_UPDATE_DB_ERROR: "+err.Error())
			}
			continue
		}
		updatedCount++
		metrics.BusinessSuccess.WithLabelValues("consumer", "batch_update", "success").Inc()
		_ = c.setUserCache(ctx, &u, &existingCopy)
	}
	duration := time.Since(start).Seconds()
	metrics.BusinessProcessingTime.WithLabelValues("consumer", "batch_update").Observe(duration)
	metrics.BusinessThroughputStats.WithLabelValues("consumer", "batch_update").Observe(duration)
	if opErr != nil {
		errorRate := 1.0
		metrics.BusinessErrorRate.WithLabelValues("consumer", "batch_update").Set(errorRate)
	} else {
		errorRate := 0.0
		metrics.BusinessErrorRate.WithLabelValues("consumer", "batch_update").Set(errorRate)
	}
	log.Infof("[æ‰¹é‡æ›´æ–°] æˆåŠŸ: %d æ¡è®°å½•", updatedCount)
}
