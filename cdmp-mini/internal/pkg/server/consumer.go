// internal/pkg/server/consumer.go
package server

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"
	"sync"
	"time"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/code"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/metrics"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/log"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/storage"
	v1 "github.com/maxiaolu1981/cretem/nexuscore/api/apiserver/v1"
	"github.com/maxiaolu1981/cretem/nexuscore/errors"
	"github.com/segmentio/kafka-go"
	"gorm.io/gorm"
)

type UserConsumer struct {
	reader     *kafka.Reader
	db         *gorm.DB
	redis      *storage.RedisCluster
	producer   *UserProducer
	topic      string
	groupID    string
	instanceID int // æ–°å¢ï¼šå®ä¾‹ID
}

func NewUserConsumer(brokers []string, topic, groupID string, db *gorm.DB, redis *storage.RedisCluster) *UserConsumer {
	consumer := &UserConsumer{
		reader: kafka.NewReader(kafka.ReaderConfig{
			Brokers: brokers,
			Topic:   topic,
			GroupID: groupID,

			// ä¼˜åŒ–é…ç½®
			MinBytes:      1 * 1024 * 1024,        // é™ä½æœ€å°å­—èŠ‚æ•°ï¼Œç«‹å³æ¶ˆè´¹
			MaxBytes:      10e6,                   // 10MB
			MaxWait:       time.Millisecond * 100, // å¢åŠ åˆ°100ms
			QueueCapacity: 100,                    // é™ä½é˜Ÿåˆ—å®¹é‡ï¼Œé¿å…æ¶ˆæ¯å †ç§¯åœ¨å†…å­˜

			CommitInterval: 0,
			StartOffset:    kafka.FirstOffset,

			// æ·»åŠ é‡è¯•é…ç½®
			MaxAttempts:    3,
			ReadBackoffMin: time.Millisecond * 100,
			ReadBackoffMax: time.Millisecond * 1000,
		}),
		db:      db,
		redis:   redis,
		topic:   topic,
		groupID: groupID,
	}
	go consumer.startLagMonitor(context.Background())
	return consumer
}

// æ¶ˆè´¹
func (c *UserConsumer) StartConsuming(ctx context.Context, workerCount int) {
	var wg sync.WaitGroup
	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			c.worker(ctx, workerID)
		}(i)
	}
	wg.Wait()
}

// æ¶ˆæ¯è°ƒåº¦
func (c *UserConsumer) worker(ctx context.Context, workerID int) {
	log.Debugf("å¯åŠ¨æ¶ˆè´¹è€…å®ä¾‹ %d, Worker %d, Topic: %s, æ¶ˆè´¹ç»„: %s",
		c.instanceID, workerID, c.topic, c.groupID)

	for {
		select {
		case <-ctx.Done():
			log.Debugf("Worker %d: åœæ­¢æ¶ˆè´¹", workerID)
			return

		default:
			log.Debugf("Worker %d: å¼€å§‹å¤„ç†æ¶ˆæ¯", workerID)
			err := c.processSingleMessage(ctx, workerID)

			if err != nil {
				log.Warnf("Worker %d: å¤„ç†æ¶ˆæ¯å¤±è´¥: %v", workerID, err)
				// æ‰€æœ‰é”™è¯¯éƒ½åªæ˜¯ä¼‘çœ åç»§ç»­ï¼Œä¸åœæ­¢workerã€‚ä½¿ç”¨æŒ‡æ•°é€€é¿ä»¥é¿å…ç´§å¾ªç¯ã€‚
				time.Sleep(200 * time.Millisecond)
			} else {
				log.Debugf("Worker %d: æ¶ˆæ¯å¤„ç†å®Œæˆ", workerID)
			}
		}
	}
}

// å¤„ç†æ¶ˆæ¯
func (c *UserConsumer) processSingleMessage(ctx context.Context, workerID int) error {

	var operation, messageKey string
	var processingErr error
	var msg kafka.Message
	var processStart time.Time //ç”¨äºè®°å½•çœŸæ­£çš„å¤„ç†å¼€å§‹æ—¶é—´

	// ä»Kafkaæ‹‰å–æ¶ˆæ¯ï¼Œæ·»åŠ é‡è¯•é€»è¾‘
	var fetchErr error
	for retry := 0; retry < 3; retry++ {
		msg, fetchErr = c.reader.FetchMessage(ctx)
		if fetchErr == nil {
			break
		}

		// æ£€æŸ¥æ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡å–æ¶ˆ
		if errors.Is(fetchErr, context.Canceled) || errors.Is(fetchErr, context.DeadlineExceeded) {
			log.Debugf("Worker %d: ä¸Šä¸‹æ–‡å·²å–æ¶ˆï¼Œåœæ­¢è·å–æ¶ˆæ¯", workerID)
			processingErr = fetchErr
			return fetchErr
		}
		log.Warnf("Worker %d: è·å–æ¶ˆæ¯å¤±è´¥ (é‡è¯• %d/3): %v", workerID, retry+1, fetchErr)
		// æŒ‡æ•°é€€é¿
		backoff := time.Second * time.Duration(1<<uint(retry))
		select {
		case <-time.After(backoff):
			// ç»§ç»­é‡è¯•
		case <-ctx.Done():
			log.Debugf("Worker %d: é‡è¯•æœŸé—´ä¸Šä¸‹æ–‡å–æ¶ˆ", workerID)
			processingErr = ctx.Err()
			return processingErr
		}
	}

	if fetchErr != nil {
		log.Errorf("Worker %d: è·å–æ¶ˆæ¯æœ€ç»ˆå¤±è´¥: %v", workerID, fetchErr)
		processingErr = fetchErr
		return fetchErr
	}

	operation = c.getOperationFromHeaders(msg.Headers)
	messageKey = string(msg.Key)

	// ğŸ”¥ ä»è¿™é‡Œå¼€å§‹è®¡æ—¶çœŸæ­£çš„å¤„ç†æ—¶é—´ï¼ˆä¸åŒ…æ‹¬ç­‰å¾…æ¶ˆæ¯çš„æ—¶é—´
	processStart = time.Now()
	defer func() {
		c.recordConsumerMetrics(operation, messageKey, processStart, processingErr, workerID)
	}()

	// å¤„ç†æ¶ˆæ¯ï¼ˆåŒ…å«ä¸šåŠ¡é€»è¾‘é‡è¯•ï¼‰
	processingErr = c.processMessageWithRetry(ctx, msg, 3)
	if processingErr != nil {
		log.Errorf("Worker %d: æ¶ˆæ¯å¤„ç†æœ€ç»ˆå¤±è´¥: %v", workerID, processingErr)
		return processingErr
	}

	// æäº¤åç§»é‡ï¼ˆç¡®è®¤æ¶ˆè´¹ï¼‰
	if err := c.commitWithRetry(ctx, msg, workerID); err != nil {
		return err
	}
	return nil
}

// commitWithRetry å°è¯•æäº¤æ¶ˆæ¯åç§»ï¼Œé‡åˆ°ä¸´æ—¶é”™è¯¯ä¼šé‡è¯•
func (c *UserConsumer) commitWithRetry(ctx context.Context, msg kafka.Message, workerID int) error {
	maxAttempts := 3
	var lastErr error
	for i := 0; i < maxAttempts; i++ {
		if err := c.reader.CommitMessages(ctx, msg); err != nil {
			lastErr = err
			metrics.ConsumerProcessingErrors.WithLabelValues(c.topic, c.groupID, "commit", "commit_error").Inc()
			log.Warnf("Worker %d: æäº¤åç§»é‡å¤±è´¥ (å°è¯• %d/%d): %v", workerID, i+1, maxAttempts, err)
			// æŒ‡æ•°é€€é¿
			wait := time.Duration(100*(1<<uint(i))) * time.Millisecond
			select {
			case <-time.After(wait):
				continue
			case <-ctx.Done():
				return ctx.Err()
			}
		} else {
			log.Debugf("Worker %d: åç§»é‡æäº¤æˆåŠŸ", workerID)
			return nil
		}
	}
	log.Errorf("Worker %d: æäº¤åç§»é‡æœ€ç»ˆå¤±è´¥: %v", workerID, lastErr)
	return lastErr
}

// ä¸šåŠ¡å¤„ç†
func (c *UserConsumer) processMessage(ctx context.Context, msg kafka.Message) error {
	operation := c.getOperationFromHeaders(msg.Headers)

	switch operation {
	case OperationCreate:
		return c.processCreateOperation(ctx, msg)
	case OperationUpdate:
		return c.processUpdateOperation(ctx, msg)
	case OperationDelete:
		return c.processDeleteOperation(ctx, msg)
	default:
		log.Errorf("æœªçŸ¥æ“ä½œç±»å‹: %s", operation)
		if c.producer != nil {
			return c.producer.SendToDeadLetterTopic(ctx, msg, "UNKNOWN_OPERATION: "+operation)
		}
		return fmt.Errorf("æœªçŸ¥æ“ä½œç±»å‹: %s", operation)
	}
}

func (c *UserConsumer) processCreateOperation(ctx context.Context, msg kafka.Message) error {

	var user v1.User
	if err := json.Unmarshal(msg.Value, &user); err != nil {
		return fmt.Errorf("UNMARSHAL_ERROR: %w", err) // è¿”å›é”™è¯¯ï¼Œè®©ä¸Šå±‚å†³å®šé‡è¯•æˆ–æ­»ä¿¡
	}

	log.Debugf("å¼€å§‹å»ºç«‹ç”¨æˆ·: username=%s", user.Name)

	// åˆ›å»ºç”¨æˆ·
	if err := c.createUserInDB(ctx, &user); err != nil {
		return fmt.Errorf("åˆ›å»ºç”¨æˆ·å¤±è´¥: %w", err) // è¿”å›é”™è¯¯ï¼Œè®©ä¸Šå±‚æ ¹æ®é”™è¯¯ç±»å‹å†³å®š
	}

	// è®¾ç½®ç¼“å­˜
	if err := c.setUserCache(ctx, &user); err != nil {
		log.Warnf("ç”¨æˆ·åˆ›å»ºæˆåŠŸä½†ç¼“å­˜è®¾ç½®å¤±è´¥: username=%s, error=%v", user.Name, err)
		// ç¼“å­˜å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ï¼Œä¸è¿”å›é”™è¯¯
	} else {
		log.Debugf("ç”¨æˆ·%sç¼“å­˜æˆåŠŸ", user.Name)
	}

	log.Debugf("ç”¨æˆ·åˆ›å»ºæˆåŠŸ: username=%s", user.Name)
	return nil
}

// åˆ é™¤
func (c *UserConsumer) processDeleteOperation(ctx context.Context, msg kafka.Message) error {

	var deleteRequest struct {
		Username  string `json:"username"`
		DeletedAt string `json:"deleted_at"`
	}

	if err := json.Unmarshal(msg.Value, &deleteRequest); err != nil {
		return c.sendToDeadLetter(ctx, msg, "UNMARSHAL_ERROR: "+err.Error())
	}

	log.Debugf("å¼€å§‹åˆ é™¤ç”¨æˆ·: username=%s", deleteRequest.Username)

	if err := c.deleteUserFromDB(ctx, deleteRequest.Username); err != nil {
		return c.sendToRetry(ctx, msg, "åˆ é™¤ç”¨æˆ·å¤±è´¥: "+err.Error())
	}

	if err := c.deleteUserCache(ctx, deleteRequest.Username); err != nil {
		log.Errorw("ç¼“å­˜åˆ é™¤å¤±è´¥", "username", deleteRequest.Username, "error", err)
	} else {
		log.Debugf("ç¼“å­˜åˆ é™¤æˆåŠŸ: username=%s", deleteRequest.Username)
	}
	log.Debugf("ç”¨æˆ·åˆ é™¤æˆåŠŸ: username=%s", deleteRequest.Username)

	return nil
}

func (c *UserConsumer) processUpdateOperation(ctx context.Context, msg kafka.Message) error {
	var user v1.User
	if err := json.Unmarshal(msg.Value, &user); err != nil {
		return c.sendToDeadLetter(ctx, msg, "UNMARSHAL_ERROR: "+err.Error())
	}

	log.Debugf("å¤„ç†ç”¨æˆ·æ›´æ–°: username=%s", user.Name)

	if err := c.updateUserInDB(ctx, &user); err != nil {
		return c.sendToRetry(ctx, msg, "æ›´æ–°ç”¨æˆ·å¤±è´¥: "+err.Error())
	}

	c.setUserCache(ctx, &user)

	log.Debugf("ç”¨æˆ·æ›´æ–°æˆåŠŸ: username=%s", user.Name)
	return nil
}

func (c *UserConsumer) createUserInDB(ctx context.Context, user *v1.User) error {

	now := time.Now()
	user.CreatedAt = now
	user.UpdatedAt = now

	// æ³¨æ„ï¼šè¿™é‡Œç›´æ¥ä½¿ç”¨ c.dbï¼Œåœ¨é›†ç¾¤æ¨¡å¼ä¸‹è¿™æ˜¯ä¸»åº“è¿æ¥
	// åœ¨å•æœºæ¨¡å¼ä¸‹è¿™æ˜¯å”¯ä¸€æ•°æ®åº“è¿æ¥
	if err := c.db.WithContext(ctx).Create(user).Error; err != nil {
		//	metrics.DatabaseQueryErrors.WithLabelValues("create", "users", getErrorType(err)).Inc()
		return fmt.Errorf("æ•°æ®åˆ›å»ºå¤±è´¥: %v", err)
	}
	return nil
}

func (c *UserConsumer) deleteUserFromDB(ctx context.Context, username string) error {
	result := c.db.WithContext(ctx).
		Where("name = ? ", username).
		Delete(&v1.User{})
	if result.Error != nil {
		return result.Error
	}
	// å…³é”®ï¼šæ£€æŸ¥å®é™…å½±å“è¡Œæ•°
	if result.RowsAffected == 0 {
		return errors.WithCode(code.ErrUserNotFound, "ç”¨æˆ·æ²¡æœ‰å‘ç°")
	}
	return nil
}

func (c *UserConsumer) updateUserInDB(ctx context.Context, user *v1.User) error {
	user.UpdatedAt = time.Now()

	if err := c.db.WithContext(ctx).Model(&v1.User{}).
		Where("name = ?", user.Name).
		Updates(map[string]interface{}{
			"email":      user.Email,
			"password":   user.Password,
			"status":     user.Status,
			"updated_at": user.UpdatedAt,
		}).Error; err != nil {
		return fmt.Errorf("æ•°æ®åº“æ›´æ–°å¤±è´¥: %v", err)
	}
	return nil
}

// è¾…åŠ©å‡½æ•°
// processMessageWithRetry å¸¦é‡è¯•çš„æ¶ˆæ¯å¤„ç†
func (c *UserConsumer) processMessageWithRetry(ctx context.Context, msg kafka.Message, maxRetries int) error {
	var lastErr error

	for attempt := 1; attempt <= maxRetries; attempt++ {
		log.Debugf("å¼€å§‹ç¬¬%dæ¬¡å¤„ç†æ¶ˆæ¯", attempt)
		err := c.processMessage(ctx, msg)
		if err == nil {
			log.Debugf("ç¬¬%dæ¬¡å¤„ç†æˆåŠŸ", attempt)
			return nil // å¤„ç†æˆåŠŸ,è·³å‡ºå¾ªç¯
		}

		lastErr = err

		// æ£€æŸ¥é”™è¯¯ç±»å‹
		if !shouldRetry(err) {
			log.Warn("è¿›å…¥ä¸å¯é‡è¯•å¤„ç†æµç¨‹...")
			return nil //è®¤ä¸ºå¤„ç†å®Œæˆ
		}

		// å¯é‡è¯•é”™è¯¯ï¼šè®°å½•æ—¥å¿—å¹¶ç­‰å¾…é‡è¯•
		log.Warnf("æ¶ˆæ¯å¤„ç†å¤±è´¥ï¼Œå‡†å¤‡é‡è¯• (å°è¯• %d/%d): %v", attempt, maxRetries, err)

		if attempt < maxRetries {
			// æŒ‡æ•°é€€é¿ï¼Œä½†æœ‰ä¸Šé™
			backoff := c.calculateBackoff(attempt)
			log.Debugf("ç­‰å¾… %v åè¿›è¡Œç¬¬%dæ¬¡é‡è¯•", backoff, attempt+1)
			select {
			case <-time.After(backoff):
				// ç»§ç»­é‡è¯•
			case <-ctx.Done():
				return fmt.Errorf("é‡è¯•æœŸé—´ä¸Šä¸‹æ–‡å–æ¶ˆ: %v", ctx.Err())
			}
		}
	}

	// é‡è¯•æ¬¡æ•°ç”¨å°½ï¼Œå‘é€åˆ°é‡è¯•ä¸»é¢˜
	log.Errorf("æ¶ˆæ¯å¤„ç†é‡è¯•æ¬¡æ•°ç”¨å°½: %v", lastErr)
	retryErr := c.sendToRetry(ctx, msg, fmt.Sprintf("é‡è¯•æ¬¡æ•°ç”¨å°½: %v", lastErr))
	if retryErr != nil {
		return fmt.Errorf("å‘é€é‡è¯•ä¸»é¢˜å¤±è´¥: %v (åŸé”™è¯¯: %v)", retryErr, lastErr)
	}

	log.Debugf("æ¶ˆæ¯å·²å‘é€åˆ°é‡è¯•ä¸»é¢˜: %s", string(msg.Key))
	return nil // é‡è¯•ä¸»é¢˜å‘é€æˆåŠŸï¼Œè®¤ä¸ºå¤„ç†å®Œæˆ
}

// calculateBackoff è®¡ç®—æŒ‡æ•°é€€é¿å»¶è¿Ÿæ—¶é—´
func (c *UserConsumer) calculateBackoff(attempt int) time.Duration {
	maxBackoff := 30 * time.Second
	minBackoff := 1 * time.Second

	// æŒ‡æ•°é€€é¿å…¬å¼ï¼šbase * 2^(attempt-1)
	backoff := minBackoff * time.Duration(1<<uint(attempt-1))

	// é™åˆ¶æœ€å¤§å»¶è¿Ÿ
	if backoff > maxBackoff {
		return maxBackoff
	}
	return backoff
}

// è®°å½•æ¶ˆè´¹ä¿¡æ¯
func (c *UserConsumer) recordConsumerMetrics(operation, messageKey string, processStart time.Time, processingErr error, workerID int) {
	processingDuration := time.Since(processStart).Seconds()

	// æ·»åŠ è¯¦ç»†çš„å¤„ç†æ—¶é—´æ—¥å¿—
	if processingErr != nil {
		log.Errorf("Worker %d ä¸šåŠ¡å¤„ç†å¤±è´¥: topic=%s, key=%s, operation=%s, å¤„ç†è€—æ—¶=%.3fs, é”™è¯¯=%v",
			workerID, c.topic, messageKey, operation, processingDuration, processingErr)
	} else {
		log.Debugf("Worker %d ä¸šåŠ¡å¤„ç†æˆåŠŸ: topic=%s, operation=%s, è€—æ—¶=%.3fs",
			workerID, c.topic, operation, processingDuration)
	}

	// è®°å½•æ¶ˆæ¯æ¥æ”¶ï¼ˆæ— è®ºæˆåŠŸå¤±è´¥ï¼‰
	if operation != "" {
		metrics.ConsumerMessagesReceived.WithLabelValues(c.topic, c.groupID, operation).Inc()
	}

	// å¦‚æœæœ‰é”™è¯¯ï¼Œè®°å½•é”™è¯¯æŒ‡æ ‡
	if processingErr != nil {
		if operation != "" {
			errorType := getErrorType(processingErr)
			metrics.ConsumerProcessingErrors.WithLabelValues(c.topic, c.groupID, operation, errorType).Inc()
			metrics.ConsumerProcessingTime.WithLabelValues(c.topic, c.groupID, operation, "error").Observe(processingDuration)
		}
		return
	}

	// è®°å½•æˆåŠŸå¤„ç†
	if operation != "" {
		metrics.ConsumerMessagesProcessed.WithLabelValues(c.topic, c.groupID, operation).Inc()
		metrics.ConsumerProcessingTime.WithLabelValues(c.topic, c.groupID, operation, "success").Observe(processingDuration)
	}
}

// æ·»åŠ é”™è¯¯ç±»å‹æå–å‡½æ•°
func getErrorType(err error) string {
	if err == nil {
		return "none"
	}
	errStr := err.Error()
	switch {
	case strings.Contains(errStr, "UNMARSHAL_ERROR"):
		return "unmarshal_error"
	case strings.Contains(errStr, "æ•°æ®åº“"):
		return "database_error"
	case strings.Contains(errStr, "ç¼“å­˜"):
		return "cache_error"
	case strings.Contains(errStr, "context deadline exceeded"):
		return "timeout"
	default:
		return "unknown_error"
	}
}

func (c *UserConsumer) getOperationFromHeaders(headers []kafka.Header) string {
	for _, header := range headers {
		if header.Key == HeaderOperation {
			return string(header.Value)
		}
	}
	return OperationCreate
}

func shouldRetry(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()

	// ç¬¬ä¸€å±‚ï¼šæ˜ç¡®ä¸å¯é‡è¯•çš„é”™è¯¯
	if isUnrecoverableError(errStr) {
		return false
	}

	// ç¬¬äºŒå±‚ï¼šæ˜ç¡®å¯é‡è¯•çš„é”™è¯¯
	if isRecoverableError(errStr) {
		return true
	}

	// ç¬¬ä¸‰å±‚ï¼šé»˜è®¤æƒ…å†µ
	return false
}

// isUnrecoverableError åˆ¤æ–­æ˜¯å¦ä¸ºä¸å¯æ¢å¤çš„é”™è¯¯
func isUnrecoverableError(errStr string) bool {
	unrecoverableErrors := []string{
		// æ•°æ®é‡å¤é”™è¯¯
		"Duplicate entry", "1062", "23000", "duplicate key value", "23505",
		"ç”¨æˆ·å·²å­˜åœ¨", "UserAlreadyExist",

		// æ¶ˆæ¯æ ¼å¼é”™è¯¯
		"UNMARSHAL_ERROR", "invalid json", "unknown operation", "poison message",

		// æƒé™å’ŒDEFINERé”™è¯¯
		"definer", "DEFINER", "1449", "permission denied",

		// æ•°æ®ä¸å­˜åœ¨é”™è¯¯ï¼ˆå¹‚ç­‰æ€§ï¼‰
		"does not exist", "not found", "record not found",

		// æ•°æ®åº“çº¦æŸé”™è¯¯
		"constraint", "foreign key", "1451", "1452", "syntax error",

		// ä¸šåŠ¡é€»è¾‘é”™è¯¯
		"invalid format", "validation failed",
	}

	for _, unrecoverableErr := range unrecoverableErrors {
		if strings.Contains(errStr, unrecoverableErr) {
			return false
		}
	}
	return true
}

// isRecoverableError åˆ¤æ–­æ˜¯å¦ä¸ºå¯æ¢å¤çš„é”™è¯¯
func isRecoverableError(errStr string) bool {
	recoverableErrors := []string{
		// è¶…æ—¶å’Œç½‘ç»œé”™è¯¯
		"timeout", "deadline exceeded", "connection refused", "network error",
		"connection reset", "broken pipe", "no route to host",

		// æ•°æ®åº“ä¸´æ—¶é”™è¯¯
		"database is closed", "deadlock", "1213", "40001",
		"temporary", "busy", "lock", "try again",

		// èµ„æºæš‚æ—¶ä¸å¯ç”¨
		"resource temporarily unavailable", "too many connections",
	}

	for _, recoverableErr := range recoverableErrors {
		if strings.Contains(errStr, recoverableErr) {
			return true
		}
	}
	return false
}

func (c *UserConsumer) setUserCache(ctx context.Context, user *v1.User) error {
	startTime := time.Now()
	var operationErr error // ç”¨äºè®°å½•æœ€ç»ˆçš„æ“ä½œé”™è¯¯
	defer func() {
		// ä½¿ç”¨deferç¡®ä¿æ— è®ºä»å“ªä¸ªreturné€€å‡ºéƒ½ä¼šè®°å½•æŒ‡æ ‡
		metrics.RecordRedisOperation("set", time.Since(startTime).Seconds(), operationErr)
	}()

	cacheKey := fmt.Sprintf("user:%s", user.Name)
	data, err := json.Marshal(user)
	if err != nil {
		operationErr = err
		return err
	}
	operationErr = c.redis.SetKey(ctx, cacheKey, string(data), 24*time.Hour)
	return operationErr
}
func (c *UserConsumer) deleteUserCache(ctx context.Context, username string) error {
	cacheKey := fmt.Sprintf("user:%s", username)
	_, err := c.redis.DeleteKey(ctx, cacheKey)
	if err != nil {
		return err
	}
	log.Debugf("åˆ é™¤:%sæˆåŠŸ", cacheKey)
	return nil
}

// å‘é€åˆ°é‡è¯•ä¸»é¢˜
func (c *UserConsumer) sendToRetry(ctx context.Context, msg kafka.Message, errorInfo string) error {

	operation := c.getOperationFromHeaders(msg.Headers)

	errorType := getErrorType(fmt.Errorf("%s", errorInfo))
	// è®°å½•é‡è¯•æŒ‡æ ‡
	metrics.ConsumerRetryMessages.WithLabelValues(c.topic, c.groupID, operation, errorType).Inc()

	log.Debugf("ğŸ”„ å‡†å¤‡å‘é€åˆ°é‡è¯•ä¸»é¢˜: key=%s, error=%s", string(msg.Key), errorInfo)
	log.Debugf("  åŸå§‹æ¶ˆæ¯Headers: %+v", msg.Headers)
	if c.producer == nil {
		return fmt.Errorf("produceræœªåˆå§‹åŒ–")
	}

	// âœ… ç¡®ä¿è¿™é‡Œä¼ é€’åŸå§‹æ¶ˆæ¯çš„Headers
	retryMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Headers: msg.Headers, // ç›´æ¥ä½¿ç”¨åŸå§‹Headers
		Time:    time.Now(),
	}

	retryMsg.Headers = append(retryMsg.Headers, kafka.Header{
		Key:   HeaderRetryError,
		Value: []byte(errorInfo),
	})

	return c.producer.sendToRetryTopic(ctx, retryMsg, errorInfo)
}

func (c *UserConsumer) sendToDeadLetter(ctx context.Context, msg kafka.Message, reason string) error {
	operation := c.getOperationFromHeaders(msg.Headers)
	errorType := getErrorType(fmt.Errorf("%s", reason))
	// è®°å½•æ­»ä¿¡æŒ‡æ ‡
	metrics.ConsumerDeadLetterMessages.WithLabelValues(c.topic, c.groupID, operation, errorType).Inc()
	if c.producer == nil {
		return fmt.Errorf("produceræœªåˆå§‹åŒ–")
	}
	return c.producer.SendToDeadLetterTopic(ctx, msg, reason)
}

// ä¿®æ”¹ startLagMonitor æ–¹æ³•
func (c *UserConsumer) startLagMonitor(ctx context.Context) {
	go func() {
		ticker := time.NewTicker(30 * time.Second)
		defer ticker.Stop()

		for {
			select {
			case <-ticker.C:
				// ç›´æ¥è·å–ç»Ÿè®¡ä¿¡æ¯ï¼Œä¸éœ€è¦æ£€æŸ¥ nil
				stats := c.reader.Stats()
				metrics.ConsumerLag.WithLabelValues(c.topic, c.groupID).Set(float64(stats.Lag))
				// å¯é€‰ï¼šè®°å½•è°ƒè¯•æ—¥å¿—
				if stats.Lag > 0 {
					log.Debugf("æ¶ˆè´¹è€…å»¶è¿Ÿ: topic=%s, group=%s, lag=%d", c.topic, c.groupID, stats.Lag)
				}
			case <-ctx.Done():
				return
			}
		}
	}()
}

// å”¯ä¸€æ–°å¢çš„æ–¹æ³•
func (c *UserConsumer) SetInstanceID(id int) {
	c.instanceID = id
}

func (c *UserConsumer) SetProducer(producer *UserProducer) {
	c.producer = producer
}

func (c *UserConsumer) Close() error {
	if c.reader != nil {
		return c.reader.Close()
	}
	return nil
}
