// internal/pkg/server/retry_consumer.go
package server

import (
	"context"
	"encoding/json"
	"fmt"
	"math/rand"
	"strconv"
	"strings"
	"sync"
	"time"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/metrics"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/options"

	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/log"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/storage"
	v1 "github.com/maxiaolu1981/cretem/nexuscore/api/apiserver/v1"
	"github.com/segmentio/kafka-go"
	"gorm.io/gorm"
)

type RetryConsumer struct {
	reader       *kafka.Reader
	db           *gorm.DB
	redis        *storage.RedisCluster
	producer     *UserProducer
	maxRetries   int
	kafkaOptions *options.KafkaOptions
}

func NewRetryConsumer(db *gorm.DB, redis *storage.RedisCluster, producer *UserProducer, kafkaOptions *options.KafkaOptions, topic, groupid string) *RetryConsumer {
	return &RetryConsumer{
		reader: kafka.NewReader(kafka.ReaderConfig{
			Brokers:        kafkaOptions.Brokers,
			Topic:          topic,
			GroupID:        groupid,
			MinBytes:       10e3,
			MaxBytes:       10e6,
			CommitInterval: 2 * time.Second,
			StartOffset:    kafka.FirstOffset,
		}),
		db:           db,
		redis:        redis,
		producer:     producer,
		maxRetries:   kafkaOptions.MaxRetries,
		kafkaOptions: kafkaOptions,
	}
}

func (rc *RetryConsumer) Close() error {
	if rc.reader != nil {
		return rc.reader.Close()
	}
	return nil
}

func (rc *RetryConsumer) StartConsuming(ctx context.Context, workerCount int) {
	log.Infof("å¯åŠ¨é‡è¯•æ¶ˆè´¹è€…ï¼Œworkeræ•°é‡: %d", workerCount)

	var wg sync.WaitGroup

	// ä¸ºæ¯ä¸ªworkerå¯åŠ¨ä¸€ä¸ªgoroutine
	for i := 0; i < workerCount; i++ {
		wg.Add(1)
		go func(workerID int) {
			defer wg.Done()
			rc.retryWorker(ctx, workerID)
		}(i)
	}

	wg.Wait()
}

func (rc *RetryConsumer) retryWorker(ctx context.Context, workerID int) {
	log.Infof("å¯åŠ¨é‡è¯•æ¶ˆè´¹è€…Worker %d", workerID)

	// æ·»åŠ å¥åº·æ£€æŸ¥è®¡æ•°å™¨
	healthCheckTicker := time.NewTicker(30 * time.Second)
	defer healthCheckTicker.Stop()

	for {
		select {
		case <-ctx.Done():
			log.Infof("é‡è¯•Worker %d: åœæ­¢æ¶ˆè´¹", workerID)
			return
		case <-healthCheckTicker.C:
			stats := rc.reader.Stats()
			log.Infof("Worker %d: æ¶ˆè´¹è€…çŠ¶æ€ - Lag: %d, é”™è¯¯æ•°: %d", workerID, stats.Lag, stats.Errors)

			// æ·»åŠ å‘Šè­¦
			if stats.Lag > 1000 {
				log.Errorf("Worker %d: æ¶ˆè´¹å»¶è¿Ÿè¿‡é«˜! Lag: %d", workerID, stats.Lag)
			}
			if stats.Errors > 10 {
				log.Errorf("Worker %d: é”™è¯¯æ•°è¿‡å¤š! Errors: %d", workerID, stats.Errors)
			}
		default:
			msg, err := rc.reader.FetchMessage(ctx)
			if err != nil {
				log.Errorf("é‡è¯•Worker %d: è·å–æ¶ˆæ¯å¤±è´¥: %v", workerID, err)
				continue
			}

			if err := rc.processRetryMessage(ctx, msg); err != nil {
				log.Errorf("é‡è¯•Worker %d: å¤„ç†æ¶ˆæ¯å¤±è´¥: %v", workerID, err)
				continue
			}

			if err := rc.reader.CommitMessages(ctx, msg); err != nil {
				log.Errorf("é‡è¯•Worker %d: æäº¤åç§»é‡å¤±è´¥: %v", workerID, err)
			}
		}
	}
}

func (rc *RetryConsumer) processRetryMessage(ctx context.Context, msg kafka.Message) error {

	operation := rc.getOperationFromHeaders(msg.Headers)

	switch operation {
	case OperationCreate:
		return rc.processRetryCreate(ctx, msg)
	case OperationUpdate:
		return rc.processRetryUpdate(ctx, msg)
	case OperationDelete:
		return rc.processRetryDelete(ctx, msg)
	default:
		log.Errorf("é‡è¯•æ¶ˆæ¯æœªçŸ¥æ“ä½œç±»å‹: %s", operation)
		return rc.producer.SendToDeadLetterTopic(ctx, msg, "UNKNOWN_OPERATION_IN_RETRY: "+operation)
	}
}

func (rc *RetryConsumer) getOperationFromHeaders(headers []kafka.Header) string {
	for _, header := range headers {
		if header.Key == HeaderOperation {
			return string(header.Value)
		}
	}
	return OperationCreate
}

func (rc *RetryConsumer) parseRetryHeaders(headers []kafka.Header) (int, time.Time, string) {
	var (
		currentRetryCount int = 0
		nextRetryTime     time.Time
		lastError         string = "æœªçŸ¥é”™è¯¯"
	)

	for _, header := range headers {
		switch header.Key {
		case HeaderRetryCount:
			if count, err := strconv.Atoi(string(header.Value)); err == nil {
				currentRetryCount = count
			}
		case HeaderNextRetryTS:
			if t, err := time.Parse(time.RFC3339, string(header.Value)); err == nil {
				nextRetryTime = t
			}
		case HeaderRetryError:
			lastError = string(header.Value)
		}
	}

	// å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸‹æ¬¡é‡è¯•æ—¶é—´ï¼Œè®¾ç½®ä¸€ä¸ªé»˜è®¤å€¼
	if nextRetryTime.IsZero() {
		nextRetryTime = time.Now().Add(10 * time.Second)
		log.Warnf("æœªæ‰¾åˆ°ä¸‹æ¬¡é‡è¯•æ—¶é—´ï¼Œä½¿ç”¨é»˜è®¤å€¼: %v", nextRetryTime)
	}

	return currentRetryCount, nextRetryTime, lastError
}

func (rc *RetryConsumer) processRetryCreate(ctx context.Context, msg kafka.Message) error {
	log.Debugf("å¼€å§‹å¤„ç†é‡è¯•åˆ›å»ºæ¶ˆæ¯: key=%s, headers=%+v", string(msg.Key), msg.Headers)
	var user v1.User
	if err := json.Unmarshal(msg.Value, &user); err != nil {
		log.Errorf("é‡è¯•æ¶ˆæ¯è§£æå¤±è´¥: %v, åŸå§‹æ¶ˆæ¯: %s", err, string(msg.Value))
		return rc.producer.SendToDeadLetterTopic(ctx, msg, "POISON_MESSAGE_IN_RETRY: "+err.Error())
	}

	currentRetryCount, nextRetryTime, lastError := rc.parseRetryHeaders(msg.Headers)

	if currentRetryCount >= rc.maxRetries {
		log.Warnf("åˆ›å»ºæ¶ˆæ¯è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°(%d), å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: username=%s",
			rc.maxRetries, user.Name)
		return rc.producer.SendToDeadLetterTopic(ctx, msg,
			fmt.Sprintf("MAX_RETRIES_EXCEEDED(%d): %s", rc.maxRetries, lastError))
	}

	if time.Now().Before(nextRetryTime) {
		remaining := time.Until(nextRetryTime)
		log.Debugf("ç­‰å¾…é‡è¯•æ—¶é—´åˆ°è¾¾: username=%s, å‰©ä½™=%v", user.Name, remaining)
		select {
		case <-time.After(remaining):
		case <-ctx.Done():
			return ctx.Err()
		}
	}

	if err := rc.createUserInDB(ctx, &user); err != nil {
		return rc.handleProcessingError(ctx, msg, currentRetryCount, "æ£€æŸ¥ç”¨æˆ·å­˜åœ¨æ€§å¤±è´¥: "+err.Error())
	}

	rc.setUserCache(ctx, &user)

	return nil
}

func (rc *RetryConsumer) processRetryUpdate(ctx context.Context, msg kafka.Message) error {
	var user v1.User
	if err := json.Unmarshal(msg.Value, &user); err != nil {
		return rc.producer.SendToDeadLetterTopic(ctx, msg, "POISON_MESSAGE_IN_RETRY: "+err.Error())
	}

	currentRetryCount, nextRetryTime, lastError := rc.parseRetryHeaders(msg.Headers)

	if currentRetryCount >= rc.maxRetries {
		return rc.producer.SendToDeadLetterTopic(ctx, msg,
			fmt.Sprintf("MAX_RETRIES_EXCEEDED(%d): %s", rc.maxRetries, lastError))
	}

	if time.Now().Before(nextRetryTime) {
		time.Sleep(time.Until(nextRetryTime))
	}

	if err := rc.updateUserInDB(ctx, &user); err != nil {
		return rc.handleProcessingError(ctx, msg, currentRetryCount, "é”™è¯¯ä¿¡æ¯: "+err.Error())
	}

	rc.setUserCache(ctx, &user)

	return nil
}

// internal/pkg/server/retry_consumer.go
// ä¿®æ”¹ processRetryDelete æ–¹æ³•
func (rc *RetryConsumer) processRetryDelete(ctx context.Context, msg kafka.Message) error {
	var deleteRequest struct {
		Username  string `json:"username"`
		DeletedAt string `json:"deleted_at"`
	}

	if err := json.Unmarshal(msg.Value, &deleteRequest); err != nil {
		return rc.producer.SendToDeadLetterTopic(ctx, msg, "POISON_MESSAGE_IN_RETRY: "+err.Error())
	}

	currentRetryCount, nextRetryTime, lastError := rc.parseRetryHeaders(msg.Headers)

	if currentRetryCount >= rc.maxRetries {
		return rc.producer.SendToDeadLetterTopic(ctx, msg,
			fmt.Sprintf("MAX_RETRIES_EXCEEDED(%d): %s", rc.maxRetries, lastError))
	}

	// ğŸ”´ ä¼˜åŒ–ï¼šå¦‚æœé‡è¯•æ—¶é—´æœªåˆ°ï¼Œç›´æ¥é‡æ–°å…¥é˜Ÿè€Œä¸æ˜¯é˜»å¡
	if time.Now().Before(nextRetryTime) {
		log.Debugf("é‡è¯•æ—¶é—´æœªåˆ°ï¼Œé‡æ–°å…¥é˜Ÿç­‰å¾…: username=%s, delay=%v",
			deleteRequest.Username, time.Until(nextRetryTime))
		return rc.prepareNextRetry(ctx, msg, currentRetryCount, lastError)
	}

	if err := rc.deleteUserFromDB(ctx, deleteRequest.Username); err != nil {
		// æ£€æŸ¥æ˜¯å¦ä¸ºå¯å¿½ç•¥çš„é”™è¯¯
		if rc.isIgnorableDeleteError(err) {
			log.Warnf("åˆ é™¤æ“ä½œé‡åˆ°å¯å¿½ç•¥é”™è¯¯ï¼Œç›´æ¥æäº¤: username=%s, error=%v",
				deleteRequest.Username, err)
			rc.deleteUserCache(ctx, deleteRequest.Username)
			return nil
		}
		return rc.handleProcessingError(ctx, msg, currentRetryCount, "é”™è¯¯ä¿¡æ¯: "+err.Error())
	}

	if err := rc.deleteUserCache(ctx, deleteRequest.Username); err != nil {
		log.Errorw("é‡è¯•åˆ é™¤æˆåŠŸä½†ç¼“å­˜åˆ é™¤å¤±è´¥", "username", deleteRequest.Username, "error", err)
	}
	return nil
}

// æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦ä¸ºå¯å¿½ç•¥çš„åˆ é™¤é”™è¯¯
func (rc *RetryConsumer) isIgnorableDeleteError(err error) bool {
	if err == nil {
		return false
	}

	errStr := err.Error()
	ignorableErrors := []string{
		"definer",          // DEFINERæƒé™é”™è¯¯
		"does not exist",   // æ•°æ®ä¸å­˜åœ¨
		"not found",        // æ•°æ®ä¸å­˜åœ¨
		"record not found", // æ•°æ®ä¸å­˜åœ¨
		"duplicate entry",  // é‡å¤æ•°æ®
		"1062",             // MySQLé‡å¤é”®
	}

	lowerError := strings.ToLower(errStr)
	for _, ignorableError := range ignorableErrors {
		if strings.Contains(lowerError, ignorableError) {
			return true
		}
	}
	return false
}

// handleProcessingError å¤„ç†é”™è¯¯ï¼Œå†³å®šæ˜¯é‡è¯•è¿˜æ˜¯è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
func (rc *RetryConsumer) handleProcessingError(ctx context.Context, msg kafka.Message, currentRetryCount int, errorInfo string) error {
	// 1. å¯ä»¥ç›´æ¥å¿½ç•¥çš„é”™è¯¯ - ç›´æ¥æäº¤
	if shouldIgnoreError(errorInfo) {
		log.Warnf("å¿½ç•¥é”™è¯¯ï¼Œç›´æ¥æäº¤æ¶ˆæ¯: %s", errorInfo)
		return nil // ç›´æ¥è¿”å›nilï¼Œå¤–å±‚ä¼šæäº¤åç§»é‡
	}

	// 2. éœ€è¦äººå·¥å¹²é¢„çš„é”™è¯¯ - å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—
	if shouldGoToDeadLetterImmediately(errorInfo) {
		log.Warnf("è‡´å‘½é”™è¯¯ï¼Œå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: %s", errorInfo)
		return rc.producer.SendToDeadLetterTopic(ctx, msg, "FATAL_ERROR: "+errorInfo)
	}

	// 3. å¯ä»¥é‡è¯•çš„é”™è¯¯ - å‡†å¤‡ä¸‹æ¬¡é‡è¯•
	return rc.prepareNextRetry(ctx, msg, currentRetryCount, errorInfo)
}

// shouldIgnoreError åˆ¤æ–­æ˜¯å¦å¯ä»¥ç›´æ¥å¿½ç•¥çš„é”™è¯¯
func shouldIgnoreError(errorInfo string) bool {
	ignorableErrors := []string{
		"definer",            // DEFINERæƒé™é”™è¯¯
		"does not exist",     // æ•°æ®ä¸å­˜åœ¨
		"not found",          // æ•°æ®ä¸å­˜åœ¨
		"record not found",   // æ•°æ®ä¸å­˜åœ¨
		"duplicate entry",    // é‡å¤æ•°æ®ï¼ˆå¹‚ç­‰æ€§ï¼‰
		"user already exist", // ç”¨æˆ·å·²å­˜åœ¨
		"1062",               // MySQLé‡å¤é”®é”™è¯¯
	}

	lowerError := strings.ToLower(errorInfo)
	for _, ignorableError := range ignorableErrors {
		if strings.Contains(lowerError, ignorableError) {
			return true
		}
	}
	return false
}

// æ›´æ–° shouldGoToDeadLetterImmediatelyï¼Œç§»é™¤å·²å½’ç±»åˆ°å¿½ç•¥çš„é”™è¯¯
func shouldGoToDeadLetterImmediately(errorInfo string) bool {
	fatalErrors := []string{
		"invalid json",      // æ¶ˆæ¯æ ¼å¼é”™è¯¯
		"unmarshal error",   // ååºåˆ—åŒ–é”™è¯¯
		"unknown operation", // æœªçŸ¥æ“ä½œç±»å‹
		"poison message",    // æ¯’è¯æ¶ˆæ¯
		"permission denied", // çœŸæ­£çš„æƒé™é”™è¯¯ï¼ˆä¸æ˜¯DEFINERï¼‰
	}

	lowerError := strings.ToLower(errorInfo)
	for _, fatalError := range fatalErrors {
		if strings.Contains(lowerError, fatalError) {
			return true
		}
	}
	return false
}

func (rc *RetryConsumer) prepareNextRetry(ctx context.Context, msg kafka.Message, currentRetryCount int, errorInfo string) error {
	// ç¡®ä¿ currentRetryCount ä¸ä¼šå¯¼è‡´è´Ÿæ•°ä½ç§»
	if currentRetryCount < 0 {
		currentRetryCount = 0
	}

	newRetryCount := currentRetryCount + 1

	// æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°
	if newRetryCount > rc.maxRetries {
		log.Warnf("è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°(%d)ï¼Œå‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: error=%s", rc.maxRetries, errorInfo)
		return rc.producer.SendToDeadLetterTopic(ctx, msg,
			fmt.Sprintf("MAX_RETRIES_EXCEEDED(%d): %s", rc.maxRetries, errorInfo))
	}

	// æ ¹æ®é”™è¯¯ç±»å‹å†³å®šé‡è¯•ç­–ç•¥
	var nextRetryDelay time.Duration
	var retryStrategy string

	switch {
	case isDBConnectionError(errorInfo):
		// å®‰å…¨çš„æŒ‡æ•°é€€é¿è®¡ç®—
		baseDelay := rc.kafkaOptions.BaseRetryDelay
		if currentRetryCount > 0 {
			baseDelay = rc.kafkaOptions.BaseRetryDelay * time.Duration(1<<(currentRetryCount-1))
		}
		jitter := time.Duration(rand.Int63n(int64(baseDelay / 2)))
		nextRetryDelay = baseDelay + jitter
		retryStrategy = "æŒ‡æ•°é€€é¿(è¿æ¥é—®é¢˜)"

	case isDBDeadlockError(errorInfo):
		nextRetryDelay = 100 * time.Millisecond
		retryStrategy = "å¿«é€Ÿé‡è¯•(æ­»é”)"

	case isDuplicateKeyError(errorInfo):
		nextRetryDelay = 2 * time.Second
		retryStrategy = "ä¸­ç­‰å»¶è¿Ÿ(é‡å¤é”®)"

	case isTemporaryError(errorInfo):
		// çº¿æ€§å¢é•¿ï¼šn * baseDelay
		nextRetryDelay = time.Duration(currentRetryCount+1) * rc.kafkaOptions.BaseRetryDelay
		retryStrategy = "çº¿æ€§å¢é•¿(ä¸´æ—¶é”™è¯¯)"

	default:
		// é»˜è®¤æŒ‡æ•°é€€é¿ - ç¡®ä¿ä¸ä¼šè´Ÿæ•°ä½ç§»
		baseDelay := rc.kafkaOptions.BaseRetryDelay
		if currentRetryCount > 0 {
			baseDelay = rc.kafkaOptions.BaseRetryDelay * time.Duration(1<<(currentRetryCount-1))
		}
		nextRetryDelay = baseDelay
		retryStrategy = "æŒ‡æ•°é€€é¿(é»˜è®¤)"
	}

	// åº”ç”¨æœ€å¤§å»¶è¿Ÿé™åˆ¶
	if nextRetryDelay > rc.kafkaOptions.MaxRetryDelay {
		nextRetryDelay = rc.kafkaOptions.MaxRetryDelay
	}

	nextRetryTime := time.Now().Add(nextRetryDelay)

	// åˆ›å»ºæ–°çš„headersæ•°ç»„
	newHeaders := make([]kafka.Header, len(msg.Headers))
	copy(newHeaders, msg.Headers)

	// æ›´æ–°é‡è¯•ç›¸å…³çš„headers
	newHeaders = rc.updateOrAddHeader(newHeaders, HeaderRetryCount, strconv.Itoa(newRetryCount))
	newHeaders = rc.updateOrAddHeader(newHeaders, HeaderNextRetryTS, nextRetryTime.Format(time.RFC3339))
	newHeaders = rc.updateOrAddHeader(newHeaders, HeaderRetryError, errorInfo)
	newHeaders = rc.updateOrAddHeader(newHeaders, "retry-strategy", retryStrategy)

	// åˆ›å»ºé‡è¯•æ¶ˆæ¯
	retryMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Headers: newHeaders,
		Time:    time.Now(),
	}

	log.Warnf("å‡†å¤‡ç¬¬%dæ¬¡é‡è¯•[%s]: delay=%v, error=%s",
		newRetryCount, retryStrategy, nextRetryDelay, errorInfo)

	return rc.producer.sendToRetryTopic(ctx, retryMsg, errorInfo)
}

// é”™è¯¯ç±»å‹åˆ¤æ–­å‡½æ•°
func isDBConnectionError(errorInfo string) bool {
	return strings.Contains(errorInfo, "connection") ||
		strings.Contains(errorInfo, "timeout") ||
		strings.Contains(errorInfo, "network") ||
		strings.Contains(errorInfo, "refused")
}

func isDBDeadlockError(errorInfo string) bool {
	return strings.Contains(errorInfo, "deadlock") ||
		strings.Contains(errorInfo, "Deadlock") ||
		strings.Contains(errorInfo, "1213") || // MySQLæ­»é”é”™è¯¯ç 
		strings.Contains(errorInfo, "40001") // SQLServeræ­»é”é”™è¯¯ç 
}

func isDuplicateKeyError(errorInfo string) bool {
	return strings.Contains(errorInfo, "1062") || // MySQLé‡å¤é”®
		strings.Contains(errorInfo, "Duplicate") ||
		strings.Contains(errorInfo, "unique constraint")
}

func isTemporaryError(errorInfo string) bool {
	return strings.Contains(errorInfo, "temporary") ||
		strings.Contains(errorInfo, "busy") ||
		strings.Contains(errorInfo, "lock")
}

func (rc *RetryConsumer) createUserInDB(ctx context.Context, user *v1.User) error {
	now := time.Now()
	user.CreatedAt = now
	user.UpdatedAt = now
	return rc.db.WithContext(ctx).Create(user).Error
}

func (rc *RetryConsumer) updateUserInDB(ctx context.Context, user *v1.User) error {
	user.UpdatedAt = time.Now()
	return rc.db.WithContext(ctx).Model(&v1.User{}).
		Where("name = ?", user.Name).
		Updates(map[string]interface{}{
			"email":      user.Email,
			"password":   user.Password,
			"status":     user.Status,
			"updated_at": user.UpdatedAt,
		}).Error
}

func (rc *RetryConsumer) deleteUserFromDB(ctx context.Context, username string) error {
	return rc.db.WithContext(ctx).
		Where("name = ? and status = 1", username).
		Delete(&v1.User{}).Error
}

func (rc *RetryConsumer) setUserCache(ctx context.Context, user *v1.User) error {
	startTime := time.Now()
	var operationErr error
	defer func() {
		metrics.RecordRedisOperation("set", float64(time.Since(startTime).Seconds()), operationErr)
	}()
	cacheKey := fmt.Sprintf("user:%s", user.Name)
	data, err := json.Marshal(user)
	if err != nil {
		operationErr = err
		log.L(ctx).Errorw("ç”¨æˆ·æ•°æ®åºåˆ—åŒ–å¤±è´¥", "username", user.Name, "error", err)
		return operationErr
	}
	operationErr = rc.redis.SetKey(ctx, cacheKey, string(data), 24*time.Hour)
	if operationErr != nil {
		log.L(ctx).Errorw("ç¼“å­˜å†™å…¥å¤±è´¥", "username", user.Name, "error", operationErr)
	}
	return operationErr
}

func (rc *RetryConsumer) deleteUserCache(ctx context.Context, username string) error {
	cacheKey := fmt.Sprintf("user:%s", username)
	_, err := rc.redis.DeleteKey(ctx, cacheKey)
	return err
}

// è‹¥keyä¸ºç©ºå­—ç¬¦ä¸²ï¼Œç›´æ¥panicï¼ˆç©ºKeyæ— ä¸šåŠ¡æ„ä¹‰ï¼Œä¼šå¯¼è‡´å¤´é€»è¾‘æ··ä¹±ï¼‰
func (rc *RetryConsumer) updateOrAddHeader(headers []kafka.Header, key, value string) []kafka.Header {
	// 1. åŸºç¡€æ ¡éªŒï¼šé˜»æ–­ç©ºKeyè¾“å…¥ï¼ˆé¿å…æ— æ•ˆå¤´ï¼‰
	if key == "" {
		panic("kafka header key cannot be empty string")
	}

	// 2. ç»Ÿä¸€ç›®æ ‡Keyä¸ºå°å†™ï¼ˆç”¨äºå¿½ç•¥å¤§å°å†™åŒ¹é…ï¼Œä¸æ”¹å˜åŸå§‹Keyçš„å±•ç¤ºï¼‰
	targetKeyLower := strings.ToLower(key)

	// 3. åˆå§‹åŒ–æ–°åˆ‡ç‰‡+æ ‡è®°ï¼šnewHeaderså­˜æœ€ç»ˆç»“æœï¼Œfoundæ ‡è®°æ˜¯å¦å·²ä¿ç•™ä¸€ä¸ªç›®æ ‡å¤´
	var newHeaders []kafka.Header
	foundTargetHeader := false

	// 4. éå†åŸå§‹åˆ‡ç‰‡ï¼šé€ä¸ªå¤„ç†æ¯ä¸ªå¤´ï¼Œç­›é€‰é‡å¤ç›®æ ‡å¤´
	for _, header := range headers {
		// 4.1 åˆ¤æ–­å½“å‰å¤´æ˜¯å¦ä¸ºç›®æ ‡å¤´ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
		currentHeaderKeyLower := strings.ToLower(header.Key)
		if currentHeaderKeyLower == targetKeyLower {
			// 4.2 è‹¥æœªä¿ç•™è¿‡ç›®æ ‡å¤´ï¼šæ›´æ–°å…¶Valueï¼ŒåŠ å…¥æ–°åˆ‡ç‰‡ï¼Œæ ‡è®°å·²ä¿ç•™
			if !foundTargetHeader {
				// ä¿ç•™åŸå§‹Keyçš„å¤§å°å†™ï¼ˆä»…æ›´æ–°Valueï¼‰ï¼Œé¿å…ä¿®æ”¹ç”¨æˆ·è¾“å…¥çš„Keyæ ¼å¼
				updatedHeader := kafka.Header{
					Key:   header.Key,    // å¦‚åŸKeyæ˜¯"Retry-Error"ï¼Œä»ä¿ç•™è¯¥æ ¼å¼
					Value: []byte(value), // å†™å…¥æœ€æ–°Value
				}
				newHeaders = append(newHeaders, updatedHeader)
				foundTargetHeader = true // æ ‡è®°å·²ä¿ç•™ï¼Œåç»­é‡å¤å¤´ä¸å†å¤„ç†
			}
			// 4.3 è‹¥å·²ä¿ç•™è¿‡ç›®æ ‡å¤´ï¼šç›´æ¥è·³è¿‡ï¼Œä¸åŠ å…¥æ–°åˆ‡ç‰‡ï¼ˆåˆ é™¤é‡å¤ï¼‰
			continue
		}

		// 4.4 éç›®æ ‡å¤´ï¼šç›´æ¥åŠ å…¥æ–°åˆ‡ç‰‡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
		newHeaders = append(newHeaders, header)
	}

	// 5. è‹¥éå†å®Œæœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡å¤´ï¼šæ–°å¢ä¸€ä¸ªï¼ˆä¿ç•™ç”¨æˆ·è¾“å…¥çš„åŸå§‹Keyå¤§å°å†™ï¼‰
	if !foundTargetHeader {
		newHeaders = append(newHeaders, kafka.Header{
			Key:   key, // å¦‚ç”¨æˆ·ä¼ "Retry-Error"ï¼Œæ–°å¢æ—¶å°±ç”¨è¯¥Keyï¼Œä¸å¼ºåˆ¶å°å†™
			Value: []byte(value),
		})
	}

	return newHeaders
}
