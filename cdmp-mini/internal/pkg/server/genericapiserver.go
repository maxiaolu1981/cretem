package server

import (
	"context"
	stdErrors "errors"
	"os"
	"sync"

	"fmt"
	"net"
	"net/http"
	"path/filepath"
	"strconv"
	"strings"
	"time"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/audit"

	"github.com/gin-gonic/gin"
	"github.com/go-redis/redis/v8"
	"github.com/segmentio/kafka-go"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/apiserver/options"

	mysql "github.com/maxiaolu1981/cretem/cdmp-mini/internal/apiserver/store"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/apiserver/store/interfaces"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/metrics"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/middleware"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/ratelimiter"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/server/producer"
	"github.com/prometheus/client_golang/prometheus"
	dto "github.com/prometheus/client_model/go"

	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/db"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/log"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/storage"
	"github.com/maxiaolu1981/cretem/nexuscore/errors"
	"gorm.io/gorm"
)

type GenericAPIServer struct {
	insecureServer *http.Server
	*gin.Engine
	options        *options.Options
	redis          *storage.RedisCluster
	redisCancel    context.CancelFunc
	initOnce       sync.Once
	producer       producer.MessageProducer
	consumerCtx    context.Context
	consumerCancel context.CancelFunc
	audit          *audit.Manager
	shutdownOnce   sync.Once
}

func (g *GenericAPIServer) isDebugMode() bool {
	return strings.EqualFold(g.options.ServerRunOptions.Mode, gin.DebugMode)
}

func (g *GenericAPIServer) fastDebugStartupEnabled() bool {
	if g == nil || g.options == nil || g.options.ServerRunOptions == nil {
		return false
	}
	return g.isDebugMode() && g.options.ServerRunOptions.FastDebugStartup
}

func (g *GenericAPIServer) shutdownAudit() {
	if g.audit == nil {
		return
	}
	timeout := g.options.AuditOptions.ShutdownTimeout
	if timeout <= 0 {
		timeout = 5 * time.Second
	}
	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()
	if err := g.audit.Shutdown(ctx); err != nil {
		log.Warnf("å®¡è®¡ç®¡ç†å™¨å…³é—­è¶…æ—¶: %v", err)
	}
}

func (g *GenericAPIServer) submitAuditEvent(ctx context.Context, event audit.Event) {
	if g.audit == nil {
		return
	}
	g.audit.Submit(ctx, event)
}

func (g *GenericAPIServer) auditMiddleware() gin.HandlerFunc {
	return func(c *gin.Context) {
		audit.InjectToGinContext(c, g.audit)
		c.Next()
	}
}

func (g *GenericAPIServer) auditServiceEvent(service, stage, outcome string, err error) {
	if g == nil || g.audit == nil {
		return
	}
	event := audit.Event{
		Action:       fmt.Sprintf("%s.%s", service, stage),
		ResourceType: "service",
		ResourceID:   service,
		Target:       service,
		Outcome:      outcome,
		Actor:        "system",
		OccurredAt:   time.Now(),
		Metadata: map[string]any{
			"stage": stage,
		},
	}
	if err != nil {
		event.ErrorMessage = err.Error()
	}
	g.audit.Submit(context.Background(), event)
}

func (g *GenericAPIServer) closeWithAudit(ctx context.Context, service string, fn func(context.Context) error) {
	g.auditServiceEvent(service, "shutdown", "start", nil)
	if fn == nil {
		g.auditServiceEvent(service, "shutdown", "success", nil)
		return
	}
	if ctx == nil {
		ctx = context.Background()
	}
	if err := fn(ctx); err != nil {
		g.auditServiceEvent(service, "shutdown", "fail", err)
	} else {
		g.auditServiceEvent(service, "shutdown", "success", nil)
	}
}

func (g *GenericAPIServer) performShutdown(ctx context.Context) {
	g.shutdownOnce.Do(func() {
		shutdownCtx := ctx
		if shutdownCtx == nil {
			shutdownCtx = context.Background()
		}
		g.closeWithAudit(shutdownCtx, "kafka", g.shutdownKafka)
		g.closeWithAudit(shutdownCtx, "redis", g.shutdownRedis)
		g.closeWithAudit(shutdownCtx, "mysql", g.shutdownMySQL)
		// å®¡è®¡ç®¡ç†å™¨æœ€åå…³é—­ï¼Œé¿å…ä¸¢å¤±å‰é¢çš„å…³é—­äº‹ä»¶
		g.shutdownAudit()
	})
}

func (g *GenericAPIServer) shutdownKafka(ctx context.Context) error {
	var combined error
	if g.consumerCancel != nil {
		g.consumerCancel()
	}
	instances := g.getConsumerInstances()
	if instances != nil {
		for _, consumer := range instances.createConsumers {
			if consumer != nil {
				if err := consumer.Close(); err != nil {
					combined = stdErrors.Join(combined, err)
				}
			}
		}
		for _, consumer := range instances.updateConsumers {
			if consumer != nil {
				if err := consumer.Close(); err != nil {
					combined = stdErrors.Join(combined, err)
				}
			}
		}
		for _, consumer := range instances.deleteConsumers {
			if consumer != nil {
				if err := consumer.Close(); err != nil {
					combined = stdErrors.Join(combined, err)
				}
			}
		}
		for _, consumer := range instances.retryConsumers {
			if consumer != nil {
				if err := consumer.Close(); err != nil {
					combined = stdErrors.Join(combined, err)
				}
			}
		}
	}
	if g.producer != nil {
		if err := g.producer.Close(); err != nil {
			combined = stdErrors.Join(combined, err)
		}
	}
	return combined
}

func (g *GenericAPIServer) shutdownRedis(ctx context.Context) error {
	if g.redisCancel != nil {
		g.redisCancel()
	}
	if g.redis == nil {
		return nil
	}
	client := g.redis.GetClient()
	if client == nil {
		return nil
	}
	return client.Close()
}

func (g *GenericAPIServer) shutdownMySQL(ctx context.Context) error {
	factory := interfaces.Client()
	if factory == nil {
		return nil
	}
	return factory.Close()
}

func NewGenericAPIServer(opts *options.Options) (*GenericAPIServer, error) {
	// åˆå§‹åŒ–æ—¥å¿—
	log.Debugf("æ­£åœ¨åˆå§‹åŒ–GenericAPIServeræœåŠ¡å™¨ï¼Œç¯å¢ƒ: %s", opts.ServerRunOptions.Mode)
	// æ‰“å° Kafka å®ä¾‹ID
	if opts.KafkaOptions != nil {
		log.Infof("[Kafka] å½“å‰å®ä¾‹ InstanceID = %s", opts.KafkaOptions.InstanceID)
	}

	//åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
	g := &GenericAPIServer{
		Engine:   gin.New(),
		options:  opts,
		initOnce: sync.Once{},
	}

	auditMgr, err := audit.NewManager(audit.Config{
		Enabled:         opts.AuditOptions.Enabled,
		BufferSize:      opts.AuditOptions.BufferSize,
		ShutdownTimeout: opts.AuditOptions.ShutdownTimeout,
		LogFile:         opts.AuditOptions.LogFile,
		EnableMetrics:   opts.AuditOptions.EnableMetrics,
	})
	if err != nil {
		log.Errorf("åˆå§‹åŒ–å®¡è®¡ç®¡ç†å™¨å¤±è´¥: %v", err)
	} else {
		g.audit = auditMgr
		// è®°å½•å®¡è®¡æœåŠ¡è‡ªèº«å¯åŠ¨äº‹ä»¶
		g.auditServiceEvent("audit", "startup", "success", nil)
	}

	g.Use(g.auditMiddleware())

	//è®¾ç½®ginè¿è¡Œæ¨¡å¼
	if err := g.configureGin(); err != nil {
		return nil, err
	}
	// åˆå§‹åŒ–mysql
	g.auditServiceEvent("mysql", "startup", "start", nil)
	storeIns, dbIns, err := mysql.GetMySQLFactoryOr(opts.MysqlOptions)
	if err != nil {
		log.Error("mysqlæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
		g.auditServiceEvent("mysql", "startup", "fail", err)
		return nil, err
	}
	interfaces.SetClient(storeIns)
	log.Debug("mysqlæœåŠ¡å™¨åˆå§‹åŒ–æˆåŠŸ")
	g.auditServiceEvent("mysql", "startup", "success", nil)

	// ========== æ–°å¢ï¼šå¢å¼ºç‰ˆé›†ç¾¤çŠ¶æ€æ£€æŸ¥å’Œåˆå§‹åŒ– ==========
	if datastore, ok := storeIns.(*mysql.Datastore); ok {
		if datastore.IsClusterMode() {
			log.Debug("ğŸš€ æ£€æµ‹åˆ°Galeraé›†ç¾¤æ¨¡å¼ï¼Œæ­£åœ¨åˆå§‹åŒ–é›†ç¾¤è¿æ¥...")

			// æ‰§è¡Œé›†ç¾¤å¥åº·æ£€æŸ¥
			if err := initializeGaleraCluster(datastore); err != nil {
				log.Errorf("Galeraé›†ç¾¤åˆå§‹åŒ–è­¦å‘Š: %v", err)
				// ä¸é˜»æ­¢å¯åŠ¨ï¼Œä½†è®°å½•è­¦å‘Š
			}

			// å®šæœŸç›‘æ§é›†ç¾¤çŠ¶æ€ï¼ˆå¯é€‰ï¼‰
			go monitorClusterHealth(datastore, opts.MysqlOptions.HealthCheckInterval)
		} else {
			log.Debug("âœ… ä½¿ç”¨å•èŠ‚ç‚¹MySQLæ¨¡å¼")
		}
	}

	mysqlWait := 30 * time.Second
	if g.fastDebugStartupEnabled() {
		mysqlWait = 5 * time.Second
	}
	if err := waitForMySQLReady(dbIns, mysqlWait); err != nil {
		if !g.fastDebugStartupEnabled() {
			log.Error("mysqlæœåŠ¡å™¨æœªå°±ç»ª")
			g.auditServiceEvent("mysql", "startup", "fail", err)
			return nil, err
		}
		log.Warnf("è°ƒè¯•å¿«é€Ÿå¯åŠ¨: MySQL æœªåœ¨ %v å†…å°±ç»ªï¼Œå°†é™çº§ç»§ç»­å¯åŠ¨ï¼ˆerr=%vï¼‰", mysqlWait, err)
		g.auditServiceEvent("mysql", "startup", "degraded", err)
		go func() {
			if followErr := waitForMySQLReady(dbIns, 30*time.Second); followErr != nil {
				log.Warnf("è°ƒè¯•å¿«é€Ÿå¯åŠ¨: åå°ç­‰å¾… MySQL ä»å¤±è´¥: %v", followErr)
			} else {
				log.Infof("è°ƒè¯•å¿«é€Ÿå¯åŠ¨: MySQL å·²åœ¨åå°å°±ç»ª")
			}
		}()
	}

	//åˆå§‹åŒ–redis
	g.auditServiceEvent("redis", "startup", "start", nil)
	if err := g.initRedisStore(); err != nil {
		log.Error("redisæœåŠ¡å™¨å¯åŠ¨å¤±è´¥")
		g.auditServiceEvent("redis", "startup", "fail", err)
		return nil, err
	}
	log.Debug("redisæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
	g.auditServiceEvent("redis", "startup", "success", nil)
	// ç”Ÿæˆå”¯ä¸€çš„ KAFKA_INSTANCE_ID
	instanceID := os.Getenv("KAFKA_INSTANCE_ID")
	if instanceID == "" {
		host, err := os.Hostname()
		if err != nil {
			host = "unknownhost"
		}
		timestamp := time.Now().UnixNano()
		instanceID = fmt.Sprintf("%s-%d", host, timestamp)
	}
	if opts.KafkaOptions != nil {
		opts.KafkaOptions.InstanceID = instanceID
		log.Infof("[Kafka] è‡ªåŠ¨ç”Ÿæˆå”¯ä¸€ InstanceID = %s", instanceID)
	}
	g.auditServiceEvent("kafka", "startup", "start", nil)
	if err := g.initKafkaComponents(dbIns); err != nil {
		log.Error("kafkaæœåŠ¡å¯åŠ¨å¤±è´¥")
		if !g.fastDebugStartupEnabled() {
			g.auditServiceEvent("kafka", "startup", "fail", err)
			return nil, err
		}
		log.Warnf("è°ƒè¯•å¿«é€Ÿå¯åŠ¨: Kafka åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç©ºç”Ÿäº§è€…ç»§ç»­è¿è¡Œï¼ˆerr=%vï¼‰", err)
		g.auditServiceEvent("kafka", "startup", "degraded", err)
		g.producer = newNoopProducer()
		g.setConsumerInstances(nil, nil, nil, nil)
	} else {
		log.Debug("kafkaæœåŠ¡å™¨å¯åŠ¨æˆåŠŸ")
		g.auditServiceEvent("kafka", "startup", "success", nil)
	}

	// å¯åŠ¨æ¶ˆè´¹è€…
	ctx, cancel := context.WithCancel(context.Background())
	g.consumerCtx = ctx
	g.consumerCancel = cancel

	// è·å–æ‰€æœ‰æ¶ˆè´¹è€…å®ä¾‹
	instances := g.getConsumerInstances()
	var consumerReady sync.WaitGroup
	if instances != nil {
		workerCount := g.options.KafkaOptions.WorkerCount
		if workerCount < 1 {
			workerCount = 1
		}

		// å¯åŠ¨æ‰€æœ‰æ¶ˆè´¹è€…å®ä¾‹ï¼ˆæ¯ä¸ªå®ä¾‹1ä¸ªworkeræˆ–è€…é…ç½®ä¸­çš„æ•°é‡ï¼‰
		for i := 0; i < len(instances.createConsumers); i++ {
			if instances.createConsumers[i] != nil {
				consumerReady.Add(1)
				go instances.createConsumers[i].StartConsuming(ctx, workerCount, &consumerReady)
			}
			if instances.updateConsumers[i] != nil {
				consumerReady.Add(1)
				go instances.updateConsumers[i].StartConsuming(ctx, 1, &consumerReady)
			}
			if instances.deleteConsumers[i] != nil {
				consumerReady.Add(1)
				go instances.deleteConsumers[i].StartConsuming(ctx, 1, &consumerReady)
			}
		}

		// å•ç‹¬å¯åŠ¨é‡è¯•æ¶ˆè´¹è€…çš„æ‰€æœ‰å®ä¾‹ï¼Œä¿è¯é‡è¯•ä¸»é¢˜èƒ½åœ¨æ¶ˆè´¹è€…ç»„ä¸­å‡è¡¡åˆ†é…åˆ†åŒº
		if len(instances.retryConsumers) > 0 {
			// æŸ¥è¯¢ topic åˆ†åŒºæ•°ç”¨äºæŒ‡æ ‡å’Œå¹¶å‘è®¡ç®—
			partitionCount := 0
			brokers := g.options.KafkaOptions.Brokers
			if len(brokers) > 0 {
				retryCtx, retryCancel := context.WithTimeout(ctx, 5*time.Second)
				p, err := getTopicPartitionCount(retryCtx, brokers, UserRetryTopic)
				retryCancel()
				if err == nil {
					partitionCount = p
				} else {
					if stdErrors.Is(err, context.DeadlineExceeded) {
						log.Warnf("è·å– topic %s åˆ†åŒºä¿¡æ¯è¶…æ—¶ï¼Œå°†ç¨åé‡è¯•: %v", UserRetryTopic, err)
					} else {
						log.Warnf("è·å– topic %s åˆ†åŒºä¿¡æ¯å¤±è´¥: %v", UserRetryTopic, err)
					}
				}
			}

			// æ›´æ–° prometheus æŒ‡æ ‡
			retryGroupId := ConsumerGroupPrefix + "-retry"
			metrics.ConsumerTopicPartitions.WithLabelValues(UserRetryTopic).Set(float64(partitionCount))
			metrics.ConsumerGroupInstances.WithLabelValues(retryGroupId).Set(float64(len(instances.retryConsumers)))
			if len(instances.retryConsumers) == 0 {
				metrics.ConsumerPartitionsNoOwner.WithLabelValues(UserRetryTopic, retryGroupId).Set(float64(partitionCount))
			} else {
				// ç®€å•å¯å‘å¼ï¼šå½“æœ‰å®ä¾‹å­˜åœ¨æ—¶ï¼Œè®¤ä¸ºæ— ä¸»åˆ†åŒºä¸º0ï¼ˆæ›´ç²¾ç¡®çš„æ£€æµ‹éœ€è¦ Kafka admin/group æŸ¥è¯¢ï¼‰
				metrics.ConsumerPartitionsNoOwner.WithLabelValues(UserRetryTopic, retryGroupId).Set(0)
			}

			// æ ¹æ®åˆ†åŒºæ•°ä¸å®ä¾‹æ•°è®¡ç®—æ¯ä¸ªå®ä¾‹éœ€è¦çš„ worker æ•°ï¼ˆä¸Šé™ä¸º RetryConsumerWorkersï¼‰
			workersPerInstance := 1
			if partitionCount > 0 && len(instances.retryConsumers) > 0 {
				workersPerInstance = (partitionCount + len(instances.retryConsumers) - 1) / len(instances.retryConsumers)
				if workersPerInstance > RetryConsumerWorkers {
					workersPerInstance = RetryConsumerWorkers
				}
				if workersPerInstance < 1 {
					workersPerInstance = 1
				}
			}

			for i := 0; i < len(instances.retryConsumers); i++ {
				if instances.retryConsumers[i] != nil {
					consumerReady.Add(1)
					go instances.retryConsumers[i].StartConsuming(ctx, workersPerInstance, &consumerReady)
				}
			}

			// å®šæœŸæ›´æ–° topic/å®ä¾‹/æ— ä¸»åˆ†åŒºæŒ‡æ ‡ï¼ˆå¯é…ç½®ï¼‰
			if g.options.KafkaOptions.EnableMetricsRefresh {
				go func() {
					ticker := time.NewTicker(g.options.KafkaOptions.MetricsRefreshInterval)
					defer ticker.Stop()
					for {
						select {
						case <-ctx.Done():
							return
						case <-ticker.C:
							if len(brokers) == 0 {
								continue
							}

							// æ›´ä¸°å¯Œçš„æ—¥å¿—åœ¨ Debug æ¨¡å¼ä¸‹æ‰“å°
							isDebug := g.options.ServerRunOptions.Mode == "debug"

							if p, err := getTopicPartitionCount(ctx, brokers, UserRetryTopic); err == nil {
								metrics.ConsumerTopicPartitions.WithLabelValues(UserRetryTopic).Set(float64(p))
								metrics.ConsumerGroupInstances.WithLabelValues(retryGroupId).Set(float64(len(instances.retryConsumers)))
								if len(instances.retryConsumers) == 0 {
									metrics.ConsumerPartitionsNoOwner.WithLabelValues(UserRetryTopic, retryGroupId).Set(float64(p))
									if isDebug {
										//	log.Debugf("æŒ‡æ ‡åˆ·æ–°: topic %s åˆ†åŒº=%d, instances=%d, noOwner=%d", UserRetryTopic, p, len(instances.retryConsumers), p)
									}
								} else {
									if noOwner, err := getPartitionsWithoutOwner(ctx, brokers, retryGroupId, UserRetryTopic); err == nil {
										metrics.ConsumerPartitionsNoOwner.WithLabelValues(UserRetryTopic, retryGroupId).Set(float64(noOwner))
										if isDebug {
											//			log.Debugf("æŒ‡æ ‡åˆ·æ–°: topic %s åˆ†åŒº=%d, instances=%d, noOwner=%d", UserRetryTopic, p, len(instances.retryConsumers), noOwner)
										}
									} else {
										// å›é€€åˆ°å¯å‘å¼
										metrics.ConsumerPartitionsNoOwner.WithLabelValues(UserRetryTopic, retryGroupId).Set(0)
										//		log.Debugf("å‘¨æœŸæ›´æ–°: æ— æ³•è®¡ç®—æ— ä¸»åˆ†åŒºï¼Œä½¿ç”¨å›é€€å€¼ 0: %v", err)
									}
								}
							} else {
								if g.options.ServerRunOptions.Mode == "debug" {
									//		log.Debugf("å‘¨æœŸæ›´æ–°: æ— æ³•è¯»å– topic %s åˆ†åŒºä¿¡æ¯: %v", UserRetryTopic, err)
								}
							}
						}
					}
				}()
			}
		}
		log.Debugf("å·²å¯åŠ¨ %d ä¸ªæ¶ˆè´¹è€…å®ä¾‹", len(instances.createConsumers))
	}

	consumerReady.Wait()
	// å¦‚æœæˆ‘ä»¬æœªåˆ›å»ºæŒ‰å®ä¾‹å­˜å‚¨ï¼ˆå›é€€æ¨¡å¼ï¼‰ï¼Œå¯åŠ¨å•ä¸ªå…¨å±€é‡è¯•æ¶ˆè´¹è€…

	log.Debug("æ‰€æœ‰Kafkaæ¶ˆè´¹è€…å·²å¯åŠ¨")
	g.printKafkaConfigInfo()

	//å®‰è£…ä¸­é—´ä»¶
	if err := middleware.InstallMiddlewares(g.Engine, opts); err != nil {
		log.Error("ä¸­é—´ä»¶å®‰è£…å¤±è´¥")
		return nil, err
	}
	log.Debug("ä¸­é—´ä»¶å®‰è£…æˆåŠŸ")

	//. å®‰è£…è·¯ç”±
	g.installRoutes()

	return g, nil
}

// ========== æ–°å¢ï¼šé›†ç¾¤å¥åº·ç›‘æ§ ==========
func monitorClusterHealth(datastore *mysql.Datastore, interval time.Duration) {
	if interval <= 0 {
		interval = 30 * time.Second // é»˜è®¤30ç§’
	}

	ticker := time.NewTicker(interval)
	defer ticker.Stop()

	var lastStatus db.ClusterStatus
	unhealthyCount := 0

	for range ticker.C {
		currentStatus := datastore.ClusterStatus()

		// åªåœ¨çŠ¶æ€å˜åŒ–æ—¶è®°å½•
		if currentStatus.PrimaryHealthy != lastStatus.PrimaryHealthy ||
			currentStatus.HealthyReplicas != lastStatus.HealthyReplicas {

			if currentStatus.PrimaryHealthy && currentStatus.HealthyReplicas > 0 {
				log.Debugf("ğŸ“Š é›†ç¾¤çŠ¶æ€: ä¸»èŠ‚ç‚¹å¥åº·ï¼Œ%d/%d å‰¯æœ¬å¯ç”¨",
					currentStatus.HealthyReplicas, currentStatus.ReplicaCount)
				unhealthyCount = 0
			} else if !currentStatus.PrimaryHealthy {
				unhealthyCount++
				log.Errorf("ğŸš¨ é›†ç¾¤å‘Šè­¦: ä¸»èŠ‚ç‚¹ä¸å¯ç”¨ (è¿ç»­%dæ¬¡)", unhealthyCount)
			} else if currentStatus.HealthyReplicas == 0 {
				log.Warn("âš ï¸  é›†ç¾¤è­¦å‘Š: æ— å¯ç”¨å‰¯æœ¬èŠ‚ç‚¹")
			}
		}

		lastStatus = currentStatus

		// å¦‚æœè¿ç»­å¤šæ¬¡æ£€æµ‹åˆ°ä¸»èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œå¯èƒ½éœ€è¦å‘Šè­¦
		if unhealthyCount >= 3 {
			log.Error("ğŸš¨ ä¸¥é‡: é›†ç¾¤ä¸»èŠ‚ç‚¹æŒç»­ä¸å¯ç”¨ï¼Œè¯·ç«‹å³æ£€æŸ¥!")
		}
	}
}

func (g *GenericAPIServer) configureGin() error {
	// è®¾ç½®è¿è¡Œæ¨¡å¼
	gin.SetMode(g.options.ServerRunOptions.Mode)

	// å¼€å‘ç¯å¢ƒé…ç½®
	if g.options.ServerRunOptions.Mode == gin.DebugMode {
		gin.DebugPrintRouteFunc = func(httpMethod, absolutePath, handlerName string, nuHandlers int) {
			log.Debugf("ğŸ“ %-6s %-50s â†’ %s (%d middleware)",
				httpMethod, absolutePath, filepath.Base(handlerName), nuHandlers)
		}
	} else {
		// ç”Ÿäº§ç¯å¢ƒç¦ç”¨è°ƒè¯•è¾“å‡º
		gin.DebugPrintRouteFunc = func(httpMethod, absolutePath, handlerName string, nuHandlers int) {}
	}

	return nil
}

func (g *GenericAPIServer) Run(ctx context.Context) error {
	if ctx == nil {
		ctx = context.Background()
	}
	g.auditServiceEvent("api-server", "startup", "start", nil)

	address := net.JoinHostPort(g.options.InsecureServingOptions.BindAddress, strconv.Itoa(g.options.InsecureServingOptions.BindPort))

	g.insecureServer = &http.Server{
		Addr:              address,
		Handler:           g,
		ReadTimeout:       30 * time.Second,
		ReadHeaderTimeout: 10 * time.Second,
		WriteTimeout:      30 * time.Second,
		IdleTimeout:       30 * time.Second,
		MaxHeaderBytes:    1 << 20,
		ConnState: func(conn net.Conn, state http.ConnState) {
			// é¢„ç•™è¿æ¥çŠ¶æ€ç›‘æ§
		},
	}

	listener, err := net.Listen("tcp", address)
	if err != nil {
		wrapped := fmt.Errorf("åˆ›å»ºç›‘å¬å™¨å¤±è´¥: %w", err)
		g.auditServiceEvent("api-server", "startup", "fail", wrapped)
		g.performShutdown(context.Background())
		return wrapped
	}

	serverErr := make(chan error, 1)
	serverStarted := make(chan struct{})

	go func() {
		close(serverStarted)
		log.Debugf("æ­£åœ¨ %s å¯åŠ¨ GenericAPIServer æœåŠ¡", address)
		if serveErr := g.insecureServer.Serve(listener); serveErr != nil {
			serverErr <- serveErr
			return
		}
		serverErr <- nil
	}()

	select {
	case <-serverStarted:
		g.auditServiceEvent("api-server", "startup", "success", nil)
		log.Debug("GenericAPIServeræœåŠ¡å™¨å·²å¼€å§‹ç›‘å¬ï¼Œå‡†å¤‡è¿›è¡Œå¥åº·æ£€æŸ¥...")
	case <-ctx.Done():
		listener.Close()
		reason := fmt.Errorf("å¯åŠ¨è¢«å–æ¶ˆ: %w", ctx.Err())
		g.auditServiceEvent("api-server", "startup", "fail", reason)
		g.performShutdown(context.Background())
		return ctx.Err()
	case <-time.After(10 * time.Second):
		err := fmt.Errorf("GenericAPIServeræœåŠ¡å™¨å¯åŠ¨è¶…æ—¶ï¼Œæ— æ³•åœ¨10ç§’å†…å¼€å§‹ç›‘å¬")
		g.auditServiceEvent("api-server", "startup", "fail", err)
		_ = g.insecureServer.Close()
		g.performShutdown(context.Background())
		return err
	}

	if g.options.ServerRunOptions.Healthz {
		healthCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
		defer cancel()

		if err := g.waitForPortReady(healthCtx, address, 10*time.Second); err != nil {
			err := fmt.Errorf("ç«¯å£å°±ç»ªæ£€æµ‹å¤±è´¥: %w", err)
			g.auditServiceEvent("api-server", "startup", "fail", err)
			_ = g.insecureServer.Close()
			g.performShutdown(context.Background())
			return err
		}
		if err := g.ping(healthCtx, address); err != nil {
			err := fmt.Errorf("å¥åº·æ£€æŸ¥å¤±è´¥: %w", err)
			g.auditServiceEvent("api-server", "startup", "fail", err)
			_ = g.insecureServer.Close()
			g.performShutdown(context.Background())
			return err
		}
	}

	for {
		select {
		case err := <-serverErr:
			if err != nil && !errors.Is(err, http.ErrServerClosed) {
				wrapped := fmt.Errorf("GenericAPIServeræœåŠ¡å™¨è¿è¡Œå¤±è´¥: %w", err)
				g.auditServiceEvent("api-server", "runtime", "fail", wrapped)
				g.performShutdown(ctx)
				return wrapped
			}
			g.auditServiceEvent("api-server", "shutdown", "success", nil)
			g.performShutdown(ctx)
			if err == nil || errors.Is(err, http.ErrServerClosed) {
				return nil
			}
			return err
		case <-ctx.Done():
			g.auditServiceEvent("api-server", "shutdown", "start", ctx.Err())
			shutdownTimeout := g.options.AuditOptions.ShutdownTimeout
			if shutdownTimeout <= 0 {
				shutdownTimeout = 10 * time.Second
			}
			shutdownCtx, cancel := context.WithTimeout(context.Background(), shutdownTimeout)
			errShutdown := g.insecureServer.Shutdown(shutdownCtx)
			cancel()
			if errShutdown != nil {
				g.auditServiceEvent("api-server", "shutdown", "fail", errShutdown)
			} else {
				g.auditServiceEvent("api-server", "shutdown", "success", nil)
			}
			g.performShutdown(ctx)
			if serveErr := <-serverErr; serveErr != nil && !errors.Is(serveErr, http.ErrServerClosed) {
				return serveErr
			}
			if errShutdown != nil {
				return errShutdown
			}
			return nil
		}
	}
}

// waitForPortReady ç­‰å¾…ç«¯å£å°±ç»ª
func (g *GenericAPIServer) waitForPortReady(ctx context.Context, address string, timeout time.Duration) error {
	deadline := time.Now().Add(timeout)
	log.Debugf("ç­‰å¾…ç«¯å£ %s å°±ç»ªï¼Œè¶…æ—¶æ—¶é—´: %v", address, timeout)

	for attempt := 1; ; attempt++ {
		// æ£€æŸ¥æ˜¯å¦è¶…æ—¶
		if time.Now().After(deadline) {
			return fmt.Errorf("ç«¯å£å°±ç»ªæ£€æµ‹è¶…æ—¶")
		}

		// å°è¯•è¿æ¥ç«¯å£
		conn, err := net.DialTimeout("tcp", address, 100*time.Millisecond)
		if err == nil {
			conn.Close()
			log.Debugf("ç«¯å£ %s å°±ç»ªæ£€æµ‹æˆåŠŸï¼Œå°è¯•æ¬¡æ•°: %d", address, attempt)
			return nil
		}

		// è®°å½•é‡è¯•ä¿¡æ¯ï¼ˆæ¯5æ¬¡å°è¯•è®°å½•ä¸€æ¬¡ï¼‰
		if attempt%5 == 0 {
			log.Debugf("ç«¯å£å°±ç»ªæ£€æµ‹å°è¯• %d: %v", attempt, err)
		}

		// ç­‰å¾…é‡è¯•æˆ–ä¸Šä¸‹æ–‡å–æ¶ˆ
		select {
		case <-ctx.Done():
			return fmt.Errorf("ç«¯å£å°±ç»ªæ£€æµ‹è¢«å–æ¶ˆ: %w", ctx.Err())
		case <-time.After(100 * time.Millisecond):
			// ç»§ç»­é‡è¯•
		}
	}
}

// åˆå§‹åŒ–Kafkaç»„ä»¶
// internal/apiserver/server/server.go

// åˆå§‹åŒ–Kafkaç»„ä»¶ - ä½¿ç”¨optionsä¸­çš„å®Œæ•´é…ç½®
func (g *GenericAPIServer) initKafkaComponents(db *gorm.DB) error {
	kafkaOpts := g.options.KafkaOptions

	// 1. åˆå§‹åŒ–ç”Ÿäº§ç«¯åŠ¨æ€é™é€Ÿå™¨
	// ç»Ÿè®¡å‡½æ•°ï¼šè¿”å›æ€»è¯·æ±‚æ•°å’Œå¤±è´¥æ•°
	getProducerStats := func() (int, int) {
		success := 0.0
		fail := 0.0
		ch := make(chan prometheus.Metric, 100)
		metrics.ProducerSuccess.Collect(ch)
		close(ch)
		for m := range ch {
			var pb dto.Metric
			m.Write(&pb)
			if pb.Counter != nil {
				success += pb.Counter.GetValue()
			}
		}
		ch2 := make(chan prometheus.Metric, 100)
		metrics.ProducerFailures.Collect(ch2)
		close(ch2)
		for m := range ch2 {
			var pb dto.Metric
			m.Write(&pb)
			if pb.Counter != nil {
				fail += pb.Counter.GetValue()
			}
		}
		return int(success + fail), int(fail)
	}

	var rateLimiter *ratelimiter.RateLimiterController
	if g.options.ServerRunOptions.EnableRateLimiter {
		log.Debug("åˆå§‹åŒ–ç”Ÿäº§ç«¯åŠ¨æ€é™é€Ÿå™¨...")
		rateLimiter = ratelimiter.NewRateLimiterController(
			float64(kafkaOpts.StartingRate), // åˆå§‹é€Ÿç‡
			float64(kafkaOpts.MinRate),      // æœ€å°é€Ÿç‡
			float64(kafkaOpts.MaxRate),      // æœ€å¤§é€Ÿç‡
			kafkaOpts.AdjustPeriod,          // è°ƒæ•´å‘¨æœŸ
			getProducerStats,
		)
	} else {
		log.Infof("[Producer] æœªå¯ç”¨é™é€Ÿå™¨ï¼ˆEnableRateLimiter=falseï¼‰")
	}

	log.Debug("åˆå§‹åŒ–Kafkaç”Ÿäº§è€…...")
	userProducer := NewUserProducer(kafkaOpts, rateLimiter)

	// ä¸ºæ¯ä¸ªä¸»é¢˜åˆ›å»ºå¤šä¸ªæ¶ˆè´¹è€…å®ä¾‹
	consumerCount := kafkaOpts.WorkerCount
	retryconsumerCount := kafkaOpts.RetryWorkerCount

	log.Debugf("ä¸ºæ¯ä¸ªä¸»é¢˜åˆ›å»º %d ä¸ªæ¶ˆè´¹è€…å®ä¾‹", consumerCount)

	// åˆ›å»ºæ¶ˆè´¹è€…å®ä¾‹åˆ‡ç‰‡
	createConsumers := make([]*UserConsumer, consumerCount)
	updateConsumers := make([]*UserConsumer, consumerCount)
	deleteConsumers := make([]*UserConsumer, consumerCount)
	retryConsumers := make([]*RetryConsumer, retryconsumerCount)

	for i := 0; i < consumerCount; i++ {
		// æ‰€æœ‰å®ä¾‹ä½¿ç”¨ç›¸åŒçš„æ¶ˆè´¹ç»„IDï¼ˆä¸åŠ åç¼€ï¼‰
		createGroupID := ConsumerGroupPrefix + "-create" // ç›¸åŒçš„ç»„ID
		updateGroupID := ConsumerGroupPrefix + "-update" // ç›¸åŒçš„ç»„ID
		deleteGroupID := ConsumerGroupPrefix + "-delete" // ç›¸åŒçš„ç»„ID

		// åˆ›å»ºæ¶ˆè´¹è€…å®ä¾‹ - ä½¿ç”¨ç›¸åŒçš„æ¶ˆè´¹ç»„ID
		createConsumers[i] = NewUserConsumer(kafkaOpts, UserCreateTopic,
			createGroupID, db, g.redis) // âœ… ç›¸åŒçš„ç»„ID
		createConsumers[i].SetProducer(userProducer)
		createConsumers[i].SetInstanceID(i)
		if g.options.ServerRunOptions.EnableRateLimiter {
			//	go createConsumers[i].startLagMonitor(context.Background())
		}

		updateConsumers[i] = NewUserConsumer(kafkaOpts, UserUpdateTopic,
			updateGroupID, db, g.redis) // âœ… ç›¸åŒçš„ç»„ID
		updateConsumers[i].SetProducer(userProducer)
		updateConsumers[i].SetInstanceID(i)
		if g.options.ServerRunOptions.EnableRateLimiter {
			//	go updateConsumers[i].startLagMonitor(context.Background())
		}

		deleteConsumers[i] = NewUserConsumer(kafkaOpts, UserDeleteTopic,
			deleteGroupID, db, g.redis) // âœ… ç›¸åŒçš„ç»„ID
		deleteConsumers[i].SetProducer(userProducer)
		deleteConsumers[i].SetInstanceID(i)
		if g.options.ServerRunOptions.EnableRateLimiter {
			//	go deleteConsumers[i].startLagMonitor(context.Background())
		}
	}

	log.Debugf("åˆå§‹åŒ–é‡è¯•æ¶ˆè´¹è€…...")
	retryGroupId := ConsumerGroupPrefix + "-retry"
	for i := 0; i < kafkaOpts.RetryWorkerCount; i++ {
		retryConsumers[i] = NewRetryConsumer(db, g.redis, userProducer, kafkaOpts, UserRetryTopic, retryGroupId)
	}
	// 3. èµ‹å€¼åˆ°æœåŠ¡å™¨å®ä¾‹
	g.producer = userProducer

	// 5. å­˜å‚¨æ‰€æœ‰æ¶ˆè´¹è€…å®ä¾‹ï¼ˆæ–°å¢å­—æ®µï¼‰
	g.setConsumerInstances(createConsumers, updateConsumers, deleteConsumers, retryConsumers)

	return nil
}

func (g *GenericAPIServer) monitorRedisConnection(ctx context.Context) {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	for {
		select {
		case <-ctx.Done():
			log.Debug("Redisé›†ç¾¤ç›‘æ§é€€å‡º")
			return
		case <-ticker.C:
			client := g.redis.GetClient()
			if client == nil {
				log.Error("Redisé›†ç¾¤å®¢æˆ·ç«¯ä¸¢å¤±")
				continue
			}

			// å‡å°‘æ—¥å¿—è¾“å‡ºï¼Œåªåœ¨å‡ºé”™æ—¶è®°å½•
			if err := g.pingRedis(ctx, client); err != nil {
				log.Errorf("Redisé›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: %v", err)
			}
			// æˆåŠŸæ—¶ä¸è¾“å‡ºæ—¥å¿—ï¼Œæˆ–è€…æ”¹ä¸ºDebugçº§åˆ«
			// log.Debug("Redisé›†ç¾¤å¥åº·æ£€æŸ¥é€šè¿‡")
		}
	}
}

func (g *GenericAPIServer) ping(ctx context.Context, address string) error {
	host, port, err := net.SplitHostPort(address)
	if err != nil {
		return fmt.Errorf("æ— æ•ˆçš„åœ°å€æ ¼å¼: %w", err)
	}

	if host == "0.0.0.0" {
		host = "127.0.0.1"
	}

	url := fmt.Sprintf("http://%s/healthz", net.JoinHostPort(host, port))
	log.Debugf("å¼€å§‹å¥åº·æ£€æŸ¥ï¼Œç›®æ ‡URL: %s", url)

	attempt := 0

	for {
		attempt++
		if err := ctx.Err(); err != nil {
			return fmt.Errorf("å¥åº·æ£€æŸ¥è¶…æ—¶: %w", err)
		}

		req, err := http.NewRequestWithContext(ctx, http.MethodGet, url, nil)
		if err != nil {
			return fmt.Errorf("åˆ›å»ºè¯·æ±‚å¤±è´¥: %w", err)
		}

		resp, err := http.DefaultClient.Do(req)
		if err != nil {
			if attempt%3 == 0 { // æ¯3æ¬¡å¤±è´¥è®°å½•ä¸€æ¬¡æ—¥å¿—ï¼Œé¿å…æ—¥å¿—è¿‡å¤š
				log.Debugf("å¥åº·æ£€æŸ¥å°è¯• %d å¤±è´¥: %v", attempt, err)
			}
		} else {
			defer resp.Body.Close()

			if resp.StatusCode == http.StatusOK {
				log.Debug("å¥åº·æ£€æŸ¥æˆåŠŸ")
				return nil
			}

			log.Debugf("å¥åº·æ£€æŸ¥å°è¯• %d: çŠ¶æ€ç  %d", attempt, resp.StatusCode)
		}

		select {
		case <-ctx.Done():
			return fmt.Errorf("å¥åº·æ£€æŸ¥è¶…æ—¶: %w", ctx.Err())
		case <-time.After(1 * time.Second):
			// ç»§ç»­é‡è¯•
		}
	}
}

func (g *GenericAPIServer) initRedisStore() error {
	ctx, cancel := context.WithCancel(context.Background())
	g.redisCancel = cancel

	// ğŸ”¥ å¿…é¡»å…ˆåˆå§‹åŒ– g.redisï¼
	g.redis = &storage.RedisCluster{
		KeyPrefix: "genericapiserver:",
		HashKeys:  false,
		IsCache:   false,
	}

	// å¯åŠ¨å¼‚æ­¥è¿æ¥ä»»åŠ¡
	go func() {
		log.Debugf("å¯åŠ¨Redisé›†ç¾¤å¼‚æ­¥è¿æ¥ä»»åŠ¡")
		storage.ConnectToRedis(ctx, g.options.RedisOptions)
		log.Warn("Redisé›†ç¾¤å¼‚æ­¥è¿æ¥ä»»åŠ¡é€€å‡ºï¼ˆå¯èƒ½ä¸Šä¸‹æ–‡å·²å–æ¶ˆï¼‰")
	}()

	// åŒæ­¥ç­‰å¾…Rediså®Œå…¨å¯åŠ¨
	log.Debugf("ç­‰å¾…Redisé›†ç¾¤å®Œå…¨å¯åŠ¨...")

	debugMode := g.fastDebugStartupEnabled()
	basicTimeout := 60 * time.Second
	healthyTimeout := 90 * time.Second
	if debugMode {
		basicTimeout = 5 * time.Second
		healthyTimeout = 10 * time.Second
		log.Debugf("è°ƒè¯•æ¨¡å¼å¯ç”¨å¿«é€Ÿå¯åŠ¨ç­–ç•¥: basicTimeout=%v healthyTimeout=%v", basicTimeout, healthyTimeout)
	}

	basicErr := g.waitForBasicConnection(basicTimeout)
	if basicErr != nil {
		if !debugMode {
			return basicErr
		}
		log.Warnf("è°ƒè¯•æ¨¡å¼: RedisåŸºç¡€è¿æ¥æœªå°±ç»ªï¼Œå°†ç»§ç»­å¯åŠ¨ï¼ˆerr=%vï¼‰", basicErr)
	}

	var healthyErr error
	if basicErr == nil {
		healthyErr = g.waitForHealthyCluster(ctx, healthyTimeout)
		if healthyErr != nil {
			if !debugMode {
				return healthyErr
			}
			log.Warnf("è°ƒè¯•æ¨¡å¼: Rediså¥åº·æ£€æŸ¥æœªé€šè¿‡ï¼Œå°†åœ¨åå°æŒç»­é‡è¯•ï¼ˆerr=%vï¼‰", healthyErr)
		}
	}

	if basicErr == nil && healthyErr == nil {
		log.Debug("âœ… Redisé›†ç¾¤å®Œå…¨å¯åŠ¨å¹¶éªŒè¯æˆåŠŸ")
	} else if debugMode {
		log.Warn("âš ï¸ è°ƒè¯•æ¨¡å¼é™çº§: Rediså°šæœªå®Œå…¨å°±ç»ªï¼Œç›¸å…³åŠŸèƒ½å¯èƒ½å—é™ï¼Œåå°é‡è¿æˆåŠŸåä¼šè‡ªåŠ¨æ¢å¤")
	}

	// å¯åŠ¨ç›‘æ§
	go g.monitorRedisConnection(ctx)
	g.setupRedisClusterMonitoring()

	return nil
}

// ç­‰å¾…é›†ç¾¤å¥åº·çŠ¶æ€ - æ·»åŠ  nil æ£€æŸ¥
func (g *GenericAPIServer) waitForHealthyCluster(ctx context.Context, timeout time.Duration) error {
	deadline := time.Now().Add(timeout)

	for attempt := 1; time.Now().Before(deadline); attempt++ {
		// ğŸ”¥ æ·»åŠ  nil æ£€æŸ¥
		if g.redis == nil {
			log.Warnf("RedisClusterå®ä¾‹ä¸ºç©ºï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
			time.Sleep(2 * time.Second)
			continue
		}

		redisClient := g.redis.GetClient()
		if redisClient != nil {
			if err := g.pingRedis(ctx, redisClient); err == nil {
				log.Debugf("Redisé›†ç¾¤å¥åº·æ£€æŸ¥é€šè¿‡ï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
				return nil
			}
		}

		if attempt%2 == 0 {
			log.Debugf("ç­‰å¾…Redisé›†ç¾¤å¥åº·æ£€æŸ¥...ï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
		}
		time.Sleep(2 * time.Second)
	}

	return fmt.Errorf("Redisé›†ç¾¤å¥åº·æ£€æŸ¥è¶…æ—¶ï¼ˆ%vï¼‰", timeout)
}

// ç­‰å¾…åŸºç¡€è¿æ¥å»ºç«‹ - æ·»åŠ  nil æ£€æŸ¥
func (g *GenericAPIServer) waitForBasicConnection(timeout time.Duration) error {
	deadline := time.Now().Add(timeout)

	for attempt := 1; time.Now().Before(deadline); attempt++ {
		// ğŸ”¥ æ·»åŠ  nil æ£€æŸ¥
		if g.redis == nil {
			log.Warnf("RedisClusterå®ä¾‹ä¸ºç©ºï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
			time.Sleep(1 * time.Second)
			continue
		}

		if storage.Connected() && g.redis.GetClient() != nil {
			log.Debugf("âœ… RedisåŸºç¡€è¿æ¥å»ºç«‹ï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
			return nil
		}

		if attempt%3 == 0 {
			log.Debugf("ç­‰å¾…RedisåŸºç¡€è¿æ¥...ï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
		}
		time.Sleep(1 * time.Second)
	}

	return fmt.Errorf("RedisåŸºç¡€è¿æ¥å»ºç«‹è¶…æ—¶ï¼ˆ%vï¼‰", timeout)
}

// setupRedisClusterMonitoring è®¾ç½®Redisé›†ç¾¤ç›‘æ§
func (g *GenericAPIServer) setupRedisClusterMonitoring() {
	// ä»Redisé…ç½®ä¸­è·å–é›†ç¾¤èŠ‚ç‚¹åœ°å€
	nodes := g.options.RedisOptions.Addrs
	if len(nodes) == 0 {
		// å¦‚æœæ²¡æœ‰é…ç½®é›†ç¾¤åœ°å€ï¼Œä½¿ç”¨é»˜è®¤çš„å•èŠ‚ç‚¹åœ°å€
		nodes = []string{fmt.Sprintf("%s:%d", g.options.RedisOptions.Host, g.options.RedisOptions.Port)}
	}

	log.Debugf("å¯åŠ¨Redisé›†ç¾¤ç›‘æ§ï¼ŒèŠ‚ç‚¹: %v", nodes)

	// åˆ›å»ºé›†ç¾¤ç›‘æ§å™¨
	monitor := metrics.NewRedisClusterMonitor(
		"generic_api_server_cluster", // é›†ç¾¤åç§°
		nodes,                        // é›†ç¾¤èŠ‚ç‚¹åœ°å€
		30*time.Second,               // æ¯30ç§’é‡‡é›†ä¸€æ¬¡
	)

	// å¯åŠ¨ç›‘æ§
	go monitor.Start(context.Background())

	log.Debug("âœ… Redisé›†ç¾¤ç›‘æ§å·²å¯åŠ¨")
}

// pingRedis æ”¯æŒredis.UniversalClientç±»å‹
func (g *GenericAPIServer) pingRedis(ctx context.Context, client redis.UniversalClient) error {
	pingCtx, cancel := context.WithTimeout(ctx, 10*time.Second)
	defer cancel()

	// æ£€æŸ¥é›†ç¾¤çŠ¶æ€
	if clusterClient, ok := client.(*redis.ClusterClient); ok {
		// é›†ç¾¤æ¨¡å¼ï¼šæ£€æŸ¥é›†ç¾¤ä¿¡æ¯
		clusterInfo, err := clusterClient.ClusterInfo(pingCtx).Result()
		if err != nil {
			return fmt.Errorf("é›†ç¾¤çŠ¶æ€æ£€æŸ¥å¤±è´¥: %v", err)
		}

		// æ‰§è¡Œ CLUSTER NODES å‘½ä»¤è·å–å®Œæ•´é›†ç¾¤ä¿¡æ¯
		clusterNodes, err := clusterClient.ClusterNodes(pingCtx).Result()
		if err != nil {
			log.Warnf("æ‰§è¡ŒCLUSTER NODESå¤±è´¥: %v", err)
		} else {
			//log.Infof("=== Redisé›†ç¾¤èŠ‚ç‚¹è¯¦æƒ… ===")
			//log.Infof("é…ç½®çš„èŠ‚ç‚¹æ•°é‡: %d", len(g.options.RedisOptions.Addrs))
			//log.Infof("é…ç½®çš„èŠ‚ç‚¹åˆ—è¡¨: %v", g.options.RedisOptions.Addrs)

			lines := strings.Split(clusterNodes, "\n")
			actualNodeCount := 0
			for _, line := range lines {
				if strings.TrimSpace(line) != "" {
					actualNodeCount++
					//		log.Infof("èŠ‚ç‚¹ %d: %s", actualNodeCount, strings.TrimSpace(line))
				}
			}
			//		log.Infof("å®é™…å‘ç°çš„èŠ‚ç‚¹æ•°é‡: %d", actualNodeCount)
		}

		// è§£æé›†ç¾¤ä¿¡æ¯
		//log.Infof("=== Redisé›†ç¾¤çŠ¶æ€ ===")
		infoLines := strings.Split(clusterInfo, "\n")
		for _, line := range infoLines {
			if strings.Contains(line, ":") {
				//			log.Infof("  %s", strings.TrimSpace(line))
			}
		}

		// ğŸ”¥ ä¿®æ”¹ï¼šåŒæ—¶æ£€æŸ¥ä¸»èŠ‚ç‚¹å’Œä»èŠ‚ç‚¹
		var lastError error
		masterCount := 0
		slaveCount := 0
		successCount := 0

		// æ£€æŸ¥æ‰€æœ‰ä¸»èŠ‚ç‚¹
		clusterClient.ForEachMaster(pingCtx, func(ctx context.Context, nodeClient *redis.Client) error {
			masterCount++
			if err := nodeClient.Ping(ctx).Err(); err != nil {
				log.Warnf("ä¸»èŠ‚ç‚¹ %d PING å¤±è´¥: %v", masterCount, err)
				lastError = err
			} else {
				//			log.Infof("âœ… ä¸»èŠ‚ç‚¹ %d PING æˆåŠŸ", masterCount)
				successCount++
			}
			return nil
		})

		// æ£€æŸ¥æ‰€æœ‰ä»èŠ‚ç‚¹
		err = clusterClient.ForEachSlave(pingCtx, func(ctx context.Context, nodeClient *redis.Client) error {
			slaveCount++
			if err := nodeClient.Ping(ctx).Err(); err != nil {
				log.Warnf("ä»èŠ‚ç‚¹ %d PING å¤±è´¥: %v", slaveCount, err)
				lastError = err
			} else {
				//		log.Infof("âœ… ä»èŠ‚ç‚¹ %d PING æˆåŠŸ", slaveCount)
				successCount++
			}
			return nil
		})

		totalNodes := masterCount + slaveCount

		// log.Infof("=== Redisé›†ç¾¤å¥åº·æ£€æŸ¥æ€»ç»“ ===")
		// log.Infof("ä¸»èŠ‚ç‚¹æ•°: %d, ä»èŠ‚ç‚¹æ•°: %d", masterCount, slaveCount)
		// log.Infof("æ€»èŠ‚ç‚¹æ•°: %d, æˆåŠŸèŠ‚ç‚¹: %d, å¤±è´¥èŠ‚ç‚¹: %d", totalNodes, successCount, totalNodes-successCount)

		if successCount == 0 {
			return fmt.Errorf("æ‰€æœ‰é›†ç¾¤èŠ‚ç‚¹PINGæ£€æŸ¥å¤±è´¥")
		}

		// ğŸ”¥ ä¿®æ”¹ï¼šæ£€æŸ¥æ˜¯å¦æ‰€æœ‰é…ç½®çš„èŠ‚ç‚¹éƒ½è¢«å‘ç°
		expectedNodes := len(g.options.RedisOptions.Addrs)
		if totalNodes != expectedNodes {
			log.Warnf("âš ï¸  èŠ‚ç‚¹æ•°é‡ä¸åŒ¹é…: é…ç½®%dä¸ª, é›†ç¾¤ä¸­å‘ç°%dä¸ª", expectedNodes, totalNodes)
		} else {
			//		log.Infof("âœ… èŠ‚ç‚¹æ•°é‡åŒ¹é…: %dä¸ª", totalNodes)
		}

		if successCount < totalNodes {
			log.Warnf("éƒ¨åˆ†èŠ‚ç‚¹è¿æ¥å¼‚å¸¸ (%d/%d æˆåŠŸ)", successCount, totalNodes)
		} else {
			//		log.Infof("âœ… æ‰€æœ‰èŠ‚ç‚¹è¿æ¥æ­£å¸¸")
		}

		// å¦‚æœè‡³å°‘æœ‰ä¸€ä¸ªèŠ‚ç‚¹æ­£å¸¸ï¼Œè®¤ä¸ºé›†ç¾¤å¯ç”¨
		if err != nil && lastError != nil {
			return fmt.Errorf("é›†ç¾¤èŠ‚ç‚¹æ£€æŸ¥å¼‚å¸¸: %v", lastError)
		}

		return nil
	}

	// å•æœºæ¨¡å¼æˆ–æ™®é€šå®¢æˆ·ç«¯
	return client.Ping(pingCtx).Err()
}

// æ‰“å°Kafkaé…ç½®ä¿¡æ¯
func (g *GenericAPIServer) printKafkaConfigInfo() {
	kafkaOpts := g.options.KafkaOptions
	instances := g.getConsumerInstances()
	instanceCount := 1
	if instances != nil {
		instanceCount = len(instances.createConsumers)
	}

	log.Debugf("ğŸ“Š Kafkaé…ç½®ä¿¡æ¯:")
	log.Debugf("  è¿è¡Œæ¨¡å¼: %s", g.options.ServerRunOptions.Mode)
	log.Debugf("  Brokers: %v", kafkaOpts.Brokers)
	log.Debugf("  ä¸»é¢˜é…ç½®:")
	log.Debugf("    - åˆ›å»º: %s (%dä¸ªæ¶ˆè´¹è€…å®ä¾‹)", UserCreateTopic, instanceCount)
	log.Debugf("    - æ›´æ–°: %s (%dä¸ªæ¶ˆè´¹è€…å®ä¾‹)", UserUpdateTopic, instanceCount)
	log.Debugf("    - åˆ é™¤: %s (%dä¸ªæ¶ˆè´¹è€…å®ä¾‹)", UserDeleteTopic, instanceCount)
	log.Debugf("    - é‡è¯•: %s", UserRetryTopic)
	log.Debugf("  é…ç½®å‚æ•°:")
	log.Debugf("    - æœ€å¤§é‡è¯•: %d", kafkaOpts.MaxRetries)
	log.Debugf("    - æ¶ˆè´¹è€…å®ä¾‹æ•°é‡: %d", instanceCount)
	log.Debugf("    - æ‰¹é‡å¤§å°: %d", kafkaOpts.BatchSize)
	log.Debugf("    - æ‰¹é‡è¶…æ—¶: %v", kafkaOpts.BatchTimeout)
}

// æ–°å¢ï¼šå­˜å‚¨æ‰€æœ‰æ¶ˆè´¹è€…å®ä¾‹
type consumerInstances struct {
	createConsumers []*UserConsumer
	updateConsumers []*UserConsumer
	deleteConsumers []*UserConsumer
	retryConsumers  []*RetryConsumer
}

var consumerInstancesStore = &consumerInstances{}

func (g *GenericAPIServer) setConsumerInstances(create, update, delete []*UserConsumer, retry []*RetryConsumer) {
	consumerInstancesStore.createConsumers = create
	consumerInstancesStore.updateConsumers = update
	consumerInstancesStore.deleteConsumers = delete
	consumerInstancesStore.retryConsumers = retry

}

func (g *GenericAPIServer) getConsumerInstances() *consumerInstances {
	return consumerInstancesStore
}

func waitForMySQLReady(db *gorm.DB, timeout time.Duration) error {
	if db == nil {
		return fmt.Errorf("mysqlæ•°æ®åº“è¿æ¥ä¸ºç©º")
	}

	ctx, cancel := context.WithTimeout(context.Background(), timeout)
	defer cancel()

	ticker := time.NewTicker(1 * time.Second)
	defer ticker.Stop()

	attempt := 0

	for {
		err := db.WithContext(ctx).Exec("SELECT 1").Error
		attempt++
		if err == nil {
			log.Debugf("MySQLå°±ç»ªï¼ˆå°è¯• %d æ¬¡ï¼‰", attempt)
			return nil
		}

		if ctx.Err() != nil {
			return fmt.Errorf("MySQLå°±ç»ªæ£€æŸ¥è¶…æ—¶: %w", ctx.Err())
		}

		if attempt%3 == 0 {
			log.Debugf("ç­‰å¾…MySQLå°±ç»ª...ï¼ˆå°è¯• %d æ¬¡, é”™è¯¯: %vï¼‰", attempt, err)
		}

		select {
		case <-ctx.Done():
			return fmt.Errorf("MySQLå°±ç»ªæ£€æŸ¥è¢«å–æ¶ˆ: %w", ctx.Err())
		case <-ticker.C:
		}
	}
}

// ========== æ–°å¢ï¼šé›†ç¾¤åˆå§‹åŒ–å‡½æ•° ==========
func initializeGaleraCluster(datastore *mysql.Datastore) error {
	maxRetries := 20                 // æœ€å¤§é‡è¯•æ¬¡æ•°
	retryInterval := 2 * time.Second // é‡è¯•é—´éš”

	for attempt := 1; attempt <= maxRetries; attempt++ {
		status := datastore.ClusterStatus()

		log.Debugf("ğŸ” é›†ç¾¤å¥åº·æ£€æŸ¥ [%d/%d]: ä¸»èŠ‚ç‚¹=%v, å‰¯æœ¬=%d/%d å¥åº·",
			attempt, maxRetries, status.PrimaryHealthy, status.HealthyReplicas, status.ReplicaCount)

		// æ£€æŸ¥é›†ç¾¤å¥åº·æ¡ä»¶
		if status.PrimaryHealthy {
			if status.HealthyReplicas >= 1 {
				// ç†æƒ³çŠ¶æ€ï¼šä¸»èŠ‚ç‚¹å¥åº·ä¸”è‡³å°‘1ä¸ªå‰¯æœ¬å¥åº·
				log.Debugf("âœ… Galeraé›†ç¾¤çŠ¶æ€è‰¯å¥½: ä¸»èŠ‚ç‚¹å¥åº·ï¼Œ%dä¸ªå‰¯æœ¬èŠ‚ç‚¹å¯ç”¨", status.HealthyReplicas)
				return nil
			} else if status.HealthyReplicas == 0 {
				// åªæœ‰ä¸»èŠ‚ç‚¹å¥åº·ï¼ˆå¯èƒ½æ˜¯å•èŠ‚ç‚¹é›†ç¾¤æˆ–å‰¯æœ¬èŠ‚ç‚¹æ•…éšœï¼‰
				log.Warn("âš ï¸  Galeraé›†ç¾¤ä¸»èŠ‚ç‚¹å¥åº·ï¼Œä½†å‰¯æœ¬èŠ‚ç‚¹ä¸å¯ç”¨")
				return nil // ä»ç„¶ç»§ç»­å¯åŠ¨
			}
		}

		if attempt < maxRetries {
			log.Debugf("â³ é›†ç¾¤æœªå°±ç»ªï¼Œ%våé‡è¯•...", retryInterval)
			time.Sleep(retryInterval)
		}
	}

	// æœ€ç»ˆæ£€æŸ¥
	finalStatus := datastore.ClusterStatus()
	if !finalStatus.PrimaryHealthy {
		return fmt.Errorf("Galeraé›†ç¾¤ä¸»èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥é›†ç¾¤çŠ¶æ€")
	}

	log.Warn("âš ï¸  Galeraé›†ç¾¤éƒ¨åˆ†èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œä½†æœåŠ¡å°†ç»§ç»­å¯åŠ¨")
	return nil
}

// getTopicPartitionCount returns the number of partitions for the given topic using kafka.Client.Metadata
func getTopicPartitionCount(ctx context.Context, brokers []string, topic string) (int, error) {
	if len(brokers) == 0 {
		return 0, fmt.Errorf("no brokers provided")
	}

	admin := &kafka.Client{Addr: kafka.TCP(brokers...)}
	metadata, err := admin.Metadata(ctx, &kafka.MetadataRequest{Topics: []string{topic}})
	if err != nil {
		return 0, err
	}

	for _, t := range metadata.Topics {
		if t.Name == topic {
			return len(t.Partitions), nil
		}
	}
	return 0, fmt.Errorf("topic %s not found in metadata", topic)
}

// getPartitionsWithoutOwner queries the consumer group and topic metadata to compute the number
// of partitions of 'topic' that are not currently assigned to any member of the consumer group.
// It uses kafka.Client to fetch Metadata and DescribeGroups.
func getPartitionsWithoutOwner(ctx context.Context, brokers []string, groupID, topic string) (int, error) {
	if len(brokers) == 0 {
		return 0, fmt.Errorf("no brokers provided")
	}

	admin := &kafka.Client{Addr: kafka.TCP(brokers...)}

	// 1) è·å– topic partitions
	metadata, err := admin.Metadata(ctx, &kafka.MetadataRequest{Topics: []string{topic}})
	if err != nil {
		return 0, fmt.Errorf("metadata error: %w", err)
	}
	var topicMeta *kafka.Topic
	for _, t := range metadata.Topics {
		if t.Name == topic {
			topicMeta = &t
			break
		}
	}
	if topicMeta == nil {
		return 0, fmt.Errorf("topic %s not found", topic)
	}
	totalPartitions := len(topicMeta.Partitions)

	// 2) Describe group to get member assignments
	describeResp, err := admin.DescribeGroups(ctx, &kafka.DescribeGroupsRequest{GroupIDs: []string{groupID}})
	if err != nil {
		return 0, fmt.Errorf("describe groups error: %w", err)
	}
	if len(describeResp.Groups) == 0 {
		// æ²¡æœ‰æˆå‘˜ï¼Œæ‰€æœ‰åˆ†åŒºéƒ½æ²¡æœ‰ owner
		return totalPartitions, nil
	}

	// Collect partitions that are owned by members (for the topic)
	owned := make(map[int]struct{})
	for _, g := range describeResp.Groups {
		for _, member := range g.Members {
			// Use MemberAssignments (Topics/Partitions)
			for _, t := range member.MemberAssignments.Topics {
				if t.Topic != topic {
					continue
				}
				for _, p := range t.Partitions {
					owned[p] = struct{}{}
				}
			}
			// Also include OwnedPartitions from MemberMetadata for cooperative assignor
			for _, op := range member.MemberMetadata.OwnedPartitions {
				if op.Topic != topic {
					continue
				}
				for _, p := range op.Partitions {
					owned[p] = struct{}{}
				}
			}
		}
	}

	// If we couldn't find owned partitions via DescribeGroups parsing, fallback to 0 ownership (conservative)
	if len(owned) == 0 {
		// Fallback: use ConsumerOffsets (deprecated helper) to see committed offsets for group/topic
		if offs, err := admin.ConsumerOffsets(ctx, kafka.TopicAndGroup{Topic: topic, GroupId: groupID}); err == nil {
			for pid := range offs {
				owned[pid] = struct{}{}
			}
		}
	}

	// Count partitions without owner
	noOwner := 0
	for _, p := range topicMeta.Partitions {
		if _, ok := owned[p.ID]; !ok {
			noOwner++
		}
	}

	return noOwner, nil
}
