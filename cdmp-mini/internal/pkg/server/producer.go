// internal/pkg/server/producer.go
package server

import (
	"context"
	"encoding/binary"
	"encoding/json"
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/code"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/metrics"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/options"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/server/producer"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/log"
	v1 "github.com/maxiaolu1981/cretem/nexuscore/api/apiserver/v1"
	"github.com/maxiaolu1981/cretem/nexuscore/errors"
	"github.com/segmentio/kafka-go"
)

var _ producer.MessageProducer = (*UserProducer)(nil)
var KafkaBrokers = []string{"192.168.10.8:9092", "192.168.10.8:9093", "192.168.10.8:9094"}

type UserProducer struct {
	writer       *kafka.Writer
	retryWriter  *kafka.Writer
	kafkaOptions *options.KafkaOptions
	maxRetries   int
}

// internal/pkg/server/producer.go

func NewUserProducer(options *options.KafkaOptions) *UserProducer {
	// ä¸»Writerï¼ˆé«˜æ€§èƒ½é…ç½®ï¼‰
	mainWriter := &kafka.Writer{
		Addr: kafka.TCP(options.Brokers...),
		// æ³¨æ„ï¼šè¿™é‡Œä¸è®¾ç½® Topicï¼Œåœ¨å‘é€æ—¶åŠ¨æ€è®¾ç½®
		MaxAttempts:     3,
		WriteBackoffMin: 100 * time.Millisecond,
		WriteBackoffMax: 1 * time.Second,
		BatchBytes:      1048576,
		BatchSize:       options.BatchSize,
		BatchTimeout:    options.BatchTimeout,
		WriteTimeout:    30 * time.Second,
		Balancer:        &kafka.LeastBytes{},
		Compression:     kafka.Snappy,
		RequiredAcks:    kafka.RequireOne,
		Async:           false,
	}

	// é‡è¯•Writerï¼ˆé«˜å¯é é…ç½®ï¼‰- ä¸è®¾ç½® Topic
	reliableWriter := &kafka.Writer{
		Addr: kafka.TCP(options.Brokers...),
		// æ³¨æ„ï¼šè¿™é‡Œä¸è®¾ç½® Topicï¼Œåœ¨å‘é€æ—¶åŠ¨æ€è®¾ç½®
		MaxAttempts:     10,
		WriteBackoffMin: 500 * time.Millisecond,
		WriteBackoffMax: 5 * time.Second,
		BatchSize:       1,
		WriteTimeout:    60 * time.Second,
		RequiredAcks:    kafka.RequireAll,
		Async:           false,
		Compression:     kafka.Snappy,
	}

	return &UserProducer{
		writer:      mainWriter,
		retryWriter: reliableWriter,
		maxRetries:  options.MaxRetries,
	}
}

func (p *UserProducer) SendUserCreateMessage(ctx context.Context, user *v1.User) error {
	return p.sendUserMessage(ctx, user, OperationCreate, UserCreateTopic)
}

func (p *UserProducer) SendUserUpdateMessage(ctx context.Context, user *v1.User) error {
	return p.sendUserMessage(ctx, user, OperationUpdate, UserUpdateTopic)
}

func (p *UserProducer) SendUserDeleteMessage(ctx context.Context, username string) error {
	deleteData := map[string]interface{}{
		"username":   username,
		"deleted_at": time.Now().Format(time.RFC3339),
	}

	data, err := json.Marshal(deleteData)
	if err != nil {
		return errors.WithCode(code.ErrEncodingJSON, "åˆ é™¤æ¶ˆæ¯åºåˆ—åŒ–å¤±è´¥")
	}

	msg := kafka.Message{
		Key:   []byte(username),
		Value: data,
		Time:  time.Now(),
		Headers: []kafka.Header{
			{Key: HeaderOperation, Value: []byte(OperationDelete)},
			{Key: HeaderOriginalTimestamp, Value: []byte(time.Now().Format(time.RFC3339))},
			{Key: HeaderRetryCount, Value: []byte("0")},
		},
	}
	return p.sendWithRetry(ctx, msg, UserDeleteTopic)
}

func (p *UserProducer) sendUserMessage(ctx context.Context, user *v1.User, operation, topic string) error {
	start := time.Now()
	var errSend error
	defer func() {
		metrics.RecordKafkaProducerOperation(topic, operation, time.Since(start).Seconds(), errSend, false)
	}()

	userData, err := json.Marshal(user)
	if err != nil {
		errSend = err
		log.Errorf("topic:%v operation:%væ¶ˆæ¯åºåˆ—å·å¤±è´¥:%v", topic, operation, err)
		return errors.WithCode(code.ErrEncodingJSON, "ç”¨æˆ·æ¶ˆæ¯åºåˆ—åŒ–å¤±è´¥")
	}
	now := time.Now()
	idBytes := make([]byte, 8)
	binary.BigEndian.PutUint64(idBytes, user.ID)
	msg := kafka.Message{
		Key:   idBytes,
		Value: userData,
		Time:  now,
		Headers: []kafka.Header{
			{Key: HeaderOperation, Value: []byte(operation)},
			{Key: HeaderOriginalTimestamp, Value: []byte(now.Format(time.RFC3339))},
			{Key: HeaderRetryCount, Value: []byte("0")},
		},
	}
	errSend = p.sendWithRetry(ctx, msg, topic)
	return errSend
}

// æ·»åŠ éªŒè¯æ–¹æ³•
func (p *UserProducer) validateMessage(msg kafka.Message) error {
	// æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å«Topicå­—æ®µï¼ˆä¸åº”è¯¥åŒ…å«ï¼‰
	if strings.TrimSpace(msg.Topic) != "" {
		err := errors.WithCode(code.ErrMissingHeader, "å¿…é¡»è®¾ç½®topic")
		log.Errorf("%v %v", errors.GetMessage(err), err)
		return err
	}
	return nil
}

// sendWithRetry å¸¦é‡è¯•çš„å‘é€é€»è¾‘
func (p *UserProducer) sendWithRetry(ctx context.Context, msg kafka.Message, topic string) error {
	startTime := time.Now()
	// æ·»åŠ è¯¦ç»†çš„å‘é€æ—¥å¿—
	//log.Errorf("å‡†å¤‡å‘é€æ¶ˆæ¯åˆ°[æµ‹è¯•ä¸¢å¤±è®°å½•é—®é¢˜] %s: key=%s", topic, string(msg.Key))
	operation := p.getOperationFromHeaders(msg.Headers)

	var sendErr error
	var isRetry bool
	var success bool

	defer func() {
		// åªæœ‰æˆåŠŸæˆ–æœ€ç»ˆå¤±è´¥æ—¶æ‰è®°å½•æŒ‡æ ‡
		if success || sendErr != nil {
			metrics.RecordKafkaProducerOperation(topic, operation,
				time.Since(startTime).Seconds(), sendErr, isRetry)
		}
	}()

	if err := p.validateMessage(msg); err != nil {
		sendErr = err
		return err
	}

	// åˆ›å»ºæ–°çš„æ¶ˆæ¯
	sendMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Time:    time.Now(),
		Topic:   topic,
		Headers: make([]kafka.Header, len(msg.Headers)),
	}
	copy(sendMsg.Headers, msg.Headers)

	//é¦–æ¬¡å‘é€
	err := p.writer.WriteMessages(ctx, sendMsg)
	if err == nil {
		success = true // æ ‡è®°ä¸ºæˆåŠŸ
		log.Infof("å‘é€æˆåŠŸ: topic=%s, key=%s, è€—æ—¶=%v", topic, string(msg.Key), time.Since(startTime))
		return nil
	}
	// é¦–æ¬¡å‘é€å¤±è´¥ï¼Œè¿›è¡Œé‡è¯•
	isRetry = true
	sendErr = p.sendToRetryTopic(ctx, msg, err.Error())

	if sendErr != nil {
		return sendErr
	}

	success = true
	sendErr = nil
	log.Infof("å‘é€æˆåŠŸ(é‡è¯•): topic=%s, key=%s, è€—æ—¶=%v", topic, string(msg.Key), time.Since(startTime))
	return nil
}

func (p *UserProducer) getOperationFromHeaders(headers []kafka.Header) string {
	for _, h := range headers {
		if h.Key == HeaderOperation {
			return string(h.Value)
		}
	}
	return "unknown"
}

func (p *UserProducer) sendToRetryTopic(ctx context.Context, msg kafka.Message, errorInfo string) error {
	// 1. è¯»å–åŸå§‹æ¶ˆæ¯çš„é‡è¯•æ¬¡æ•°
	//log.Debugf("ğŸ“¨ è¿›å…¥sendToRetryTopic: key=%s", string(msg.Key))

	// for i, header := range msg.Headers {
	// 	log.Debugf("  è¾“å…¥æ¶ˆæ¯Header[%d]: %s=%s", i, header.Key, string(header.Value))
	// }
	currentRetryCount := 0
	for _, h := range msg.Headers {
		if h.Key == HeaderRetryCount {
			if cnt, err := strconv.Atoi(string(h.Value)); err == nil {
				currentRetryCount = cnt
			}
			break
		}
	}
	currentRetryCount++

	// 2. æ£€æŸ¥æœ€å¤§é‡è¯•æ¬¡æ•°
	if currentRetryCount > p.maxRetries {
		errMsg := fmt.Sprintf("å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ%dæ¬¡ï¼‰", p.maxRetries)
		log.Warnf("key=%s: %s", string(msg.Key), errMsg)
		return p.SendToDeadLetterTopic(ctx, msg, errMsg+": "+errorInfo)
	}

	// 3. åˆ›å»ºé‡è¯•æ¶ˆæ¯
	retryMsg := kafka.Message{
		Key:   msg.Key,
		Value: msg.Value,
		Time:  time.Now(),
	}

	// å¤åˆ¶å¹¶æ›´æ–°headersï¼ˆä¿®å¤é‡å¤é—®é¢˜ï¼‰
	retryMsg.Headers = make([]kafka.Header, len(msg.Headers))
	copy(retryMsg.Headers, msg.Headers)
	retryMsg.Headers = p.updateOrAddHeader(retryMsg.Headers, HeaderRetryCount, strconv.Itoa(currentRetryCount))
	retryMsg.Headers = p.updateOrAddHeader(retryMsg.Headers, HeaderRetryError, errorInfo)
	retryMsg.Headers = p.updateOrAddHeader(retryMsg.Headers, HeaderNextRetryTS, p.calcNextRetryTS(currentRetryCount).Format(time.RFC3339))

	// 4. ä½¿ç”¨å¢å¼ºçš„åŒæ­¥å‘é€ï¼ˆä¸æ”¹å˜åŸæœ‰å‡½æ•°åï¼‰
	retryCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()
	err := p.sendMessageWithRetry(retryCtx, retryMsg, UserRetryTopic)
	if err != nil {
		log.Errorf("é‡è¯•å‘é€å¤±è´¥ï¼ˆkey=%s, ç¬¬%dæ¬¡ï¼‰: %v", string(msg.Key), currentRetryCount, err)
		// è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
		return p.SendToDeadLetterTopic(ctx, msg, "é‡è¯•å‘é€å¤±è´¥: "+err.Error())
	}

	log.Infow("æˆåŠŸå‘é€åˆ°é‡è¯•Topic",
		"key", string(msg.Key),
		"retry_count", currentRetryCount,
		"next_retry", p.calcNextRetryTS(currentRetryCount).Format(time.RFC3339))
	return nil
}

// æ–°å¢ï¼šè®¡ç®—ä¸‹æ¬¡é‡è¯•æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰
func (p *UserProducer) calcNextRetryTS(retryCount int) time.Time {
	// åŸºç¡€å»¶è¿Ÿ * 2^(é‡è¯•æ¬¡æ•°-1)ï¼Œé¿å…çŸ­æœŸå†…é¢‘ç¹é‡è¯•
	delay := p.kafkaOptions.BaseRetryDelay * time.Duration(1<<(retryCount-1))
	// é™åˆ¶æœ€å¤§å»¶è¿Ÿï¼Œé¿å…é‡è¯•é—´éš”è¿‡é•¿
	if delay > p.kafkaOptions.MaxRetryDelay {
		delay = p.kafkaOptions.MaxRetryDelay
	}
	return time.Now().Add(delay)
}

func (p *UserProducer) SendToDeadLetterTopic(ctx context.Context, msg kafka.Message, errorInfo string) error {
	start := time.Now()
	operation := p.getOperationFromHeaders(msg.Headers)

	var sendErr error
	defer func() {
		p.recordDeadLetterOperation(UserDeadLetterTopic, operation, start, sendErr, errorInfo, string(msg.Key))
	}()
	// åˆ›å»ºæ­»ä¿¡æ¶ˆæ¯
	deadLetterMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Time:    time.Now(),
		Headers: p.updateOrAddHeader(msg.Headers, "deadletter-reason", errorInfo),
	}
	deadLetterMsg.Headers = p.updateOrAddHeader(deadLetterMsg.Headers, "deadletter-timestamp", time.Now().Format(time.RFC3339))
	log.Warnf("å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: key=%s, reason=%s", string(msg.Key), errorInfo)
	// ä½¿ç”¨å¢å¼ºçš„åŒæ­¥å‘é€
	sendErr = p.sendMessageWithRetry(ctx, deadLetterMsg, UserDeadLetterTopic)
	return sendErr
}

// recordDeadLetterOperation è®°å½•æ­»ä¿¡é˜Ÿåˆ—æ“ä½œæŒ‡æ ‡
func (p *UserProducer) recordDeadLetterOperation(topic, operation string, start time.Time, err error, errorInfo, messageKey string) {
	duration := time.Since(start).Seconds()

	// è®°å½•æ­»ä¿¡æ¶ˆæ¯è®¡æ•°
	metrics.DeadLetterMessages.WithLabelValues(topic, operation).Inc()

	// è®°å½•å¤„ç†æ—¶é—´
	status := "dead_letter_success"
	if err != nil {
		status = "dead_letter_failure"
		// è®°å½•æ­»ä¿¡å‘é€å¤±è´¥çš„é”™è¯¯
		errorType := metrics.GetKafkaErrorType(err)
		metrics.ProducerFailures.WithLabelValues(topic, operation, errorType).Inc()
	}
	metrics.MessageProcessingTime.WithLabelValues(topic, operation, status).Observe(duration)

	// è®°å½•æ—¥å¿—
	if err != nil {
		log.Errorf("æ­»ä¿¡é˜Ÿåˆ—å‘é€å¤±è´¥: key=%s, reason=%s, error=%v", messageKey, errorInfo, err)
	} else {
		log.Warnf("å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—æˆåŠŸ: key=%s, reason=%s, è€—æ—¶=%v", messageKey, errorInfo, duration)
	}
}

func (p *UserProducer) Close() error {
	if p.writer != nil {
		if err := p.writer.Close(); err != nil {
			return err
		}
	}

	if p.retryWriter != nil {
		if err := p.retryWriter.Close(); err != nil {
			return err
		}
	}

	return nil
}

func (p *UserProducer) updateOrAddHeader(headers []kafka.Header, key, value string) []kafka.Header {
	// 1. åŸºç¡€æ ¡éªŒï¼šé˜»æ–­ç©ºKeyè¾“å…¥ï¼ˆé¿å…æ— æ•ˆå¤´ï¼‰
	if key == "" {
		panic("kafka header key cannot be empty string")
	}

	// 2. ç»Ÿä¸€ç›®æ ‡Keyä¸ºå°å†™ï¼ˆç”¨äºå¿½ç•¥å¤§å°å†™åŒ¹é…ï¼Œä¸æ”¹å˜åŸå§‹Keyçš„å±•ç¤ºï¼‰
	targetKeyLower := strings.ToLower(key)

	// 3. åˆå§‹åŒ–æ–°åˆ‡ç‰‡+æ ‡è®°ï¼šnewHeaderså­˜æœ€ç»ˆç»“æœï¼Œfoundæ ‡è®°æ˜¯å¦å·²ä¿ç•™ä¸€ä¸ªç›®æ ‡å¤´
	var newHeaders []kafka.Header
	foundTargetHeader := false

	// 4. éå†åŸå§‹åˆ‡ç‰‡ï¼šé€ä¸ªå¤„ç†æ¯ä¸ªå¤´ï¼Œç­›é€‰é‡å¤ç›®æ ‡å¤´
	for _, header := range headers {
		// 4.1 åˆ¤æ–­å½“å‰å¤´æ˜¯å¦ä¸ºç›®æ ‡å¤´ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
		currentHeaderKeyLower := strings.ToLower(header.Key)
		if currentHeaderKeyLower == targetKeyLower {
			// 4.2 è‹¥æœªä¿ç•™è¿‡ç›®æ ‡å¤´ï¼šæ›´æ–°å…¶Valueï¼ŒåŠ å…¥æ–°åˆ‡ç‰‡ï¼Œæ ‡è®°å·²ä¿ç•™
			if !foundTargetHeader {
				// ä¿ç•™åŸå§‹Keyçš„å¤§å°å†™ï¼ˆä»…æ›´æ–°Valueï¼‰ï¼Œé¿å…ä¿®æ”¹ç”¨æˆ·è¾“å…¥çš„Keyæ ¼å¼
				updatedHeader := kafka.Header{
					Key:   header.Key,    // å¦‚åŸKeyæ˜¯"Retry-Error"ï¼Œä»ä¿ç•™è¯¥æ ¼å¼
					Value: []byte(value), // å†™å…¥æœ€æ–°Value
				}
				newHeaders = append(newHeaders, updatedHeader)
				foundTargetHeader = true // æ ‡è®°å·²ä¿ç•™ï¼Œåç»­é‡å¤å¤´ä¸å†å¤„ç†
			}
			// 4.3 è‹¥å·²ä¿ç•™è¿‡ç›®æ ‡å¤´ï¼šç›´æ¥è·³è¿‡ï¼Œä¸åŠ å…¥æ–°åˆ‡ç‰‡ï¼ˆåˆ é™¤é‡å¤ï¼‰
			continue
		}

		// 4.4 éç›®æ ‡å¤´ï¼šç›´æ¥åŠ å…¥æ–°åˆ‡ç‰‡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
		newHeaders = append(newHeaders, header)
	}

	// 5. è‹¥éå†å®Œæœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡å¤´ï¼šæ–°å¢ä¸€ä¸ªï¼ˆä¿ç•™ç”¨æˆ·è¾“å…¥çš„åŸå§‹Keyå¤§å°å†™ï¼‰
	if !foundTargetHeader {
		newHeaders = append(newHeaders, kafka.Header{
			Key:   key, // å¦‚ç”¨æˆ·ä¼ "Retry-Error"ï¼Œæ–°å¢æ—¶å°±ç”¨è¯¥Keyï¼Œä¸å¼ºåˆ¶å°å†™
			Value: []byte(value),
		})
	}

	return newHeaders
}

// sendMessageWithRetry å¢å¼ºçš„åŒæ­¥å‘é€æ–¹æ³•ï¼ˆæ–°å¢ï¼‰
// sendMessageWithRetry å¢å¼ºçš„åŒæ­¥å‘é€æ–¹æ³•
func (p *UserProducer) sendMessageWithRetry(ctx context.Context, msg kafka.Message, topic string) error {
	const maxSendRetries = 3
	var lastErr error

	for i := 0; i < maxSendRetries; i++ {
		// ä½¿ç”¨ä¸´æ—¶writerï¼Œé¿å…é•¿æœŸå ç”¨è¿æ¥
		writer := &kafka.Writer{
			Addr:                   kafka.TCP(KafkaBrokers...),
			Topic:                  topic,
			Balancer:               &kafka.LeastBytes{},
			BatchSize:              1, // åŒæ­¥å‘é€ï¼Œæ¯æ‰¹æ¬¡1æ¡
			BatchTimeout:           100 * time.Millisecond,
			Async:                  false,            // åŒæ­¥æ¨¡å¼
			RequiredAcks:           kafka.RequireOne, // åªéœ€è¦leaderç¡®è®¤
			AllowAutoTopicCreation: true,             // å…è®¸è‡ªåŠ¨åˆ›å»ºä¸»é¢˜
		}

		// è®¾ç½®å‘é€è¶…æ—¶
		sendCtx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer cancel()

		err := writer.WriteMessages(sendCtx, msg)

		// æ— è®ºæˆåŠŸå¤±è´¥éƒ½ç«‹å³å…³é—­writer
		if closeErr := writer.Close(); closeErr != nil {
			log.Warnf("å…³é—­writerå¤±è´¥: %v", closeErr)
		}
		if err == nil {
			return nil
		}

		lastErr = err
		log.Warnf("å‘é€å¤±è´¥ï¼ˆkey=%s, topic=%s, å°è¯•%d/%dï¼‰: %v",
			string(msg.Key), topic, i+1, maxSendRetries, err)

		// ç­‰å¾…åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
		if i < maxSendRetries-1 {
			waitTime := time.Duration(1<<uint(i)) * time.Second
			select {
			case <-time.After(waitTime):
				continue
			case <-ctx.Done():
				return ctx.Err()
			}
		}
	}

	return fmt.Errorf("å‘é€åˆ°ä¸»é¢˜%sé‡è¯•%dæ¬¡å‡å¤±è´¥: %v", topic, maxSendRetries, lastErr)
}
