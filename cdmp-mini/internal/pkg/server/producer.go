// internal/pkg/server/producer.go
package server

import (
	"context"
	"encoding/json"
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/code"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/metrics"
	"github.com/maxiaolu1981/cretem/cdmp-mini/internal/pkg/server/producer"
	"github.com/maxiaolu1981/cretem/cdmp-mini/pkg/log"
	v1 "github.com/maxiaolu1981/cretem/nexuscore/api/apiserver/v1"
	"github.com/maxiaolu1981/cretem/nexuscore/errors"
	"github.com/segmentio/kafka-go"
)

var _ producer.MessageProducer = (*UserProducer)(nil)
var KafkaBrokers = []string{"127.0.0.1:9092"}

// åœ¨å…¨å±€å˜é‡åŒºåŸŸæ·»åŠ  Prometheus æŒ‡æ ‡

type UserProducer struct {
	writer           *kafka.Writer
	retryWriter      *kafka.Writer
	maxRetries       int
	deadLetterWriter *kafka.Writer
}

// internal/pkg/server/producer.go

func NewUserProducer(brokers []string, batchSize int, batchTimeout time.Duration) *UserProducer {
	// ä¸»Writerï¼ˆé«˜æ€§èƒ½é…ç½®ï¼‰
	mainWriter := &kafka.Writer{
		Addr: kafka.TCP(brokers...),
		// æ³¨æ„ï¼šè¿™é‡Œä¸è®¾ç½® Topicï¼Œåœ¨å‘é€æ—¶åŠ¨æ€è®¾ç½®
		MaxAttempts:     3,
		WriteBackoffMin: 100 * time.Millisecond,
		WriteBackoffMax: 1 * time.Second,
		BatchBytes:      1048576,
		BatchSize:       batchSize,
		BatchTimeout:    batchTimeout,
		WriteTimeout:    30 * time.Second,
		Balancer:        &kafka.LeastBytes{},
		Compression:     kafka.Snappy,
		RequiredAcks:    kafka.RequireOne,
		Async:           false,
	}

	// é‡è¯•Writerï¼ˆé«˜å¯é é…ç½®ï¼‰- ä¸è®¾ç½® Topic
	reliableWriter := &kafka.Writer{
		Addr: kafka.TCP(brokers...),
		// æ³¨æ„ï¼šè¿™é‡Œä¸è®¾ç½® Topicï¼Œåœ¨å‘é€æ—¶åŠ¨æ€è®¾ç½®
		MaxAttempts:     10,
		WriteBackoffMin: 500 * time.Millisecond,
		WriteBackoffMax: 5 * time.Second,
		BatchSize:       1,
		WriteTimeout:    60 * time.Second,
		RequiredAcks:    kafka.RequireAll,
		Async:           false,
		Compression:     kafka.Snappy,
	}

	return &UserProducer{
		writer:      mainWriter,
		retryWriter: reliableWriter,
		maxRetries:  MaxRetryCount,
	}
}

func (p *UserProducer) SendUserCreateMessage(ctx context.Context, user *v1.User) error {
	return p.sendUserMessage(ctx, user, OperationCreate, UserCreateTopic)
}

func (p *UserProducer) SendUserUpdateMessage(ctx context.Context, user *v1.User) error {
	return p.sendUserMessage(ctx, user, OperationUpdate, UserUpdateTopic)
}

func (p *UserProducer) SendUserDeleteMessage(ctx context.Context, username string) error {
	deleteData := map[string]interface{}{
		"username":   username,
		"deleted_at": time.Now().Format(time.RFC3339),
	}

	data, err := json.Marshal(deleteData)
	if err != nil {
		return errors.WithCode(code.ErrEncodingJSON, "åˆ é™¤æ¶ˆæ¯åºåˆ—åŒ–å¤±è´¥")
	}

	msg := kafka.Message{
		Key:   []byte(username),
		Value: data,
		Time:  time.Now(),
		Headers: []kafka.Header{
			{Key: HeaderOperation, Value: []byte(OperationDelete)},
			{Key: HeaderOriginalTimestamp, Value: []byte(time.Now().Format(time.RFC3339))},
			{Key: HeaderRetryCount, Value: []byte("0")},
		},
	}

	return p.sendWithRetry(ctx, msg, UserDeleteTopic)
}

func (p *UserProducer) sendUserMessage(ctx context.Context, user *v1.User, operation, topic string) error {
	userData, err := json.Marshal(user)
	if err != nil {
		return errors.WithCode(code.ErrEncodingJSON, "ç”¨æˆ·æ¶ˆæ¯åºåˆ—åŒ–å¤±è´¥")
	}

	msg := kafka.Message{
		Key:   []byte(user.Name),
		Value: userData,
		Time:  time.Now(),
		Headers: []kafka.Header{
			{Key: HeaderOperation, Value: []byte(operation)},
			{Key: HeaderOriginalTimestamp, Value: []byte(time.Now().Format(time.RFC3339))},
			{Key: HeaderRetryCount, Value: []byte("0")},
		},
	}

	return p.sendWithRetry(ctx, msg, topic)
}

// æ·»åŠ éªŒè¯æ–¹æ³•
func (p *UserProducer) validateMessage(msg kafka.Message) error {
	// æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å«Topicå­—æ®µï¼ˆä¸åº”è¯¥åŒ…å«ï¼‰
	if msg.Topic != "" {
		return fmt.Errorf("message should not have Topic field set")
	}
	return nil
}

// sendWithRetry å¸¦é‡è¯•çš„å‘é€é€»è¾‘
func (p *UserProducer) sendWithRetry(ctx context.Context, msg kafka.Message, topic string) error {
	startTime := time.Now()
	operation := p.getOperationFromHeaders(msg.Headers)

	// è®°å½•å‘é€å°è¯•
	metrics.ProducerAttempts.WithLabelValues(topic, operation).Inc()

	// åˆ›å»ºæ–°çš„æ¶ˆæ¯
	sendMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Time:    time.Now(),
		Topic:   topic,
		Headers: make([]kafka.Header, len(msg.Headers)),
	}
	copy(sendMsg.Headers, msg.Headers)

	if err := p.validateMessage(msg); err != nil {
		return fmt.Errorf("invalid message: %v", err)
	}

	err := p.writer.WriteMessages(ctx, sendMsg)

	if err != nil {
		log.Errorf("Topic %s å‘é€å¤±è´¥ï¼Œå°è¯•é‡è¯•Topic. Key: %s", topic, string(msg.Key))
		// è®°å½•å¤±è´¥
		metrics.ProducerFailures.WithLabelValues(topic, operation, "initial_send").Inc()

		retryErr := p.sendToRetryTopic(ctx, msg, err.Error())
		if retryErr != nil {
			// è®°å½•é‡è¯•å¤±è´¥çš„æ€»æ—¶é—´
			metrics.MessageProcessingTime.WithLabelValues(topic, operation, "failure").Observe(time.Since(startTime).Seconds())
			return retryErr
		}
		// è®°å½•é‡è¯•æˆåŠŸ
		metrics.ProducerRetries.WithLabelValues(topic, operation).Inc()
		metrics.ProducerSuccess.WithLabelValues(topic, operation).Inc() // âœ… é‡è¯•æˆåŠŸä¹Ÿè¦è®°å½•æˆåŠŸï¼
		metrics.MessageProcessingTime.WithLabelValues(topic, operation, "retry_success").Observe(time.Since(startTime).Seconds())
		return nil
	}

	// âœ… ä¿®å¤ï¼šç›´æ¥æˆåŠŸæ—¶è®°å½•æˆåŠŸæŒ‡æ ‡
	metrics.ProducerSuccess.WithLabelValues(topic, operation).Inc()
	metrics.MessageProcessingTime.WithLabelValues(topic, operation, "success").Observe(time.Since(startTime).Seconds())
	log.Infow("æ¶ˆæ¯æˆåŠŸå‘é€åˆ°Topic", "topic", topic, "key", string(msg.Key))
	return nil
}

func (p *UserProducer) getOperationFromHeaders(headers []kafka.Header) string {
	for _, h := range headers {
		if h.Key == HeaderOperation {
			return string(h.Value)
		}
	}
	return "unknown"
}

func (p *UserProducer) sendToRetryTopic(ctx context.Context, msg kafka.Message, errorInfo string) error {
	// 1. è¯»å–åŸå§‹æ¶ˆæ¯çš„é‡è¯•æ¬¡æ•°
	log.Debugf("ğŸ“¨ è¿›å…¥sendToRetryTopic: key=%s", string(msg.Key))

	for i, header := range msg.Headers {
		log.Debugf("  è¾“å…¥æ¶ˆæ¯Header[%d]: %s=%s", i, header.Key, string(header.Value))
	}
	currentRetryCount := 0
	for _, h := range msg.Headers {
		if h.Key == HeaderRetryCount {
			if cnt, err := strconv.Atoi(string(h.Value)); err == nil {
				currentRetryCount = cnt
			}
			break
		}
	}
	currentRetryCount++

	// 2. æ£€æŸ¥æœ€å¤§é‡è¯•æ¬¡æ•°
	if currentRetryCount > p.maxRetries {
		errMsg := fmt.Sprintf("å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ˆ%dæ¬¡ï¼‰", p.maxRetries)
		log.Warnf("key=%s: %s", string(msg.Key), errMsg)
		return p.SendToDeadLetterTopic(ctx, msg, errMsg+": "+errorInfo)
	}

	// 3. åˆ›å»ºé‡è¯•æ¶ˆæ¯
	retryMsg := kafka.Message{
		Key:   msg.Key,
		Value: msg.Value,
		Time:  time.Now(),
	}

	// å¤åˆ¶å¹¶æ›´æ–°headersï¼ˆä¿®å¤é‡å¤é—®é¢˜ï¼‰
	retryMsg.Headers = make([]kafka.Header, len(msg.Headers))
	copy(retryMsg.Headers, msg.Headers)
	retryMsg.Headers = p.updateOrAddHeader(retryMsg.Headers, HeaderRetryCount, strconv.Itoa(currentRetryCount))
	retryMsg.Headers = p.updateOrAddHeader(retryMsg.Headers, HeaderRetryError, errorInfo)
	retryMsg.Headers = p.updateOrAddHeader(retryMsg.Headers, HeaderNextRetryTS, p.calcNextRetryTS(currentRetryCount).Format(time.RFC3339))

	// 4. ä½¿ç”¨å¢å¼ºçš„åŒæ­¥å‘é€ï¼ˆä¸æ”¹å˜åŸæœ‰å‡½æ•°åï¼‰
	retryCtx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
	defer cancel()
	err := p.sendMessageWithRetry(retryCtx, retryMsg, UserRetryTopic, currentRetryCount)
	if err != nil {
		log.Errorf("é‡è¯•å‘é€å¤±è´¥ï¼ˆkey=%s, ç¬¬%dæ¬¡ï¼‰: %v", string(msg.Key), currentRetryCount, err)
		// è¿›å…¥æ­»ä¿¡é˜Ÿåˆ—
		return p.SendToDeadLetterTopic(ctx, msg, "é‡è¯•å‘é€å¤±è´¥: "+err.Error())
	}

	log.Infow("æˆåŠŸå‘é€åˆ°é‡è¯•Topic",
		"key", string(msg.Key),
		"retry_count", currentRetryCount,
		"next_retry", p.calcNextRetryTS(currentRetryCount).Format(time.RFC3339))
	return nil
}

// æ–°å¢ï¼šè®¡ç®—ä¸‹æ¬¡é‡è¯•æ—¶é—´ï¼ˆæŒ‡æ•°é€€é¿ç­–ç•¥ï¼‰
func (p *UserProducer) calcNextRetryTS(retryCount int) time.Time {
	// åŸºç¡€å»¶è¿Ÿ * 2^(é‡è¯•æ¬¡æ•°-1)ï¼Œé¿å…çŸ­æœŸå†…é¢‘ç¹é‡è¯•
	delay := BaseRetryDelay * time.Duration(1<<(retryCount-1))
	// é™åˆ¶æœ€å¤§å»¶è¿Ÿï¼Œé¿å…é‡è¯•é—´éš”è¿‡é•¿
	if delay > MaxRetryDelay {
		delay = MaxRetryDelay
	}
	return time.Now().Add(delay)
}



func (p *UserProducer) SendToDeadLetterTopic(ctx context.Context, msg kafka.Message, errorInfo string) error {
	operation := p.getOperationFromHeaders(msg.Headers)
	metrics.DeadLetterMessages.WithLabelValues(UserDeadLetterTopic, operation).Inc()
	// åˆ›å»ºæ­»ä¿¡æ¶ˆæ¯
	deadLetterMsg := kafka.Message{
		Key:     msg.Key,
		Value:   msg.Value,
		Time:    time.Now(),
		Headers: p.updateOrAddHeader(msg.Headers, "deadletter-reason", errorInfo),
	}
	deadLetterMsg.Headers = p.updateOrAddHeader(deadLetterMsg.Headers, "deadletter-timestamp", time.Now().Format(time.RFC3339))

	log.Warnf("å‘é€åˆ°æ­»ä¿¡é˜Ÿåˆ—: key=%s, reason=%s", string(msg.Key), errorInfo)

	// ä½¿ç”¨å¢å¼ºçš„åŒæ­¥å‘é€
	return p.sendMessageWithRetry(ctx, deadLetterMsg, UserDeadLetterTopic, 0)
}

func (p *UserProducer) Close() error {
	if p.writer != nil {
		if err := p.writer.Close(); err != nil {
			return err
		}
	}

	if p.retryWriter != nil {
		if err := p.retryWriter.Close(); err != nil {
			return err
		}
	}

	return nil
}

func (p *UserProducer) updateOrAddHeader(headers []kafka.Header, key, value string) []kafka.Header {
	// 1. åŸºç¡€æ ¡éªŒï¼šé˜»æ–­ç©ºKeyè¾“å…¥ï¼ˆé¿å…æ— æ•ˆå¤´ï¼‰
	if key == "" {
		panic("kafka header key cannot be empty string")
	}

	// 2. ç»Ÿä¸€ç›®æ ‡Keyä¸ºå°å†™ï¼ˆç”¨äºå¿½ç•¥å¤§å°å†™åŒ¹é…ï¼Œä¸æ”¹å˜åŸå§‹Keyçš„å±•ç¤ºï¼‰
	targetKeyLower := strings.ToLower(key)

	// 3. åˆå§‹åŒ–æ–°åˆ‡ç‰‡+æ ‡è®°ï¼šnewHeaderså­˜æœ€ç»ˆç»“æœï¼Œfoundæ ‡è®°æ˜¯å¦å·²ä¿ç•™ä¸€ä¸ªç›®æ ‡å¤´
	var newHeaders []kafka.Header
	foundTargetHeader := false

	// 4. éå†åŸå§‹åˆ‡ç‰‡ï¼šé€ä¸ªå¤„ç†æ¯ä¸ªå¤´ï¼Œç­›é€‰é‡å¤ç›®æ ‡å¤´
	for _, header := range headers {
		// 4.1 åˆ¤æ–­å½“å‰å¤´æ˜¯å¦ä¸ºç›®æ ‡å¤´ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰
		currentHeaderKeyLower := strings.ToLower(header.Key)
		if currentHeaderKeyLower == targetKeyLower {
			// 4.2 è‹¥æœªä¿ç•™è¿‡ç›®æ ‡å¤´ï¼šæ›´æ–°å…¶Valueï¼ŒåŠ å…¥æ–°åˆ‡ç‰‡ï¼Œæ ‡è®°å·²ä¿ç•™
			if !foundTargetHeader {
				// ä¿ç•™åŸå§‹Keyçš„å¤§å°å†™ï¼ˆä»…æ›´æ–°Valueï¼‰ï¼Œé¿å…ä¿®æ”¹ç”¨æˆ·è¾“å…¥çš„Keyæ ¼å¼
				updatedHeader := kafka.Header{
					Key:   header.Key,    // å¦‚åŸKeyæ˜¯"Retry-Error"ï¼Œä»ä¿ç•™è¯¥æ ¼å¼
					Value: []byte(value), // å†™å…¥æœ€æ–°Value
				}
				newHeaders = append(newHeaders, updatedHeader)
				foundTargetHeader = true // æ ‡è®°å·²ä¿ç•™ï¼Œåç»­é‡å¤å¤´ä¸å†å¤„ç†
			}
			// 4.3 è‹¥å·²ä¿ç•™è¿‡ç›®æ ‡å¤´ï¼šç›´æ¥è·³è¿‡ï¼Œä¸åŠ å…¥æ–°åˆ‡ç‰‡ï¼ˆåˆ é™¤é‡å¤ï¼‰
			continue
		}

		// 4.4 éç›®æ ‡å¤´ï¼šç›´æ¥åŠ å…¥æ–°åˆ‡ç‰‡ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ä¸å˜ï¼‰
		newHeaders = append(newHeaders, header)
	}

	// 5. è‹¥éå†å®Œæœªæ‰¾åˆ°ä»»ä½•ç›®æ ‡å¤´ï¼šæ–°å¢ä¸€ä¸ªï¼ˆä¿ç•™ç”¨æˆ·è¾“å…¥çš„åŸå§‹Keyå¤§å°å†™ï¼‰
	if !foundTargetHeader {
		newHeaders = append(newHeaders, kafka.Header{
			Key:   key, // å¦‚ç”¨æˆ·ä¼ "Retry-Error"ï¼Œæ–°å¢æ—¶å°±ç”¨è¯¥Keyï¼Œä¸å¼ºåˆ¶å°å†™
			Value: []byte(value),
		})
	}

	return newHeaders
}

// sendMessageWithRetry å¢å¼ºçš„åŒæ­¥å‘é€æ–¹æ³•ï¼ˆæ–°å¢ï¼‰
// sendMessageWithRetry å¢å¼ºçš„åŒæ­¥å‘é€æ–¹æ³•
func (p *UserProducer) sendMessageWithRetry(ctx context.Context, msg kafka.Message, topic string, attempt int) error {
	const maxSendRetries = 3
	var lastErr error

	for i := 0; i < maxSendRetries; i++ {
		// ä½¿ç”¨ä¸´æ—¶writerï¼Œé¿å…é•¿æœŸå ç”¨è¿æ¥
		writer := &kafka.Writer{
			Addr:                   kafka.TCP(KafkaBrokers...),
			Topic:                  topic,
			Balancer:               &kafka.LeastBytes{},
			BatchSize:              1, // åŒæ­¥å‘é€ï¼Œæ¯æ‰¹æ¬¡1æ¡
			BatchTimeout:           100 * time.Millisecond,
			Async:                  false,            // åŒæ­¥æ¨¡å¼
			RequiredAcks:           kafka.RequireOne, // åªéœ€è¦leaderç¡®è®¤
			AllowAutoTopicCreation: true,             // å…è®¸è‡ªåŠ¨åˆ›å»ºä¸»é¢˜
		}

		// è®¾ç½®å‘é€è¶…æ—¶
		sendCtx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
		defer cancel()

		err := writer.WriteMessages(sendCtx, msg)

		// æ— è®ºæˆåŠŸå¤±è´¥éƒ½ç«‹å³å…³é—­writer
		if closeErr := writer.Close(); closeErr != nil {
			log.Warnf("å…³é—­writerå¤±è´¥: %v", closeErr)
		}
		if err == nil {
			return nil
		}

		lastErr = err
		log.Warnf("å‘é€å¤±è´¥ï¼ˆkey=%s, topic=%s, å°è¯•%d/%dï¼‰: %v",
			string(msg.Key), topic, i+1, maxSendRetries, err)

		// ç­‰å¾…åé‡è¯•ï¼ˆæŒ‡æ•°é€€é¿ï¼‰
		if i < maxSendRetries-1 {
			waitTime := time.Duration(1<<uint(i)) * time.Second
			select {
			case <-time.After(waitTime):
				continue
			case <-ctx.Done():
				return ctx.Err()
			}
		}
	}

	return fmt.Errorf("å‘é€åˆ°ä¸»é¢˜%sé‡è¯•%dæ¬¡å‡å¤±è´¥: %v", topic, maxSendRetries, lastErr)
}
