package main

import (
	"context"
	"fmt"
	"log"
	"math"
	"os"
	"strconv"
	"strings"
	"time"

	"github.com/prometheus/client_golang/api"
	v1 "github.com/prometheus/client_golang/api/prometheus/v1"
	"github.com/prometheus/common/model"
)

// é…ç½®å‚æ•°
type Config struct {
	PrometheusURL string        `json:"prometheus_url"`
	Interval      time.Duration `json:"interval"`
	Topic         string        `json:"topic"`
	Groups        []string      `json:"groups"`
	Operation     string        `json:"operation"`
	Table         string        `json:"table"`
}

// æ¶ˆè´¹è€…ç»„æŒ‡æ ‡
type ConsumerGroupMetrics struct {
	Group             string  `json:"group"`
	Received          float64 `json:"received"`
	Processed         float64 `json:"processed"`
	Errors            float64 `json:"errors"`
	Retry             float64 `json:"retry"`
	DeadLetter        float64 `json:"dead_letter"`
	Lag               float64 `json:"lag"`
	ProcessingTime95  float64 `json:"processing_time_95"`
	ProcessingTimeAvg float64 `json:"processing_time_avg"`
	ErrorRate         float64 `json:"error_rate"`
	RetryRate         float64 `json:"retry_rate"`
	DeadLetterRate    float64 `json:"dead_letter_rate"`
	Throughput        float64 `json:"throughput"`
}

// ç”Ÿäº§è€…æŒ‡æ ‡
type ProducerMetrics struct {
	Attempts           float64 `json:"attempts"`
	Success            float64 `json:"success"`
	Failures           float64 `json:"failures"`
	Retries            float64 `json:"retries"`
	DeadLetterMessages float64 `json:"dead_letter_messages"`
	SuccessRate        float64 `json:"success_rate"`
	FailureRate        float64 `json:"failure_rate"`
	RetryRate          float64 `json:"retry_rate"`
	Throughput         float64 `json:"throughput"`
}

// ä¸šåŠ¡å¤„ç†æŒ‡æ ‡
type BusinessMetrics struct {
	Success           float64 `json:"success"`
	Failures          float64 `json:"failures"`
	ProcessingTime95  float64 `json:"processing_time_95"`
	ProcessingTimeAvg float64 `json:"processing_time_avg"`
	SuccessRate       float64 `json:"success_rate"`
	Throughput        float64 `json:"throughput"`
}

// æ¶ˆæ¯å¤„ç†æŒ‡æ ‡
type MessageProcessingMetrics struct {
	Time95     float64 `json:"time_95"`
	TimeAvg    float64 `json:"time_avg"`
	Count      float64 `json:"count"`
	ErrorRate  float64 `json:"error_rate"`
	Throughput float64 `json:"throughput"`
}

// æ•°æ®åº“æŒ‡æ ‡
// æ•°æ®åº“æŒ‡æ ‡
type DatabaseMetrics struct {
	QueryCount   float64            `json:"query_count"`
	QueryErrors  float64            `json:"query_errors"`
	QueryTime95  float64            `json:"query_time_95"`
	QueryTimeAvg float64            `json:"query_time_avg"`
	ErrorRate    float64            `json:"error_rate"`
	Throughput   float64            `json:"throughput"`
	ErrorTypes   map[string]float64 `json:"error_types"`
	Operations   map[string]float64 `json:"operations"` // æ·»åŠ è¿™ä¸ªå­—æ®µ
}

// å®Œæ•´çš„ç›‘æ§æŒ‡æ ‡
type Metrics struct {
	Producer          ProducerMetrics          `json:"producer"`
	ConsumerGroups    []ConsumerGroupMetrics   `json:"consumer_groups"`
	Business          BusinessMetrics          `json:"business"`
	MessageProcessing MessageProcessingMetrics `json:"message_processing"`
	Database          DatabaseMetrics          `json:"database"`

	TotalReceived   float64 `json:"total_received"`
	TotalProcessed  float64 `json:"total_processed"`
	TotalErrors     float64 `json:"total_errors"`
	TotalRetry      float64 `json:"total_retry"`
	TotalDeadLetter float64 `json:"total_dead_letter"`
}

// Prometheuså®¢æˆ·ç«¯
type PrometheusClient struct {
	client v1.API
}

func NewPrometheusClient(url string) (*PrometheusClient, error) {
	client, err := api.NewClient(api.Config{
		Address: url,
	})
	if err != nil {
		return nil, err
	}

	return &PrometheusClient{
		client: v1.NewAPI(client),
	}, nil
}

// æŸ¥è¯¢Prometheus
func (p *PrometheusClient) Query(query string) (float64, error) {
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	result, warnings, err := p.client.Query(ctx, query, time.Now())
	if err != nil {
		return 0, err
	}

	if len(warnings) > 0 {
		log.Printf("Warnings: %v", warnings)
	}

	if result == nil || result.Type() != model.ValVector {
		return 0, nil
	}

	vector := result.(model.Vector)
	if len(vector) == 0 {
		return 0, nil
	}

	return float64(vector[0].Value), nil
}

// æŸ¥è¯¢é€Ÿç‡
func (p *PrometheusClient) QueryRate(query string, duration string) (float64, error) {
	rateQuery := fmt.Sprintf("rate(%s[%s])", query, duration)
	return p.Query(rateQuery)
}

// è·å–ç”Ÿäº§è€…æŒ‡æ ‡
func (p *PrometheusClient) GetProducerMetrics(topic, operation string) (*ProducerMetrics, error) {
	var metrics ProducerMetrics
	var success, failures, retries, deadLetter float64
	var err error

	success, err = p.Query(fmt.Sprintf(`sum(kafka_producer_success_total{topic="%s",operation="%s"})`, topic, operation))
	if err != nil {
		return nil, err
	}
	metrics.Success = success

	failures, err = p.Query(fmt.Sprintf(`sum(kafka_producer_failures_total{topic="%s",operation="%s"})`, topic, operation))
	if err != nil {
		return nil, err
	}
	metrics.Failures = failures

	retries, err = p.Query(fmt.Sprintf(`sum(kafka_producer_retries_total{topic="%s",operation="%s"})`, topic, operation))
	if err != nil {
		return nil, err
	}
	metrics.Retries = retries

	deadLetter, err = p.Query(fmt.Sprintf(`sum(kafka_dead_letter_messages_total{topic="%s",operation="%s"})`, topic, operation))
	if err != nil {
		return nil, err
	}
	metrics.DeadLetterMessages = deadLetter

	metrics.Attempts = metrics.Success + metrics.Failures

	// è®¡ç®—æ¯”ç‡
	if metrics.Attempts > 0 {
		metrics.SuccessRate = (metrics.Success / metrics.Attempts) * 100
		metrics.FailureRate = (metrics.Failures / metrics.Attempts) * 100
		metrics.RetryRate = (metrics.Retries / metrics.Attempts) * 100
	}

	// ååé‡
	metrics.Throughput, err = p.QueryRate(fmt.Sprintf(`kafka_producer_success_total{topic="%s",operation="%s"}`, topic, operation), "5m")
	if err != nil {
		return nil, err
	}

	return &metrics, nil
}

// è·å–æ¶ˆè´¹è€…ç»„æŒ‡æ ‡
func (p *PrometheusClient) GetConsumerGroupMetrics(topic, group, operation string) (*ConsumerGroupMetrics, error) {
	var metrics ConsumerGroupMetrics
	metrics.Group = group

	var received, processed, errors, retry, deadLetter, lag float64
	var err error

	received, err = p.Query(fmt.Sprintf(`sum(kafka_consumer_messages_received_total{topic="%s",group="%s",operation="%s"})`, topic, group, operation))
	if err != nil {
		return nil, err
	}
	metrics.Received = received

	processed, err = p.Query(fmt.Sprintf(`sum(kafka_consumer_messages_processed_total{topic="%s",group="%s",operation="%s"})`, topic, group, operation))
	if err != nil {
		return nil, err
	}
	metrics.Processed = processed

	errors, err = p.Query(fmt.Sprintf(`sum(kafka_consumer_processing_errors_total{topic="%s",group="%s",operation="%s"})`, topic, group, operation))
	if err != nil {
		return nil, err
	}
	metrics.Errors = errors

	retry, err = p.Query(fmt.Sprintf(`sum(kafka_consumer_retry_messages_total{topic="%s",group="%s",operation="%s"})`, topic, group, operation))
	if err != nil {
		return nil, err
	}
	metrics.Retry = retry

	deadLetter, err = p.Query(fmt.Sprintf(`sum(kafka_consumer_dead_letter_messages_total{topic="%s",group="%s",operation="%s"})`, topic, group, operation))
	if err != nil {
		return nil, err
	}
	metrics.DeadLetter = deadLetter

	lag, err = p.Query(fmt.Sprintf(`kafka_consumer_lag{topic="%s",group="%s"}`, topic, group))
	if err != nil {
		return nil, err
	}
	metrics.Lag = lag

	// å¤„ç†æ—¶é—´åˆ†ä½æ•°
	metrics.ProcessingTime95, _ = p.Query(fmt.Sprintf(`histogram_quantile(0.95, sum(rate(kafka_consumer_processing_seconds_bucket{topic="%s",group="%s",operation="%s"}[5m])) by (le))`, topic, group, operation))
	metrics.ProcessingTimeAvg, _ = p.Query(fmt.Sprintf(`rate(kafka_consumer_processing_seconds_sum{topic="%s",group="%s",operation="%s"}[5m]) / rate(kafka_consumer_processing_seconds_count{topic="%s",group="%s",operation="%s"}[5m])`, topic, group, operation, topic, group, operation))

	// ååé‡
	metrics.Throughput, err = p.QueryRate(fmt.Sprintf(`kafka_consumer_messages_processed_total{topic="%s",group="%s",operation="%s"}`, topic, group, operation), "5m")
	if err != nil {
		return nil, err
	}

	// è®¡ç®—æ¯”ç‡
	if metrics.Received > 0 {
		metrics.ErrorRate = (metrics.Errors / metrics.Received) * 100
		metrics.RetryRate = (metrics.Retry / metrics.Received) * 100
		metrics.DeadLetterRate = (metrics.DeadLetter / metrics.Received) * 100
	}

	return &metrics, nil
}

// è·å–ä¸šåŠ¡æŒ‡æ ‡
// è·å–ä¸šåŠ¡æŒ‡æ ‡ï¼ˆä¿®æ­£ç‰ˆï¼‰
func (p *PrometheusClient) GetBusinessMetrics(operation string) (*BusinessMetrics, error) {
	var metrics BusinessMetrics

	// æ ¹æ®ä½ çš„å®é™…æŒ‡æ ‡è°ƒæ•´operationåç§°
	// ä» "create" æ”¹ä¸º "user_create"
	actualOperation := "user_create"
	if operation == "create" {
		actualOperation = "user_create"
	} else if operation == "update" {
		actualOperation = "user_update"
	}

	var success, failures float64
	var err error

	success, err = p.Query(fmt.Sprintf(`sum(business_operations_success_total{operation="%s"})`, actualOperation))
	if err != nil {
		return nil, err
	}
	metrics.Success = success

	failures, err = p.Query(fmt.Sprintf(`sum(business_operations_failures_total{operation="%s"})`, actualOperation))
	if err != nil {
		// å¤±è´¥æŒ‡æ ‡å¯èƒ½ä¸å­˜åœ¨ï¼Œå¿½ç•¥é”™è¯¯
		metrics.Failures = 0
	} else {
		metrics.Failures = failures
	}

	// å¤„ç†æ—¶é—´
	metrics.ProcessingTime95, _ = p.Query(fmt.Sprintf(`histogram_quantile(0.95, sum(rate(business_processing_seconds_bucket{operation="%s"}[5m])) by (le))`, actualOperation))
	metrics.ProcessingTimeAvg, _ = p.Query(fmt.Sprintf(`rate(business_processing_seconds_sum{operation="%s"}[5m]) / rate(business_processing_seconds_count{operation="%s"}[5m])`, actualOperation, actualOperation))

	// æˆåŠŸç‡
	total := metrics.Success + metrics.Failures
	if total > 0 {
		metrics.SuccessRate = (metrics.Success / total) * 100
	}

	// ååé‡
	metrics.Throughput, err = p.QueryRate(fmt.Sprintf(`business_operations_success_total{operation="%s"}`, actualOperation), "5m")
	if err != nil {
		return nil, err
	}

	return &metrics, nil
}

// è·å–æ¶ˆæ¯å¤„ç†æŒ‡æ ‡
func (p *PrometheusClient) GetMessageProcessingMetrics(topic, operation string) (*MessageProcessingMetrics, error) {
	var metrics MessageProcessingMetrics
	var err error

	metrics.Time95, _ = p.Query(fmt.Sprintf(`histogram_quantile(0.95, sum(rate(kafka_message_processing_seconds_bucket{topic="%s",operation="%s"}[5m])) by (le))`, topic, operation))
	metrics.TimeAvg, _ = p.Query(fmt.Sprintf(`rate(kafka_message_processing_seconds_sum{topic="%s",operation="%s"}[5m]) / rate(kafka_message_processing_seconds_count{topic="%s",operation="%s"}[5m])`, topic, operation, topic, operation))
	metrics.Count, err = p.QueryRate(fmt.Sprintf(`kafka_message_processing_seconds_count{topic="%s",operation="%s"}`, topic, operation), "5m")
	if err != nil {
		return nil, err
	}

	// é”™è¯¯ç‡
	errorCount, _ := p.QueryRate(fmt.Sprintf(`kafka_consumer_processing_errors_total{topic="%s",operation="%s"}`, topic, operation), "5m")
	if metrics.Count > 0 {
		metrics.ErrorRate = (errorCount / metrics.Count) * 100
	}

	metrics.Throughput = metrics.Count

	return &metrics, nil
}

// è·å–æ•°æ®åº“æŒ‡æ ‡
// è·å–æ•°æ®åº“æŒ‡æ ‡ï¼ˆè°ƒè¯•ç‰ˆæœ¬ï¼‰
func (p *PrometheusClient) GetDatabaseMetrics() (*DatabaseMetrics, error) {
	var metrics DatabaseMetrics
	metrics.ErrorTypes = make(map[string]float64)
	metrics.Operations = make(map[string]float64)
	p.getErrorTypeDistribution(&metrics)

	// è·å–æ“ä½œç±»å‹åˆ†å¸ƒ
	p.getOperationDistribution(&metrics)
	// è°ƒè¯•è¾“å‡º
	log.Printf("æ­£åœ¨æŸ¥è¯¢æ•°æ®åº“æŒ‡æ ‡...")

	// æ€»æŸ¥è¯¢æ¬¡æ•°
	queryCount, err := p.Query(`sum(rate(database_query_duration_seconds_count[5m]))`)
	if err != nil {
		log.Printf("æŸ¥è¯¢æ¬¡æ•°æŸ¥è¯¢å¤±è´¥: %v", err)
	} else {
		metrics.QueryCount = queryCount
		log.Printf("æŸ¥è¯¢æ¬¡æ•°ç»“æœ: %.2f", queryCount)
	}

	// æ€»é”™è¯¯æ•°
	queryErrors, err := p.Query(`sum(rate(database_query_errors_total[5m]))`)
	if err != nil {
		log.Printf("é”™è¯¯æ•°æŸ¥è¯¢å¤±è´¥: %v", err)
	} else {
		metrics.QueryErrors = queryErrors
		log.Printf("é”™è¯¯æ•°ç»“æœ: %.2f", queryErrors)
	}

	// æŸ¥è¯¢æ—¶é—´åˆ†ä½æ•°
	metrics.QueryTime95, _ = p.Query(`histogram_quantile(0.95, sum(rate(database_query_duration_seconds_bucket[5m])) by (le))`)
	log.Printf("P95æŸ¥è¯¢æ—¶é—´: %.3f", metrics.QueryTime95)

	// å¹³å‡æŸ¥è¯¢æ—¶é—´
	sum, _ := p.Query(`sum(rate(database_query_duration_seconds_sum[5m]))`)
	count, _ := p.Query(`sum(rate(database_query_duration_seconds_count[5m]))`)
	if count > 0 {
		metrics.QueryTimeAvg = sum / count
		log.Printf("å¹³å‡æŸ¥è¯¢æ—¶é—´: %.3f (sum=%.3f, count=%.3f)", metrics.QueryTimeAvg, sum, count)
	}

	// é”™è¯¯ç‡ - ä¿®å¤è®¡ç®—é€»è¾‘
	if metrics.QueryCount > 0 {
		metrics.ErrorRate = (metrics.QueryErrors / metrics.QueryCount) * 100
	} else if metrics.QueryErrors > 0 {
		metrics.ErrorRate = 100 // å¦‚æœåªæœ‰é”™è¯¯æ²¡æœ‰æˆåŠŸæŸ¥è¯¢
	} else {
		metrics.ErrorRate = 0
	}
	log.Printf("é”™è¯¯ç‡: %.1f%% (é”™è¯¯æ•°=%.0f, æŸ¥è¯¢æ•°=%.0f)", metrics.ErrorRate, metrics.QueryErrors, metrics.QueryCount)

	metrics.Throughput = metrics.QueryCount

	// è·å–é”™è¯¯ç±»å‹åˆ†å¸ƒ
	log.Printf("è·å–é”™è¯¯ç±»å‹åˆ†å¸ƒ...")
	errorResults, err := p.queryWithLabels(`sum by (error_type) (rate(database_query_errors_total[5m]))`)
	if err != nil {
		log.Printf("é”™è¯¯ç±»å‹æŸ¥è¯¢å¤±è´¥: %v", err)
	} else {
		log.Printf("é”™è¯¯ç±»å‹ç»“æœæ•°é‡: %d", len(errorResults))
		for i, result := range errorResults {
			log.Printf("é”™è¯¯ç±»å‹ç»“æœ[%d]: %+v", i, result)
			if errorType, exists := result["error_type"]; exists {
				if value, exists := result["value"]; exists {
					if count, err := strconv.ParseFloat(value, 64); err == nil && count > 0 {
						metrics.ErrorTypes[errorType] = count
						log.Printf("é”™è¯¯ç±»å‹ %s: %.0f", errorType, count)
					}
				}
			}
		}
	}

	// è·å–æ“ä½œç±»å‹åˆ†å¸ƒ
	log.Printf("è·å–æ“ä½œç±»å‹åˆ†å¸ƒ...")
	operationResults, err := p.queryWithLabels(`sum by (operation) (rate(database_query_duration_seconds_count[5m]))`)
	if err != nil {
		log.Printf("æ“ä½œç±»å‹æŸ¥è¯¢å¤±è´¥: %v", err)
	} else {
		log.Printf("æ“ä½œç±»å‹ç»“æœæ•°é‡: %d", len(operationResults))
		for i, result := range operationResults {
			log.Printf("æ“ä½œç±»å‹ç»“æœ[%d]: %+v", i, result)
			if operation, exists := result["operation"]; exists {
				if value, exists := result["value"]; exists {
					if count, err := strconv.ParseFloat(value, 64); err == nil && count > 0 {
						metrics.Operations[operation] = count
						log.Printf("æ“ä½œç±»å‹ %s: %.0f", operation, count)
					}
				}
			}
		}
	}

	log.Printf("æ•°æ®åº“æŒ‡æ ‡è·å–å®Œæˆ")
	return &metrics, nil
}

// è·å–å®Œæ•´çš„å®æ—¶æŒ‡æ ‡
func (p *PrometheusClient) GetRealtimeMetrics(topic string, groups []string, operation string) (*Metrics, error) {
	var metrics Metrics
	var err error

	// ç”Ÿäº§è€…æŒ‡æ ‡
	producerMetrics, err := p.GetProducerMetrics(topic, operation)
	if err != nil {
		log.Printf("è·å–ç”Ÿäº§è€…æŒ‡æ ‡å¤±è´¥: %v", err)
	} else {
		metrics.Producer = *producerMetrics
	}

	// æ¶ˆè´¹è€…ç»„æŒ‡æ ‡
	for _, group := range groups {
		groupMetrics, err := p.GetConsumerGroupMetrics(topic, group, operation)
		if err != nil {
			log.Printf("è·å–æ¶ˆè´¹è€…ç»„ %s æŒ‡æ ‡å¤±è´¥: %v", group, err)
			continue
		}
		metrics.ConsumerGroups = append(metrics.ConsumerGroups, *groupMetrics)
		metrics.TotalReceived += groupMetrics.Received
		metrics.TotalProcessed += groupMetrics.Processed
		metrics.TotalErrors += groupMetrics.Errors
		metrics.TotalRetry += groupMetrics.Retry
		metrics.TotalDeadLetter += groupMetrics.DeadLetter
	}

	// ä¸šåŠ¡æŒ‡æ ‡
	businessMetrics, err := p.GetBusinessMetrics(operation)
	if err != nil {
		log.Printf("è·å–ä¸šåŠ¡æŒ‡æ ‡å¤±è´¥: %v", err)
	} else {
		metrics.Business = *businessMetrics
	}

	// æ¶ˆæ¯å¤„ç†æŒ‡æ ‡
	messageMetrics, err := p.GetMessageProcessingMetrics(topic, operation)
	if err != nil {
		log.Printf("è·å–æ¶ˆæ¯å¤„ç†æŒ‡æ ‡å¤±è´¥: %v", err)
	} else {
		metrics.MessageProcessing = *messageMetrics
	}

	// æ•°æ®åº“æŒ‡æ ‡
	databaseMetrics, err := p.GetDatabaseMetrics()
	if err != nil {
		log.Printf("è·å–æ•°æ®åº“æŒ‡æ ‡å¤±è´¥: %v", err)
	} else {
		metrics.Database = *databaseMetrics
	}

	return &metrics, nil
}

// ä¿®æ­£ï¼šæ•°æ®åº“æŒ‡æ ‡æ˜¾ç¤º
// å®Œæ•´çš„æ˜¾ç¤ºé¢æ¿å‡½æ•°ï¼ˆä¿®æ­£ç‰ˆï¼‰
func displayDashboard(metrics *Metrics, config *Config) {
	fmt.Print("\033[2J\033[H")
	fmt.Printf("\033[1;34m=================================================\033[0m\n")
	fmt.Printf("\033[1;34m    Kafkaå…¨é“¾è·¯ç›‘æ§ä»ªè¡¨æ¿ (åˆ·æ–°é—´éš”: %v)   \033[0m\n", config.Interval)
	fmt.Printf("\033[1;34m=================================================\033[0m\n")
	fmt.Printf("ä¸»é¢˜: %s  æ“ä½œ: %s  æ¶ˆè´¹è€…ç»„: %v\n", config.Topic, config.Operation, config.Groups)
	fmt.Printf("æ—¶é—´: %s\n", time.Now().Format("2006-01-02 15:04:05"))
	fmt.Printf("\033[1;34m-------------------------------------------------\033[0m\n")

	// 1. ç”Ÿäº§è€…æŒ‡æ ‡
	fmt.Printf("\033[1;32mğŸ“Š ç”Ÿäº§è€…æŒ‡æ ‡:\033[0m\n")
	fmt.Printf("   å°è¯•: %-8.0f  æˆåŠŸ: %-8.0f (%.1f%%)  å¤±è´¥: %-8.0f (%.1f%%)\n",
		metrics.Producer.Attempts, metrics.Producer.Success, metrics.Producer.SuccessRate,
		metrics.Producer.Failures, metrics.Producer.FailureRate)
	fmt.Printf("   é‡è¯•: %-8.0f (%.1f%%)  ååé‡: %.1f msg/s\n",
		metrics.Producer.Retries, metrics.Producer.RetryRate, metrics.Producer.Throughput)

	// 2. æ¶ˆè´¹è€…ç»„æŒ‡æ ‡
	fmt.Printf("\033[1;34m-------------------------------------------------\033[0m\n")
	fmt.Printf("\033[1;32mğŸ“Š æ¶ˆè´¹è€…ç»„æŒ‡æ ‡ (æ€»è®¡: æ¥æ”¶=%.0f, å¤„ç†=%.0f, é”™è¯¯=%.0f):\033[0m\n",
		metrics.TotalReceived, metrics.TotalProcessed, metrics.TotalErrors)

	for _, group := range metrics.ConsumerGroups {
		fmt.Printf("   %s:\n", group.Group)
		fmt.Printf("     æ¥æ”¶: %-8.0f  å¤„ç†: %-8.0f  é”™è¯¯: %-8.0f (%.1f%%)\n",
			group.Received, group.Processed, group.Errors, group.ErrorRate)
		fmt.Printf("     é‡è¯•: %-8.0f (%.1f%%)  æ­»ä¿¡: %-8.0f (%.1f%%)  å»¶è¿Ÿ: %.0f\n",
			group.Retry, group.RetryRate, group.DeadLetter, group.DeadLetterRate, group.Lag)
		fmt.Printf("     å¤„ç†æ—¶é—´: P95=%.3fs, å¹³å‡=%.3fs  ååé‡: %.1f msg/s\n",
			group.ProcessingTime95, group.ProcessingTimeAvg, group.Throughput)
	}

	// 3. æ¶ˆæ¯å¤„ç†æŒ‡æ ‡
	fmt.Printf("\033[1;34m-------------------------------------------------\033[0m\n")
	fmt.Printf("\033[1;32mğŸ“Š æ¶ˆæ¯å¤„ç†æŒ‡æ ‡:\033[0m\n")
	fmt.Printf("   å¤„ç†æ—¶é—´: P95=%-8.3fs  å¹³å‡=%-8.3fs  ååé‡: %.1f msg/s\n",
		metrics.MessageProcessing.Time95, metrics.MessageProcessing.TimeAvg, metrics.MessageProcessing.Throughput)
	fmt.Printf("   é”™è¯¯ç‡: %.1f%%\n", metrics.MessageProcessing.ErrorRate)

	// 4. ä¸šåŠ¡æŒ‡æ ‡
	fmt.Printf("\033[1;34m-------------------------------------------------\033[0m\n")
	fmt.Printf("\033[1;32mğŸ“Š ä¸šåŠ¡æŒ‡æ ‡:\033[0m\n")
	fmt.Printf("   æˆåŠŸ: %-8.0f  å¤±è´¥: %-8.0f  æˆåŠŸç‡: %.1f%%\n",
		metrics.Business.Success, metrics.Business.Failures, metrics.Business.SuccessRate)
	fmt.Printf("   å¤„ç†æ—¶é—´: P95=%-8.3fs  å¹³å‡=%-8.3fs  ååé‡: %.1f ops/s\n",
		metrics.Business.ProcessingTime95, metrics.Business.ProcessingTimeAvg, metrics.Business.Throughput)

	// 5. æ•°æ®åº“æŒ‡æ ‡
	fmt.Printf("\033[1;34m-------------------------------------------------\033[0m\n")
	fmt.Printf("\033[1;32mğŸ“Š æ•°æ®åº“æŒ‡æ ‡:\033[0m\n")

	// æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
	hasDatabaseData := metrics.Database.QueryCount > 0 || metrics.Database.QueryErrors > 0
	hasErrorTypes := false
	hasOperations := false

	// æ£€æŸ¥é”™è¯¯ç±»å‹æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
	for _, count := range metrics.Database.ErrorTypes {
		if count > 0.001 {
			hasErrorTypes = true
			break
		}
	}

	// æ£€æŸ¥æ“ä½œç±»å‹æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
	for _, count := range metrics.Database.Operations {
		if count > 0.001 {
			hasOperations = true
			break
		}
	}

	if hasDatabaseData {
		fmt.Printf("   æŸ¥è¯¢æ¬¡æ•°: %-8.3f/s  é”™è¯¯: %-8.3f (%.1f%%)  ååé‡: %.3f qps\n",
			metrics.Database.QueryCount, metrics.Database.QueryErrors,
			metrics.Database.ErrorRate, metrics.Database.Throughput)
		fmt.Printf("   æŸ¥è¯¢æ—¶é—´: P95=%-8.3fs  å¹³å‡=%-8.3fs\n",
			metrics.Database.QueryTime95, metrics.Database.QueryTimeAvg)
	} else {
		fmt.Printf("   ğŸ“Š æ— æ•°æ®åº“æŸ¥è¯¢æ•°æ®\n")
	}

	// é”™è¯¯ç±»å‹åˆ†å¸ƒ - åªæ˜¾ç¤ºæœ‰æ•°æ®çš„ç±»å‹
	if hasErrorTypes {
		fmt.Printf("   ğŸ”´ é”™è¯¯ç±»å‹åˆ†å¸ƒ:\n")
		totalErrors := metrics.Database.QueryErrors
		for errorType, count := range metrics.Database.ErrorTypes {
			if count > 0.001 {
				percentage := 0.0
				if totalErrors > 0 {
					percentage = (count / totalErrors) * 100
				}
				errorDesc := getErrorTypeDescription(errorType)
				fmt.Printf("     %-20s: %8.3f/s (%5.1f%%)\n", errorDesc, count, percentage)
			}
		}
	} else if hasDatabaseData {
		fmt.Printf("   ğŸ”´ é”™è¯¯ç±»å‹: æ— é”™è¯¯æ•°æ®\n")
	}

	// æ“ä½œç±»å‹åˆ†å¸ƒ - åªæ˜¾ç¤ºæœ‰æ•°æ®çš„ç±»å‹
	if hasOperations {
		fmt.Printf("   ğŸ“‹ æ“ä½œç±»å‹åˆ†å¸ƒ:\n")
		totalOperations := metrics.Database.QueryCount
		for operation, count := range metrics.Database.Operations {
			if count > 0.001 {
				percentage := 0.0
				if totalOperations > 0 {
					percentage = (count / totalOperations) * 100
				}
				operationDesc := getOperationDescription(operation)
				fmt.Printf("     %-20s: %8.3f/s (%5.1f%%)\n", operationDesc, count, percentage)
			}
		}
	} else if hasDatabaseData {
		fmt.Printf("   ğŸ“‹ æ“ä½œç±»å‹: æ— æ“ä½œæ•°æ®\n")
	}

	// 6. æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
	fmt.Printf("\033[1;34m-------------------------------------------------\033[0m\n")
	fmt.Printf("\033[1;35mâœ… æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥:\033[0m\n")

	// ç”Ÿäº§æ¶ˆè´¹ä¸€è‡´æ€§
	producerConsumerDiff := metrics.Producer.Success - metrics.TotalReceived
	if math.Abs(producerConsumerDiff) > 100 {
		fmt.Printf("   ğŸ”´ ç”Ÿäº§æ¶ˆè´¹ä¸ä¸€è‡´: ç”Ÿäº§(%.0f) â‰  æ¶ˆè´¹(%.0f), å·®å¼‚: %.0f\n",
			metrics.Producer.Success, metrics.TotalReceived, producerConsumerDiff)
	} else {
		fmt.Printf("   âœ… ç”Ÿäº§æ¶ˆè´¹ä¸€è‡´: ç”Ÿäº§=%.0f, æ¶ˆè´¹=%.0f\n",
			metrics.Producer.Success, metrics.TotalReceived)
	}

	// é”™è¯¯å¤„ç†ä¸€è‡´æ€§
	if metrics.TotalErrors > 0 {
		totalErrorHandling := metrics.TotalRetry + metrics.TotalDeadLetter
		errorHandlingDiff := metrics.TotalErrors - totalErrorHandling
		if math.Abs(errorHandlingDiff) > 10 {
			fmt.Printf("   ğŸ”´ é”™è¯¯å¤„ç†ä¸ä¸€è‡´: æ€»é”™è¯¯(%.0f) â‰  é‡è¯•(%.0f)+æ­»ä¿¡(%.0f)=%.0f, å·®å¼‚: %.0f\n",
				metrics.TotalErrors, metrics.TotalRetry, metrics.TotalDeadLetter, totalErrorHandling, errorHandlingDiff)
		} else {
			fmt.Printf("   âœ… é”™è¯¯å¤„ç†ä¸€è‡´: é”™è¯¯=%.0f, é‡è¯•+æ­»ä¿¡=%.0f\n",
				metrics.TotalErrors, totalErrorHandling)
		}
	} else {
		fmt.Printf("   âœ… æ— é”™è¯¯éœ€è¦å¤„ç†\n")
	}

	// ä¸šåŠ¡å¤„ç†ä¸€è‡´æ€§
	if metrics.TotalProcessed > 0 && metrics.Business.Success > 0 {
		businessDiff := metrics.TotalProcessed - metrics.Business.Success
		if math.Abs(businessDiff) > 50 {
			fmt.Printf("   ğŸ”´ ä¸šåŠ¡å¤„ç†ä¸ä¸€è‡´: æ¶ˆè´¹å¤„ç†(%.0f) â‰  ä¸šåŠ¡æˆåŠŸ(%.0f), å·®å¼‚: %.0f\n",
				metrics.TotalProcessed, metrics.Business.Success, businessDiff)
		} else {
			fmt.Printf("   âœ… ä¸šåŠ¡å¤„ç†ä¸€è‡´: æ¶ˆè´¹å¤„ç†=%.0f, ä¸šåŠ¡æˆåŠŸ=%.0f\n",
				metrics.TotalProcessed, metrics.Business.Success)
		}
	} else {
		fmt.Printf("   âš ï¸  ä¸šåŠ¡æ•°æ®ä¸è¶³: å¤„ç†=%.0f, æˆåŠŸ=%.0f\n",
			metrics.TotalProcessed, metrics.Business.Success)
	}

	// æ•°æ®åº“æ“ä½œæ¯”ä¾‹
	if metrics.TotalProcessed > 0 && metrics.Database.QueryCount > 0 {
		dbOperationRatio := metrics.Database.QueryCount / metrics.TotalProcessed
		if dbOperationRatio < 0.5 || dbOperationRatio > 5 {
			fmt.Printf("   âš ï¸  æ•°æ®åº“æ“ä½œæ¯”ä¾‹å¼‚å¸¸: æ¯ä¸ªæ¶ˆæ¯ %.1f æ¬¡æ•°æ®åº“æ“ä½œ\n", dbOperationRatio)
		} else {
			fmt.Printf("   âœ… æ•°æ®åº“æ“ä½œæ­£å¸¸: æ¯ä¸ªæ¶ˆæ¯ %.1f æ¬¡æ•°æ®åº“æ“ä½œ\n", dbOperationRatio)
		}
	}

	// å»¶è¿Ÿæ£€æŸ¥
	highLagGroups := 0
	for _, group := range metrics.ConsumerGroups {
		if group.Lag > 1000 {
			highLagGroups++
			fmt.Printf("   ğŸ”´ é«˜å»¶è¿Ÿ: %s ç»„å»¶è¿Ÿ %.0f æ¡æ¶ˆæ¯\n", group.Group, group.Lag)
		}
	}
	if highLagGroups == 0 {
		fmt.Printf("   âœ… æ‰€æœ‰æ¶ˆè´¹è€…ç»„å»¶è¿Ÿæ­£å¸¸\n")
	}

	// é”™è¯¯ç‡æ£€æŸ¥
	if metrics.Producer.FailureRate > 5 {
		fmt.Printf("   ğŸ”´ ç”Ÿäº§è€…é”™è¯¯ç‡é«˜: %.1f%%\n", metrics.Producer.FailureRate)
	} else {
		fmt.Printf("   âœ… ç”Ÿäº§è€…é”™è¯¯ç‡æ­£å¸¸: %.1f%%\n", metrics.Producer.FailureRate)
	}

	fmt.Printf("\033[1;34m=================================================\033[0m\n")
}

func main() {
	// é»˜è®¤é…ç½®
	config := &Config{
		PrometheusURL: "http://localhost:9090",
		Interval:      60 * time.Second,
		Topic:         "user.create.v1",
		Groups:        []string{"user-create-group", "user-service"},
		Operation:     "create",
		Table:         "users",
	}

	// è§£æå‘½ä»¤è¡Œå‚æ•°
	args := os.Args[1:]
	for i := 0; i < len(args); i++ {
		switch args[i] {
		case "-url":
			if i+1 < len(args) {
				config.PrometheusURL = args[i+1]
				i++
			}
		case "-interval":
			if i+1 < len(args) {
				if interval, err := strconv.Atoi(args[i+1]); err == nil {
					config.Interval = time.Duration(interval) * time.Second
				}
				i++
			}
		case "-topic":
			if i+1 < len(args) {
				config.Topic = args[i+1]
				i++
			}
		case "-groups":
			if i+1 < len(args) {
				config.Groups = strings.Split(args[i+1], ",")
				i++
			}
		case "-operation":
			if i+1 < len(args) {
				config.Operation = args[i+1]
				i++
			}
		case "-table":
			if i+1 < len(args) {
				config.Table = args[i+1]
				i++
			}
		case "-help":
			fmt.Println("Usage: kafka-full-monitor [options]")
			fmt.Println("Options:")
			fmt.Println("  -url <url>          Prometheus URL (default: http://localhost:9090)")
			fmt.Println("  -interval <seconds> Refresh interval in seconds (default: 5)")
			fmt.Println("  -topic <topic>      Kafka topic (default: user.create.v1)")
			fmt.Println("  -groups <groups>    Consumer groups (comma separated)")
			fmt.Println("  -operation <op>     Operation type (default: create)")
			fmt.Println("  -table <table>      Database table name (default: users)")
			fmt.Println("  -help               Show this help message")
			os.Exit(0)
		}
	}

	// åˆ›å»ºPrometheuså®¢æˆ·ç«¯
	client, err := NewPrometheusClient(config.PrometheusURL)
	if err != nil {
		log.Fatalf("åˆ›å»ºPrometheuså®¢æˆ·ç«¯å¤±è´¥: %v", err)
	}

	fmt.Printf("\033[1;32mğŸš€ å¯åŠ¨Kafkaå…¨é“¾è·¯ç›‘æ§...\033[0m\n")
	fmt.Printf("Prometheusåœ°å€: %s\n", config.PrometheusURL)
	fmt.Printf("ç›‘æ§é…ç½®: topic=%s, groups=%v, operation=%s, table=%s\n",
		config.Topic, config.Groups, config.Operation, config.Table)
	fmt.Printf("åˆ·æ–°é—´éš”: %v\n", config.Interval)
	fmt.Printf("æŒ‰ Ctrl+C åœæ­¢ç›‘æ§\n")
	time.Sleep(2 * time.Second)

	// ç›‘æ§å¾ªç¯
	for {
		metrics, err := client.GetRealtimeMetrics(config.Topic, config.Groups, config.Operation)
		if err != nil {
			log.Printf("è·å–æŒ‡æ ‡å¤±è´¥: %v", err)
			time.Sleep(config.Interval)
			continue
		}

		displayDashboard(metrics, config)
		time.Sleep(config.Interval)
	}
}

// æ–°å¢ï¼šæŸ¥è¯¢å¸¦æ ‡ç­¾çš„ç»“æœ
// ä¿®æ­£ï¼šæŸ¥è¯¢å¸¦æ ‡ç­¾çš„ç»“æœï¼ˆå¤„ç†æµ®ç‚¹æ•°å€¼ï¼‰
func (p *PrometheusClient) queryWithLabels(query string) ([]map[string]string, error) {
	ctx, cancel := context.WithTimeout(context.Background(), 10*time.Second)
	defer cancel()

	result, warnings, err := p.client.Query(ctx, query, time.Now())
	if err != nil {
		return nil, err
	}

	if len(warnings) > 0 {
		log.Printf("Warnings: %v", warnings)
	}

	var results []map[string]string

	if result != nil && result.Type() == model.ValVector {
		vector := result.(model.Vector)
		for _, sample := range vector {
			resultMap := make(map[string]string)
			for key, value := range sample.Metric {
				resultMap[string(key)] = string(value)
			}
			// ä¿®æ­£ï¼šä¿ç•™å°æ•°ä½ï¼Œä¸è¦ç›´æ¥å–æ•´
			resultMap["value"] = fmt.Sprintf("%.6f", float64(sample.Value))
			results = append(results, resultMap)
		}
	}

	return results, nil
}

// ä¿®æ­£ï¼šè·å–é”™è¯¯ç±»å‹å’Œæ“ä½œç±»å‹åˆ†å¸ƒ
// è·å–é”™è¯¯ç±»å‹åˆ†å¸ƒ
func (p *PrometheusClient) getErrorTypeDistribution(metrics *DatabaseMetrics) {
	errorResults, err := p.queryWithLabels(`sum by (error_type) (rate(database_query_errors_total[5m]))`)
	if err != nil {
		log.Printf("é”™è¯¯ç±»å‹æŸ¥è¯¢å¤±è´¥: %v", err)
		return
	}

	for _, result := range errorResults {
		if errorType, exists := result["error_type"]; exists {
			if valueStr, exists := result["value"]; exists {
				// ä¿®æ­£ï¼šæ­£ç¡®è§£ææµ®ç‚¹æ•°å€¼
				if value, err := strconv.ParseFloat(valueStr, 64); err == nil {
					metrics.ErrorTypes[errorType] = value
					log.Printf("é”™è¯¯ç±»å‹ %s: %f", errorType, value)
				} else {
					log.Printf("è§£æé”™è¯¯ç±»å‹å€¼å¤±è´¥: %s, error: %v", valueStr, err)
				}
			}
		}
	}
}

// è·å–æ“ä½œç±»å‹åˆ†å¸ƒ
func (p *PrometheusClient) getOperationDistribution(metrics *DatabaseMetrics) {
	operationResults, err := p.queryWithLabels(`sum by (operation) (rate(database_query_duration_seconds_count[5m]))`)
	if err != nil {
		log.Printf("æ“ä½œç±»å‹æŸ¥è¯¢å¤±è´¥: %v", err)
		return
	}

	for _, result := range operationResults {
		if operation, exists := result["operation"]; exists {
			if valueStr, exists := result["value"]; exists {
				// ä¿®æ­£ï¼šæ­£ç¡®è§£ææµ®ç‚¹æ•°å€¼
				if value, err := strconv.ParseFloat(valueStr, 64); err == nil {
					metrics.Operations[operation] = value
					log.Printf("æ“ä½œç±»å‹ %s: %f", operation, value)
				} else {
					log.Printf("è§£ææ“ä½œç±»å‹å€¼å¤±è´¥: %s, error: %v", valueStr, err)
				}
			}
		}
	}
}

// é”™è¯¯ç±»å‹æ˜ å°„
var errorTypeMapping = map[string]string{
	"constraint_violation":  "å”¯ä¸€ç´¢å¼•å†²çª",
	"timeout":               "è¶…æ—¶",
	"database_error":        "æ•°æ®åº“é”™è¯¯",
	"network_error":         "ç½‘ç»œé”™è¯¯",
	"deadlock":              "æ­»é”",
	"foreign_key_violation": "å¤–é”®çº¦æŸå†²çª",
	"unique_violation":      "å”¯ä¸€çº¦æŸå†²çª",
	"connection_error":      "è¿æ¥é”™è¯¯",
	"permanent_error":       "æ°¸ä¹…é”™è¯¯",
	"unmarshal_error":       "æ•°æ®è§£æé”™è¯¯",
}

// æ“ä½œç±»å‹æ˜ å°„
var operationMapping = map[string]string{
	"check_exists": "æ£€æŸ¥å­˜åœ¨",
	"create":       "åˆ›å»º",
	"insert":       "æ’å…¥",
	"select":       "æŸ¥è¯¢",
	"update":       "æ›´æ–°",
	"delete":       "åˆ é™¤",
}

func getErrorTypeDescription(errorType string) string {
	if desc, exists := errorTypeMapping[errorType]; exists {
		return desc
	}
	return errorType
}

func getOperationDescription(operation string) string {
	if desc, exists := operationMapping[operation]; exists {
		return desc
	}
	return operation
}
