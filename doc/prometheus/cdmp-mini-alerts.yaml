# Prometheus alerting rules for cdmp-mini
# Place this file in your Prometheus `rules` directory or create a PrometheusRule (CRD)
# Adjust thresholds to your environment (node count, partition count, DB capacity)

groups:
- name: cdmp-mini.rules
  rules:

  - alert: HighConsumerLag
    expr: kafka_consumer_lag{topic="users"} > 10000
    for: 5m
    labels:
      severity: critical
      service: cdmp-mini
    annotations:
      summary: "High consumer lag for cdmp-mini (topic=users)"
      description: |
        kafka_consumer_lag for topic=users is above 10000 for more than 5 minutes.
        Possible causes: DB is slow / lock contention / consumers are down or under-provisioned.

        Runbook:
        - Check consumer instance count and their logs for errors.
        - Check DB (95/99 latency, slow queries, locks).
        - If DB is the bottleneck: optimize queries, scale DB, or temporarily reduce write traffic via admin API.
        - If consumers are under-provisioned: increase consumer replicas or workerCount.

  - alert: ProducerInFlightHigh
    expr: kafka_producer_inflight_current > 800
    for: 5m
    labels:
      severity: warning
      service: cdmp-mini
    annotations:
      summary: "Producer in-flight count high"
      description: |
        kafka_producer_inflight_current is above 800 for more than 5 minutes.
        Default ProducerMaxInFlight is 1000; adjust threshold according to your configured limit.

        Runbook:
        - Check ProducerInFlightCurrent and ProducerMaxInFlight config.
        - If producers are saturated: consider temporarily lowering global write limit, check Kafka brokers health, and investigate producer errors/latency.
        - Avoid arbitrarily increasing ProducerMaxInFlight without ensuring downstream capacity.

  - alert: DeadLetterMessagesSeen
    expr: increase(kafka_dead_letter_messages_total[5m]) > 0
    for: 1m
    labels:
      severity: critical
      service: cdmp-mini
    annotations:
      summary: "Messages arriving at Dead Letter queue"
      description: |
        kafka_dead_letter_messages_total increased in the last 5m. Messages are being moved to DLQ.

        Runbook:
        - Inspect dead-letter topic contents to determine failure reasons.
        - Typical causes: payload schema errors, unrecoverable processing errors.
        - If many messages land in DLQ, consider pausing producers or running remediation to repair and replay messages.

  - alert: ProducerFailuresSpike
    expr: increase(kafka_producer_failures_total[5m]) > 50
    for: 5m
    labels:
      severity: warning
      service: cdmp-mini
    annotations:
      summary: "Spike in producer failures"
      description: |
        kafka_producer_failures_total increased significantly in the last 5 minutes.

        Runbook:
        - Check Kafka broker connectivity and leader/ISR status.
        - Check network errors and producer logs.
        - If failures coincide with increased load, consider throttling writes until brokers recover.

  - alert: WriteRateLimiterSpike
    expr: sum by (path, reason) (rate(write_rate_limiter_total[1m])) > 10
    for: 5m
    labels:
      severity: warning
      service: cdmp-mini
    annotations:
      summary: "High write rate limiter triggers"
      description: |
        write_rate_limiter_total is triggering frequently for some path(s) and reason(s).

        Runbook:
        - Identify path/reason from alert labels.
        - If reason==redis_timeout, check Redis latency and availability.
        - If reason==local_rate, consider moving throttling upstream (gateway/client) or increase capacity.

  - alert: ConsumerCommitFailures
    expr: increase(kafka_consumer_commit_failures_total[5m]) > 10
    for: 5m
    labels:
      severity: warning
      service: cdmp-mini
    annotations:
      summary: "Consumer commit failures are occurring"
      description: |
        Consumer commit operations are failing repeatedly. This may cause duplicate processing or offsets not being advanced.

        Runbook:
        - Check consumer logs for commit errors.
        - Investigate broker connectivity or timeout issues.
        - If transient, commits may recover; if persistent, restart consumer or investigate broker/consumer config.

  - alert: DBHighLatency_Placeholder
    expr: db_query_duration_seconds{quantile="0.99"} > 1
    for: 5m
    labels:
      severity: critical
      service: cdmp-mini
    annotations:
      summary: "High DB 99th percentile latency (placeholder metric)"
      description: |
        This rule is a placeholder. Replace `db_query_duration_seconds{quantile="0.99"}` with your actual DB latency metric.

        Runbook:
        - Check slow queries, locks, and DB resource utilization.
        - Consider scaling DB or optimizing queries.

# Notes:
# - Thresholds (10000 for lag, 800 for in-flight, etc.) are example defaults based on repository defaults. Tune them for your environment.
# - If you use Prometheus Operator (PrometheusRule CRD), wrap the rules into a PrometheusRule manifest accordingly.
# - Consider adding alertmanager: notify channels and runbook links in Alertmanager configuration.
